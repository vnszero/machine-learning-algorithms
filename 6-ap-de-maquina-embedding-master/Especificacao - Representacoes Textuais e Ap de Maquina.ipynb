{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aluno: `Victor Le Roy Matos`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta prática iremos apresentar o uso de embeddings. Para isso, você deve primeiro instalar as dependencias usando `pip install -r requirements.txt` (ou `pip3`, dependendo da forma que seu python está instalado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting alembic==1.5.8\n",
      "  Using cached alembic-1.5.8-py2.py3-none-any.whl (159 kB)\n",
      "Collecting attrs==20.3.0\n",
      "  Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "Collecting cliff==3.7.0\n",
      "  Using cached cliff-3.7.0-py3-none-any.whl (80 kB)\n",
      "Collecting cmaes==0.8.2\n",
      "  Using cached cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
      "Collecting cmd2==1.5.0\n",
      "  Using cached cmd2-1.5.0-py3-none-any.whl (133 kB)\n",
      "Collecting colorama==0.4.4\n",
      "  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting colorlog==5.0.1\n",
      "  Using cached colorlog-5.0.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting cycler==0.10.0\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting greenlet==1.0.0\n",
      "  Using cached greenlet-1.0.0.tar.gz (84 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting importlib-metadata==4.0.1\n",
      "  Using cached importlib_metadata-4.0.1-py3-none-any.whl (16 kB)\n",
      "Collecting joblib==1.0.1\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Collecting kiwisolver==1.3.1\n",
      "  Using cached kiwisolver-1.3.1.tar.gz (53 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting Mako==1.1.4\n",
      "  Using cached Mako-1.1.4-py2.py3-none-any.whl (75 kB)\n",
      "Collecting MarkupSafe==1.1.1\n",
      "  Using cached MarkupSafe-1.1.1.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting matplotlib==3.3.4\n",
      "  Using cached matplotlib-3.3.4.tar.gz (37.9 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting numpy==1.19.5\n",
      "  Using cached numpy-1.19.5.zip (7.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [202 lines of output]\n",
      "      setup.py:67: RuntimeWarning: NumPy 1.19.5 may not yet support Python 3.11.\n",
      "        warnings.warn(\n",
      "      Running from numpy source directory.\n",
      "      setup.py:480: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates\n",
      "        run_build = parse_setuppy_commands()\n",
      "      Cythonizing sources\n",
      "      Processing numpy/random\\_bounded_integers.pxd.in\n",
      "      Processing numpy/random\\bit_generator.pyx\n",
      "      Processing numpy/random\\mtrand.pyx\n",
      "      Processing numpy/random\\_bounded_integers.pyx.in\n",
      "      Processing numpy/random\\_common.pyx\n",
      "      Processing numpy/random\\_generator.pyx\n",
      "      Processing numpy/random\\_mt19937.pyx\n",
      "      Processing numpy/random\\_pcg64.pyx\n",
      "      Processing numpy/random\\_philox.pyx\n",
      "      Processing numpy/random\\_sfc64.pyx\n",
      "      blas_opt_info:\n",
      "      blas_mkl_info:\n",
      "      No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils\n",
      "      customize MSVCCompiler\n",
      "        libraries mkl_rt not found in ['h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\lib', 'C:\\\\', 'h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\libs']\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      blis_info:\n",
      "        libraries blis not found in ['h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\lib', 'C:\\\\', 'h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\libs']\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      openblas_info:\n",
      "        libraries openblas not found in ['h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\lib', 'C:\\\\', 'h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\libs']\n",
      "      get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'\n",
      "      customize GnuFCompiler\n",
      "      Could not locate executable g77\n",
      "      Could not locate executable f77\n",
      "      customize IntelVisualFCompiler\n",
      "      Could not locate executable ifort\n",
      "      Could not locate executable ifl\n",
      "      customize AbsoftFCompiler\n",
      "      Could not locate executable f90\n",
      "      customize CompaqVisualFCompiler\n",
      "      Could not locate executable DF\n",
      "      customize IntelItaniumVisualFCompiler\n",
      "      Could not locate executable efl\n",
      "      customize Gnu95FCompiler\n",
      "      Could not locate executable gfortran\n",
      "      Could not locate executable f95\n",
      "      customize G95FCompiler\n",
      "      Could not locate executable g95\n",
      "      customize IntelEM64VisualFCompiler\n",
      "      customize IntelEM64TFCompiler\n",
      "      Could not locate executable efort\n",
      "      Could not locate executable efc\n",
      "      customize PGroupFlangCompiler\n",
      "      Could not locate executable flang\n",
      "      don't know how to compile Fortran code on platform 'nt'\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      atlas_3_10_blas_threads_info:\n",
      "      Setting PTATLAS=ATLAS\n",
      "        libraries tatlas not found in ['h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\lib', 'C:\\\\', 'h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\libs']\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      atlas_3_10_blas_info:\n",
      "        libraries satlas not found in ['h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\lib', 'C:\\\\', 'h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\libs']\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      atlas_blas_threads_info:\n",
      "      Setting PTATLAS=ATLAS\n",
      "        libraries ptf77blas,ptcblas,atlas not found in ['h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\lib', 'C:\\\\', 'h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\libs']\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      atlas_blas_info:\n",
      "        libraries f77blas,cblas,atlas not found in ['h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\lib', 'C:\\\\', 'h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\libs']\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      accelerate_info:\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      C:\\Users\\Victor\\AppData\\Local\\Temp\\pip-install-csl1bclf\\numpy_abaf5b91a5c94ff6bafc8096ccc5a673\\numpy\\distutils\\system_info.py:1914: UserWarning:\n",
      "          Optimized (vendor) Blas libraries are not found.\n",
      "          Falls back to netlib Blas library which has worse performance.\n",
      "          A better performance should be easily gained by switching\n",
      "          Blas library.\n",
      "        if self._calc_info(blas):\n",
      "      blas_info:\n",
      "        libraries blas not found in ['h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\lib', 'C:\\\\', 'h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\libs']\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      C:\\Users\\Victor\\AppData\\Local\\Temp\\pip-install-csl1bclf\\numpy_abaf5b91a5c94ff6bafc8096ccc5a673\\numpy\\distutils\\system_info.py:1914: UserWarning:\n",
      "          Blas (http://www.netlib.org/blas/) libraries not found.\n",
      "          Directories to search for the libraries can be specified in the\n",
      "          numpy/distutils/site.cfg file (section [blas]) or by setting\n",
      "          the BLAS environment variable.\n",
      "        if self._calc_info(blas):\n",
      "      blas_src_info:\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      C:\\Users\\Victor\\AppData\\Local\\Temp\\pip-install-csl1bclf\\numpy_abaf5b91a5c94ff6bafc8096ccc5a673\\numpy\\distutils\\system_info.py:1914: UserWarning:\n",
      "          Blas (http://www.netlib.org/blas/) sources not found.\n",
      "          Directories to search for the sources can be specified in the\n",
      "          numpy/distutils/site.cfg file (section [blas_src]) or by setting\n",
      "          the BLAS_SRC environment variable.\n",
      "        if self._calc_info(blas):\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      non-existing path in 'numpy\\\\distutils': 'site.cfg'\n",
      "      lapack_opt_info:\n",
      "      lapack_mkl_info:\n",
      "        libraries mkl_rt not found in ['h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\lib', 'C:\\\\', 'h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\libs']\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      openblas_lapack_info:\n",
      "        libraries openblas not found in ['h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\lib', 'C:\\\\', 'h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\libs']\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      openblas_clapack_info:\n",
      "        libraries openblas,lapack not found in ['h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\lib', 'C:\\\\', 'h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\libs']\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      flame_info:\n",
      "        libraries flame not found in ['h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\lib', 'C:\\\\', 'h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\libs']\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      atlas_3_10_threads_info:\n",
      "      Setting PTATLAS=ATLAS\n",
      "        libraries lapack_atlas not found in h:\\Development\\Python\\Python 3.11.2\\lib\n",
      "        libraries tatlas,tatlas not found in h:\\Development\\Python\\Python 3.11.2\\lib\n",
      "        libraries lapack_atlas not found in C:\\\n",
      "        libraries tatlas,tatlas not found in C:\\\n",
      "        libraries lapack_atlas not found in h:\\Development\\Python\\Python 3.11.2\\libs\n",
      "        libraries tatlas,tatlas not found in h:\\Development\\Python\\Python 3.11.2\\libs\n",
      "      <class 'numpy.distutils.system_info.atlas_3_10_threads_info'>\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      atlas_3_10_info:\n",
      "        libraries lapack_atlas not found in h:\\Development\\Python\\Python 3.11.2\\lib\n",
      "        libraries satlas,satlas not found in h:\\Development\\Python\\Python 3.11.2\\lib\n",
      "        libraries lapack_atlas not found in C:\\\n",
      "        libraries satlas,satlas not found in C:\\\n",
      "        libraries lapack_atlas not found in h:\\Development\\Python\\Python 3.11.2\\libs\n",
      "        libraries satlas,satlas not found in h:\\Development\\Python\\Python 3.11.2\\libs\n",
      "      <class 'numpy.distutils.system_info.atlas_3_10_info'>\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      atlas_threads_info:\n",
      "      Setting PTATLAS=ATLAS\n",
      "        libraries lapack_atlas not found in h:\\Development\\Python\\Python 3.11.2\\lib\n",
      "        libraries ptf77blas,ptcblas,atlas not found in h:\\Development\\Python\\Python 3.11.2\\lib\n",
      "        libraries lapack_atlas not found in C:\\\n",
      "        libraries ptf77blas,ptcblas,atlas not found in C:\\\n",
      "        libraries lapack_atlas not found in h:\\Development\\Python\\Python 3.11.2\\libs\n",
      "        libraries ptf77blas,ptcblas,atlas not found in h:\\Development\\Python\\Python 3.11.2\\libs\n",
      "      <class 'numpy.distutils.system_info.atlas_threads_info'>\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      atlas_info:\n",
      "        libraries lapack_atlas not found in h:\\Development\\Python\\Python 3.11.2\\lib\n",
      "        libraries f77blas,cblas,atlas not found in h:\\Development\\Python\\Python 3.11.2\\lib\n",
      "        libraries lapack_atlas not found in C:\\\n",
      "        libraries f77blas,cblas,atlas not found in C:\\\n",
      "        libraries lapack_atlas not found in h:\\Development\\Python\\Python 3.11.2\\libs\n",
      "        libraries f77blas,cblas,atlas not found in h:\\Development\\Python\\Python 3.11.2\\libs\n",
      "      <class 'numpy.distutils.system_info.atlas_info'>\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      lapack_info:\n",
      "        libraries lapack not found in ['h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\lib', 'C:\\\\', 'h:\\\\Development\\\\Python\\\\Python 3.11.2\\\\libs']\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      C:\\Users\\Victor\\AppData\\Local\\Temp\\pip-install-csl1bclf\\numpy_abaf5b91a5c94ff6bafc8096ccc5a673\\numpy\\distutils\\system_info.py:1748: UserWarning:\n",
      "          Lapack (http://www.netlib.org/lapack/) libraries not found.\n",
      "          Directories to search for the libraries can be specified in the\n",
      "          numpy/distutils/site.cfg file (section [lapack]) or by setting\n",
      "          the LAPACK environment variable.\n",
      "        return getattr(self, '_calc_info_{}'.format(name))()\n",
      "      lapack_src_info:\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      C:\\Users\\Victor\\AppData\\Local\\Temp\\pip-install-csl1bclf\\numpy_abaf5b91a5c94ff6bafc8096ccc5a673\\numpy\\distutils\\system_info.py:1748: UserWarning:\n",
      "          Lapack (http://www.netlib.org/lapack/) sources not found.\n",
      "          Directories to search for the sources can be specified in the\n",
      "          numpy/distutils/site.cfg file (section [lapack_src]) or by setting\n",
      "          the LAPACK_SRC environment variable.\n",
      "        return getattr(self, '_calc_info_{}'.format(name))()\n",
      "        NOT AVAILABLE\n",
      "      \n",
      "      numpy_linalg_lapack_lite:\n",
      "        FOUND:\n",
      "          language = c\n",
      "          define_macros = [('HAVE_BLAS_ILP64', None), ('BLAS_SYMBOL_SUFFIX', '64_')]\n",
      "      \n",
      "      C:\\Users\\Victor\\AppData\\Local\\Temp\\pip-build-env-khq8a54l\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:275: UserWarning: Unknown distribution option: 'define_macros'\n",
      "        warnings.warn(msg)\n",
      "      running dist_info\n",
      "      running build_src\n",
      "      build_src\n",
      "      building py_modules sources\n",
      "      creating build\n",
      "      creating build\\src.win-amd64-3.11\n",
      "      creating build\\src.win-amd64-3.11\\numpy\n",
      "      creating build\\src.win-amd64-3.11\\numpy\\distutils\n",
      "      building library \"npymath\" sources\n",
      "      error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, você deverá baixar os repositórios em português e inglês e salvá-los na pasta `embedding_data` seguindo as seguintes instruções: \n",
    "\n",
    "- [No respositório da USP](http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc) baixe [este arquivo (Glove 100 dimensões)](http://143.107.183.175:22980/download.php?file=embeddings/glove/glove_s100.zip). Ele possui  um pouco mais de 600 mil palavras retiradas de textos de páginas Web tais como a Wikipedia e canais de notícias [(Hartmann et al., 2017)](https://arxiv.org/abs/1708.06025). Descomprima e renomeie o arquivo txt para `glove.pt.100.txt`.\n",
    "\n",
    "- No [repositório de Stanford](https://nlp.stanford.edu/projects/glove/), baixe [este arquivo](http://nlp.stanford.edu/data/glove.6B.zip) use o arquivo . Este arquivo compreende ~400 mil palavras de textos extraidos da Wikipédia e [GigaWord](https://catalog.ldc.upenn.edu/LDC2011T07) [(Pennington et al., 2015)](https://nlp.stanford.edu/pubs/glove.pdf). Descomprima e salve o arquivo com embeddings de 100 dimensões (nome `glove.6B.100d.txt`) na pasta `embedding_data` renomeando esse arquivo para `glove.en.100.txt`.\n",
    "\n",
    "Como você pode perceber, esta prática demandará um espaço livre em disco de aproximadamente 3GB. Os arquivos estão no seguinte formato: em cada linha, uma palavra e N valores representando o valor em cada uma das N dimensões do embedding desta palavra. Por exemplo, caso as palavras `casa`, `redondel` e `rei` sejam representadas por um embedding de 4 dimensões, uma possível representação seria:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "casa 0.12 0.1 0.5 -0.4\n",
    "redondel 0.2 0.1 -0.4 0.5\n",
    "rei 0.1 0.5 -0.1 0.1\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `get_embedding`, do arquivo `embeddings/utils.py` é responsável por ler esse arquivo e gerar um dicionário em que a chave é a palavra e o valor é sua representação por meio de embeddings. Para a  representação acima, a saída desta função seria seria: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dict_embedding_ex = {\n",
    "                        \"casa\":np.array([0.12,0.1,0.5,-0.4]),\n",
    "                        \"redondel\":np.array([0.2,0.1,-0.4,0.5]),\n",
    "                        \"rei\":np.array([0.1,0.5,-0.1,0.1]),\n",
    "                    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa função, também é salvo o objeto criado usando [pickle](https://docs.python.org/3/library/pickle.html), assim, a próxima vez que seja lido o embedding, a leitura será mais rápida.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 1 - obtenção do embedding**: Complete a função `get_embedding` obtendo a palavra e o vetor de embeddings com a dimensão `embeddings_size` substituindo os `None` apropriadamente. O dataset possui algumas incosistencias que você deve considerar ao modificar essas linhas: no dataset em português, a maioria das palavras compostas são separadas por hífen, porém, foi verificado que umas palavras foi separado por espaço. Por caso disso, você deve considerar que as `embeddings_size` últimas posições são os valores de cada dimensão, separados por espaço e, as demais, são a palavra. Sugiro \"brincar\" abaixo com o uso de [índice negativo](https://www.geeksforgeeks.org/python-negative-index-of-element-in-list/) entenda também o [método join](https://www.geeksforgeeks.org/join-function-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pé de moleque': [ 0.1 -0.5  0.5  0.1 -0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "linha = \"pé de moleque 0.1 -0.5 0.5 0.1 -0.5\"\n",
    "embedding_size = 5\n",
    "arr_line = linha.strip().split()\n",
    "\n",
    "word = \" \".join(arr_line[0:-5])\n",
    "#colocamos float16 para economizar memória\n",
    "embedding = np.array(arr_line[-5:], dtype=np.float16)\n",
    "print(f\"'{word}': {embedding}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o teste unitário abaixo para verificar o funcionamento do `get_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: rei\n",
      "Palavras ignoradas: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_get_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute os embeddings em português e ingles. Não se preocupe com as palavras ignoradas: foram algumas inconsistências no dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: distribuída\n",
      "20000: selena\n",
      "30000: sailor\n",
      "40000: aguaceiros\n",
      "50000: retrô\n",
      "60000: indesmentível\n",
      "70000: kouchner\n",
      "80000: hoya\n",
      "90000: j&f\n",
      "100000: castra\n",
      "110000: gynt\n",
      "120000: caddie\n",
      "130000: afluíam\n",
      "140000: nashua\n",
      "150000: amok\n",
      "160000: pormenorizou\n",
      "170000: otway\n",
      "180000: bandeirismo\n",
      "190000: críptico\n",
      "200000: kinyarwanda\n",
      "210000: yari\n",
      "220000: picotado\n",
      "230000: roberth\n",
      "240000: illex\n",
      "250000: og00\n",
      "260000: kalin\n",
      "270000: autoridadeslocais\n",
      "280000: goleava\n",
      "290000: mambos\n",
      "300000: interesado\n",
      "310000: cpdlc\n",
      "320000: samenwerkende\n",
      "330000: dimensсo\n",
      "340000: monteggia\n",
      "350000: sangrur\n",
      "360000: wuncler\n",
      "370000: villaputzu\n",
      "380000: zika.a\n",
      "390000: salvares\n",
      "400000: panik\n",
      "410000: hh000\n",
      "420000: boggies\n",
      "430000: super-licença\n",
      "440000: imeadiato\n",
      "450000: ad-libs\n",
      "460000: niinimaki\n",
      "470000: chhu\n",
      "480000: neuropáticas\n",
      "490000: atufando-se\n",
      "500000: megaigrejas\n",
      "510000: analisávamos\n",
      "520000: gitaigo\n",
      "530000: quichua\n",
      "540000: baiocchi\n",
      "550000: jeder\n",
      "560000: tadros\n",
      "570000: celebrou-a\n",
      "580000: hep-ph/0000000\n",
      "590000: palmview\n",
      "600000: tuyakbay\n",
      "610000: comapny\n",
      "620000: júnior.\n",
      "630000: reptiliomorfos\n",
      "640000: aglonas\n",
      "650000: coloniaes\n",
      "660000: frontalot\n",
      "670000: locomotivos\n",
      "680000: podlažice\n",
      "690000: tamta\n",
      "700000: alvadias\n",
      "710000: decoded\n",
      "720000: holder-bank\n",
      "730000: notificar-lhe\n",
      "740000: sipuncula\n",
      "750000: 0000pelos\n",
      "760000: batukada\n",
      "770000: conirostris\n",
      "780000: ergoespirometria\n",
      "790000: harleyville\n",
      "800000: lanlan\n",
      "810000: navigação\n",
      "820000: prolongara-se\n",
      "830000: sitoli\n",
      "840000: vassiljeva\n",
      "850000: ajuda-lhe\n",
      "860000: can-didaturas\n",
      "870000: dewar's\n",
      "880000: fritagelse\n",
      "890000: keppelmann\n",
      "900000: nauti\n",
      "910000: quarteirenses\n",
      "920000: successfactors\n",
      "Palavras ignoradas: 3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from embeddings.utils import get_embedding, plot_words_embeddings\n",
    "\n",
    "# str_dataset = \"glove.en.100.txt\"\n",
    "# dict_embedding_en = get_embedding(str_dataset)\n",
    "str_dataset = \"glove.pt.100.txt\"\n",
    "dict_embedding_pt = get_embedding(str_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `plot_words_embeddings` utiliza [Análise de Componentes Principais](https://pt.wikipedia.org/wiki/An%C3%A1lise_de_componentes_principais) (PCA, do inglês Principal Component Analisys) para reduzir cada embedding em 2 dimensões para, logo após, plotar em um grafico a posição dessas palavras de acordo com o embedding. Veja o gráfico apresentado abaixo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_to_use = {\n",
    "                        # \"en\":\n",
    "                        #     {\"embedding\":dict_embedding_en,\n",
    "                        #     \"words_to_use\":[\"prince\",\"princess\",\n",
    "                        #                     \"duchess\", \"duke\", \"countess\", \"marquis\", \n",
    "                        #                     \"marquise\",\"king\",\"queen\",\n",
    "                        #                     \"girl\",\"boy\",\"man\",\"woman\",\"child\"]},\n",
    "\n",
    "                        \"pt\":{\"embedding\":dict_embedding_pt,\n",
    "                          \"words_to_use\":[\"principe\",\"rei\",\"rainha\",\"conde\",\"duquesa\",\"duque\",\"condessa\",\n",
    "                           \"marquês\",\"marquesa\",\n",
    "                           \"homem\",\"mulher\",\"princesa\",\"menina\",\"menino\",\"criança\",\n",
    "                           \"garoto\",\"garota\"]}\n",
    "                }\n",
    "\n",
    "language = \"pt\"#mude de 'pt' para 'en' para ver em ingles tb!\n",
    "plot_words_embeddings(embeddings_to_use[language][\"embedding\"], \n",
    "                    embeddings_to_use[language][\"words_to_use\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo acima, em português, veja que podemos pensar em dois conceitos claramente divididos: a realeza e o gênero. Pense: neste plano cartesiano, qual eixo corresponde ao conceito de realeza? E o de gênero? Perceba que \"criança\" deveria ter gênero neutro - de fato, está mais próximo do zero. Porém, pode haver algum ruído associando a palavra criança ao genero feminino. Isso, em português, pode haver uma explicação, pois utilizamos o artigo `a`, usado para palavras que remetem ao genero feminino, para se referir a criança. Assim, em português, os artigos podem aproximar uma palavra de gênero neutro a um determinado gênero.\n",
    "\n",
    "\n",
    "Em inglês, não foi possível verificar tão bem a divisão entre os conceitos de `genero` e `realeza`. Isso pode ocorrer devido a redução de dimensionalidade: os conceitos não necessariamente correspondem a um eixo no plano cartesiano e, mesmo se corresponderem, ao mapear itens com $n$ dimensões para um plano bidimensional, pode haver perda de informação. Mesmo assim, conseguimos ver a separação entre palavras da realeza e que não são da realeza. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinta-se livre para \"brincar\", alterando/adicionando palavras. Por exemplo, adicione animais. Devido à ambiguidades, ao dataset e à própria redução de dimensionalidade, podem existir palavras que estão erroneamente próximas, se considerarmos o conceito das mesmas,  principalmente se adicionarmos palavras de conceitos muito distintos. Um detalhe: no dataset em português, há uso de palavras compostas e elas estão (geralmente) separadas por hífen. No dataset em inglês não há palavras compostas.\n",
    "\n",
    "Tanto nesta tarefa quanto na próxima você poderá perceber que os embeddings podem carregar preconceitos. Há uma forma de modificar os vetores para eliminar um determinado tipo de preconceito. Por exemplo, nesses embeddings existirão palavras erronemente similares a um determinado genero e, para corrigir, é possível deixar todas as palavras sem distinção pelo genero. Caso queira saber como minimizar esse problema, veja o artigo \"[Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/abs/1607.06520)\". O título do artigo se remete a um preconceito descoberto ao usar analogias, que será o próximo tópico desta prática. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de analogias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra caracteristica muito interessante ao usar embedding é a criação de analogias. Por exemplo, na frase `homem está para mulher assim como rei está para...`, fazendo operações com os _embeddings_, muitas vezes é possível chegar na analogia mais provável que, neste caso, seria a palavra `rainha`.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 2 - cálculo da analogia:** Nesta atividade, iremos implementar o método `calcula_embedding_analogia` da classe `Analogy`. Essa classe tem acesso ao dicionário de embeddings e a estrutura KDTree, que iremos explicá-la posteriormente. Considerando a frase <span style=\"color:blue\">\"**palavra_x** está para **palavra_y** assim como **assim_como** esta para **palavra_z**\"</span>, o método `calcula_embedding_analogia` recebe como parâmetro as palavras `palavra_x`, `esta_para` e `assim_ como` e retorna um embedding que, possivelmente, será muito próximo da `palavra_z`. \n",
    "\n",
    "Veja [na aula](https://docs.google.com/presentation/d/1-CggYUA2s7LW7_LcnGv7vlpUGFg9kEWG0j6lWGUnaLI/edit?usp=sharing) como é feito o cálculo e, logo após, faça o teste unitário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.0, -1.0, -1.0, ]\n",
      "[ 12.296875, 53.09375, 30.984375, ]\n",
      "[ -10.96875, -30.90625, -9.6015625, ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 2, 3],[-1.2, 3.2, 1.2],[12.2, 31.2, 11.2]], dtype=np.float16)\n",
    "esta_para = np.array([[-3, 0, 1],[11, 56, 32.2],[0, 0.2, 0.4]], dtype=np.float16)\n",
    "assim_como = np.array([[2, 1, 1],[0.1,0.3,0],[1.23, 0.1, 1.2]], dtype=np.float16)\n",
    "\n",
    "for i,x_val in enumerate(x):\n",
    "    arr_embedding = assim_como[i]-x[i]+esta_para[i]\n",
    "    print(\"[\",end=\" \")\n",
    "    for val in arr_embedding:\n",
    "        print(float(val),end=\", \")\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_calculo_analogia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 3 - busca da palavra mais similar:** O cálculo da atividade anterior resultou em um embedding e, agora, precisamos  procuramos a palavra mais próxima a este embedding obtido. Para isso, precisamos de: (1) uma forma eficiente para percorrer os embeddings para descobrir o mais similar; (2) uma métrica de similaridade/distância; \n",
    "\n",
    "**Como percorrer embeddings?** Para encontrarmos os embeddings similares, uma alternativa seria percorrer todos os vetores de embeddings e encontrar o mais similar. Porém, como estamos trabalhando com centenas de milhares de embeddings, essa operação seria muito custosa. Para isso, podemos usar uma estrutura de dados chamada **KDTree**. KDtree é uma arvore que organiza dados espaciais de tal forma que conseguimos alcançar elementos similares de forma mais eficiente. Caso esteja interessado em mais detalhes, [veja este video](https://www.youtube.com/watch?v=Glp7THUpGow).\n",
    "\n",
    "**Qual métrica de distancia/similaridade usaremos?**  Já foi demonstrado que esta métrica é eficiente para similaridade entre embeddings é a distância euclidiana [(Pennington et al., 2015)](https://nlp.stanford.edu/pubs/glove.pdf). A [distancia euclidiana](https://pt.wikipedia.org/wiki/Dist%C3%A2ncia_euclidiana) entre dois pontos $p$ e $q$ é calculada por meio do tamanho da linha entre esses pontos. Para um espaço bidimensional, considerando que os pontos $p$ e $q$ são representados pelas coordenadas $(p_1,p_2)$ e $(q_1,q_2)$, respectivamente, a equação é dada pela seguinte fórmula: $d(p,q) = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2}$ veja uma representação gráfica: \n",
    "\n",
    "<img width=\"400px\" src=\"img/distancia_euclidiana.svg\">\n",
    "\n",
    "Esta métrica pode ser generalizada para um espaço n-dimensional e o cálculo seria: $d(p,q) = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+...+(p_n-q_n)^n}$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, nesta atividade iremos utilizar [a implementação do kdtree do scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html). Nessa estrutura, é possível armazenar os embeddings e, logo após fazer consultas eficientes para, por exemplo, procurar os k elementos mais próximos. Veja o exemplo abaixo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O ponto [3, 3] é o 1º ponto mais próximo de [3, 2] distância: 1.0\n",
      "O ponto [2, 2] é o 2º ponto mais próximo de [3, 2] distância: 1.0\n",
      "O ponto [1, 1] é o 3º ponto mais próximo de [3, 2] distância: 2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "elementos = [[1,1],\n",
    "             [2,2],\n",
    "             [3,3],\n",
    "             [4,4],\n",
    "             [5,5],\n",
    "             [6,6],\n",
    "             ]\n",
    "#os elementos são passados como parametro na construção do KDTree junto com a métrica \n",
    "#de distancia que iremos usar\n",
    "kdtree = KDTree(elementos,  metric='euclidean')\n",
    "\n",
    "#retorna os 2 elementos mais próximos e sua distancia\n",
    "#como podemos fazer uma consulta por lista de pontos, temos que \n",
    "#passar uma lista de pontos como parametro\n",
    "ponto = [3,2]\n",
    "distancia,pos_mais_prox = kdtree.query([ponto], k=3, return_distance=True)\n",
    "for i,pos in enumerate(pos_mais_prox[0]):\n",
    "    elemento = elementos[pos]\n",
    "    distancia_ponto = distancia[0][i]\n",
    "    print(f\"O ponto {elemento} é o {i+1}º ponto mais próximo de {ponto} distância: {distancia_ponto}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, cada embedding pode ser armazenado no KTree para, logo após, obtermos os embeddings mais próximos a um embedding em questão. Não é possível armazenar na estrutura do KDTree a palavra referente a cada embedding representado, por isso, armazenamos essa estrutura como um atributo da classe `KDTreeEmbedding` (arquivo `utils.py`) que armazena também os atributos `pos_to_word` mapeando, para cada posição a palavra correspondente e o atributo `word_to_pos` que faz o oposto: mapeia, para cada palavra, a posição correspondente. Veja no construtor de `KDTreeEmbedding` como é criado o KDTree. Nela, também será salvo um arquivo com a implementação do KDtree e os atributos `pot_to_word` e `word_to_pos` isso é necessário pois a criação da KDTree é muito custosa.\n",
    "\n",
    "\n",
    "Nesta atividade, você deverá implementar `get_most_similar_embedding` que obtém as $k$ palavras mais similares à palavra (ou embedding) representado pelo parâmetro `query` por meio do método `query` da KDTree. O parâmetro `query` pode ser a palavra (`string`) ou o proprio embedding (`np.array`). Logo após, implemente também o método `get_embeddings_by_similarity` que utiliza o método `query_radius` ([veja documentação](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree.query_radius)) que retorna todas as palavras que estão em um raio de `max_distance` da palavra alvo especificada pelo parametro `query`. Para ambas as implementações, utiliza-se o método `positions_to_word`, já implementado, para retornar as palavras de acordo com as posições indicadas. Caso haja alguma palavra a ser ignorada em `words_to_ignore` ela será excluída também no método `positions_to_word`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_get_most_similar_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_embeddings_by_similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, você pode testar os métodos utilizando os datasets de embeddings. Lembre-se  que o KDTree pode demorar mais de 30 minutos para ser criado na primeira execução de cada idioma. Caso queira testar para o inglês, não esqueça de mudar de `\"kdtree.pt.p\"` para `\"kdtree.en.p\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: distribuída\n",
      "20000: selena\n",
      "30000: sailor\n",
      "40000: aguaceiros\n",
      "50000: retrô\n",
      "60000: indesmentível\n",
      "70000: kouchner\n",
      "80000: hoya\n",
      "90000: j&f\n",
      "100000: castra\n",
      "110000: gynt\n",
      "120000: caddie\n",
      "130000: afluíam\n",
      "140000: nashua\n",
      "150000: amok\n",
      "160000: pormenorizou\n",
      "170000: otway\n",
      "180000: bandeirismo\n",
      "190000: críptico\n",
      "200000: kinyarwanda\n",
      "210000: yari\n",
      "220000: picotado\n",
      "230000: roberth\n",
      "240000: illex\n",
      "250000: og00\n",
      "260000: kalin\n",
      "270000: autoridadeslocais\n",
      "280000: goleava\n",
      "290000: mambos\n",
      "300000: interesado\n",
      "310000: cpdlc\n",
      "320000: samenwerkende\n",
      "330000: dimensсo\n",
      "340000: monteggia\n",
      "350000: sangrur\n",
      "360000: wuncler\n",
      "370000: villaputzu\n",
      "380000: zika.a\n",
      "390000: salvares\n",
      "400000: panik\n",
      "410000: hh000\n",
      "420000: boggies\n",
      "430000: super-licença\n",
      "440000: imeadiato\n",
      "450000: ad-libs\n",
      "460000: niinimaki\n",
      "470000: chhu\n",
      "480000: neuropáticas\n",
      "490000: atufando-se\n",
      "500000: megaigrejas\n",
      "510000: analisávamos\n",
      "520000: gitaigo\n",
      "530000: quichua\n",
      "540000: baiocchi\n",
      "550000: jeder\n",
      "560000: tadros\n",
      "570000: celebrou-a\n",
      "580000: hep-ph/0000000\n",
      "590000: palmview\n",
      "600000: tuyakbay\n",
      "610000: comapny\n",
      "620000: júnior.\n",
      "630000: reptiliomorfos\n",
      "640000: aglonas\n",
      "650000: coloniaes\n",
      "660000: frontalot\n",
      "670000: locomotivos\n",
      "680000: podlažice\n",
      "690000: tamta\n",
      "700000: alvadias\n",
      "710000: decoded\n",
      "720000: holder-bank\n",
      "730000: notificar-lhe\n",
      "740000: sipuncula\n",
      "750000: 0000pelos\n",
      "760000: batukada\n",
      "770000: conirostris\n",
      "780000: ergoespirometria\n",
      "790000: harleyville\n",
      "800000: lanlan\n",
      "810000: navigação\n",
      "820000: prolongara-se\n",
      "830000: sitoli\n",
      "840000: vassiljeva\n",
      "850000: ajuda-lhe\n",
      "860000: can-didaturas\n",
      "870000: dewar's\n",
      "880000: fritagelse\n",
      "890000: keppelmann\n",
      "900000: nauti\n",
      "910000: quarteirenses\n",
      "920000: successfactors\n",
      "Palavras ignoradas: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  3.8529124618382,\n",
       "  3.879822890301186,\n",
       "  4.110205347652384,\n",
       "  4.330550049048636],\n",
       " ['carro', 'veículo', 'caminhão', 'motorista', 'moto'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "str_dataset = \"glove.pt.100.txt\"\n",
    "kdtree_file = \"kdtree.pt.p\"\n",
    "dict_embedding = get_embedding(str_dataset)\n",
    "kdtree = KDTreeEmbedding(dict_embedding, kdtree_file)\n",
    "kdtree.get_most_similar_embedding(\"carro\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5 - 💞 apresentando as analogias 💞:** Agora você deverá implementar o método `analogia` da classe `Analogy` que deverá utilizar os métodos `calcula_embedding_analogia` e o `get_most_similar_embedding` para retornar as 4 palavras mais prováveis para completar uma determinada analogia, com os parâmetros indicados. Caso, dentre as 4 palavras, haja uma palavra dos pârametro de entrada, a mesma pode ser excluída, retorando menos palavras. Por exemplo, considerando \"**rei** está para **rainha** assim como **homem** está para...\", caso uma das palavras de saída para essa entrada  seja `rainha`, o método poderá retornar 3 palavras (eliminando a palavra rainha). Isso já é considerado no método `get_most_similar_embedding`. Lembre-se que o método `get_most_similar_embedding` é da classe KDTreeEmbedding e a `Analogy` possui o atributo `kdtree_embedding` que é uma instância da classe `KDTreeEmbedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_analogy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja as analogias (brinque à vontade com a representação em português e em inglês)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: distribuída\n",
      "20000: selena\n",
      "30000: sailor\n",
      "40000: aguaceiros\n",
      "50000: retrô\n",
      "60000: indesmentível\n",
      "70000: kouchner\n",
      "80000: hoya\n",
      "90000: j&f\n",
      "100000: castra\n",
      "110000: gynt\n",
      "120000: caddie\n",
      "130000: afluíam\n",
      "140000: nashua\n",
      "150000: amok\n",
      "160000: pormenorizou\n",
      "170000: otway\n",
      "180000: bandeirismo\n",
      "190000: críptico\n",
      "200000: kinyarwanda\n",
      "210000: yari\n",
      "220000: picotado\n",
      "230000: roberth\n",
      "240000: illex\n",
      "250000: og00\n",
      "260000: kalin\n",
      "270000: autoridadeslocais\n",
      "280000: goleava\n",
      "290000: mambos\n",
      "300000: interesado\n",
      "310000: cpdlc\n",
      "320000: samenwerkende\n",
      "330000: dimensсo\n",
      "340000: monteggia\n",
      "350000: sangrur\n",
      "360000: wuncler\n",
      "370000: villaputzu\n",
      "380000: zika.a\n",
      "390000: salvares\n",
      "400000: panik\n",
      "410000: hh000\n",
      "420000: boggies\n",
      "430000: super-licença\n",
      "440000: imeadiato\n",
      "450000: ad-libs\n",
      "460000: niinimaki\n",
      "470000: chhu\n",
      "480000: neuropáticas\n",
      "490000: atufando-se\n",
      "500000: megaigrejas\n",
      "510000: analisávamos\n",
      "520000: gitaigo\n",
      "530000: quichua\n",
      "540000: baiocchi\n",
      "550000: jeder\n",
      "560000: tadros\n",
      "570000: celebrou-a\n",
      "580000: hep-ph/0000000\n",
      "590000: palmview\n",
      "600000: tuyakbay\n",
      "610000: comapny\n",
      "620000: júnior.\n",
      "630000: reptiliomorfos\n",
      "640000: aglonas\n",
      "650000: coloniaes\n",
      "660000: frontalot\n",
      "670000: locomotivos\n",
      "680000: podlažice\n",
      "690000: tamta\n",
      "700000: alvadias\n",
      "710000: decoded\n",
      "720000: holder-bank\n",
      "730000: notificar-lhe\n",
      "740000: sipuncula\n",
      "750000: 0000pelos\n",
      "760000: batukada\n",
      "770000: conirostris\n",
      "780000: ergoespirometria\n",
      "790000: harleyville\n",
      "800000: lanlan\n",
      "810000: navigação\n",
      "820000: prolongara-se\n",
      "830000: sitoli\n",
      "840000: vassiljeva\n",
      "850000: ajuda-lhe\n",
      "860000: can-didaturas\n",
      "870000: dewar's\n",
      "880000: fritagelse\n",
      "890000: keppelmann\n",
      "900000: nauti\n",
      "910000: quarteirenses\n",
      "920000: successfactors\n",
      "Palavras ignoradas: 3\n",
      "brasil está para brasilia assim como...\n",
      "\tperu está para aneura (ou ['trong', 'lacrima', 'vorskla'])\n",
      "\tgana está para aneura (ou ['seraing', 'fene', 'dewsbury', 'gria'])\n",
      "\tjapão está para yeonpyeong (ou ['lieja', 'pottstown', 'belém-pa', 'flexi'])\n",
      "\tespanha está para logroño (ou ['valladolid', 'kalapa', 'cádiz', 'baiona'])\n",
      "\tindia está para vjm00 (ou ['mailman', 'excursion', 'nsi', 'clementia'])\n",
      "bahia está para salvador assim como...\n",
      "\tacre está para macapá (ou ['aracaju', 'cuiabá'])\n",
      "\talagoas está para aracaju (ou ['maceió', 'teresina'])\n",
      "\tamapá está para macapá (ou ['amazonas', 'xinguara'])\n",
      "\tamazonas está para maceió (ou ['aracaju', 'macapá'])\n",
      "\tceará está para maceió (ou ['cuiabá', 'aracaju', 'recife'])\n",
      "\tgoiás está para goiânia (ou ['cuiabá', 'aracaju', 'macapá'])\n",
      "brasil está para feijoada assim como...\n",
      "\titalia está para esparguete (ou ['via-crúcis', 'negundo', 'pana', 'taxifolia'])\n",
      "\testados-unidos está para cebolada (ou ['portugas', 'atribuindo-os', 'bróculos', 'surtida'])\n",
      "\tinglaterra está para worcestershire (ou ['falmouth', 'lincolnshire', 'lowestoft'])\n",
      "\targentina está para retrete (ou ['esparguete', 'carnico', 'frita'])\n",
      "\tperu está para frita (ou ['molho', 'guisado', 'assado'])\n",
      "homem está para mulher assim como...\n",
      "\tgaroto está para menina (ou ['garota', 'namorada', 'mãe', 'irmã'])\n",
      "\trei está para rainha (ou ['princesa', 'esposa', 'príncipe'])\n",
      "\tpríncipe está para princesa (ou ['rainha', 'filha', 'esposa'])\n",
      "\tpai está para filha (ou ['mãe', 'esposa', 'irmã', 'marido'])\n",
      "\tcavalo está para dama (ou ['égua', 'carruagem', 'irmã'])\n",
      "\tgarçon está para cabeleireira (ou ['edna', 'pescadora', 'elisabetha'])\n",
      "grande está para pequeno assim como...\n",
      "\tcheio está para armário (ou ['saco', 'gato', 'minúsculo'])\n",
      "\talto está para baixo (ou ['comprido', 'redondo'])\n",
      "\tforte está para fraco (ou ['parecido', 'baixo'])\n",
      "\tlargo está para beco (ou ['fronteiro', 'comprido'])\n",
      "pelé está para futebol assim como...\n",
      "\ttyson está para hóquei (ou ['basquetebol', 'campeão'])\n",
      "\tbolt está para hóquei (ou ['atletismo', 'ciclismo'])\n",
      "\tsenna está para ciclismo (ou ['liga', 'campeonato'])\n",
      "atena está para sabedoria assim como...\n",
      "\tafrodite está para bondade (ou ['harmonia', 'compaixão', 'benevolência'])\n",
      "\tposeidon está para inefável (ou ['sábio', 'bondade', 'intuição'])\n",
      "\tzeus está para compaixão (ou ['bondade', 'alma', 'espiritual'])\n",
      "\tatena está para bondade (ou ['serenidade', 'generosidade', 'compaixão'])\n",
      "cruzeiro está para raposa assim como...\n",
      "\tatlético está para galo (ou ['csa', 'azulão'])\n",
      "\tgremio está para exquisite (ou ['arara-azul-de-lear', 'noreña', 'bifobia'])\n",
      "\tpalmeiras está para macaca (ou ['verdão', 'galo'])\n",
      "\tcorinthians está para timão (ou ['galo', 'verdão'])\n"
     ]
    }
   ],
   "source": [
    "from embeddings.utils import *\n",
    "dict_embedding = get_embedding( \"glove.pt.100.txt\",100)\n",
    "obj_analogy = Analogy(dict_embedding,\"kdtree.pt.p\")\n",
    "\n",
    "\n",
    "dict_analogias = {(\"brasil\",\"brasilia\"):[\"peru\",\"gana\",\"japão\",\"espanha\",\"india\"],\n",
    "                  (\"bahia\",\"salvador\"):[\"acre\",\"alagoas\",\"amapá\",\"amazonas\",\"ceará\",\"goiás\"],\n",
    "                  (\"brasil\",\"feijoada\"):[\"italia\",\"estados-unidos\",\"inglaterra\",\"argentina\",\"peru\"],\n",
    "                  (\"homem\",\"mulher\"):[\"garoto\",\"rei\",\"príncipe\",\"pai\",\"cavalo\",\"garçon\"],\n",
    "                  (\"grande\",\"pequeno\"):[\"cheio\",\"alto\",\"forte\",\"largo\"],\n",
    "                  (\"pelé\",\"futebol\"):[\"tyson\",\"bolt\",\"senna\"],\n",
    "                  (\"atena\",\"sabedoria\"):[\"afrodite\",\"poseidon\",\"zeus\",\"atena\"],\n",
    "                  (\"cruzeiro\",\"raposa\"):[\"atlético\",\"gremio\",\"palmeiras\",\"corinthians\"],\n",
    "                 }\n",
    "\n",
    "for (palavra,esta_para), arr_assim_como in dict_analogias.items():\n",
    "    print(f\"{palavra} está para {esta_para} assim como...\")\n",
    "    for assim_como in arr_assim_como:\n",
    "        palavras = obj_analogy.analogia(palavra,esta_para,assim_como)\n",
    "        print(f\"\\t{assim_como} está para {palavras[0]} (ou {palavras[1:]})\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas limitações desses embeddings é a dependência de idiomas e que palavras ambiguas não são tratadas. Por exemplo, Jaguar pode ser uma marca de carro ou um animal, dependendo do contexto.  Para diminuir o problema de ambuiguidades, o [BERT](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/) é um embedding que a representação da palavra é diferente de acordo com o seu contexto. O [MUSE](https://github.com/facebookresearch/MUSE) é um embedding multilingue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representação textual usando embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitas vezes, precisamos de um único vetor para representar uma frase ou um texto ainda maior. Para isso, podemos usar a representação Bag of Words ou, ainda, representar por palavras chaves ou utilizarmos uma combinação de nossas representações por palavras. Neste tutorial, iremos mostrar como combinar embeddings de palavras e usar a representação por palavras chaves - podendo, inclusive, fazer uma expansão de palavras chaves por embeddings.\n",
    "\n",
    "Para isso, iremos usar o seguinte contexto: por meio de um dataset de revisões de produto da amazon, deseja-se prever automaticamente o sentimento do mesmo (positivo ou negativo). Utilizou-se uma amostra do [dataset do Kaggle para este exemplo](https://www.kaggle.com/bittlingmayer/amazonreviews). Veja abaixo o dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>Do NOT WASTE Your Time: This book, to put it n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>Peels the paint off the walls: I first heard t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>History With Modern Appeal: This is a must rea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>Worse Music cd ever: I tried putting this in a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>Deliberately Obtuse Nonsense: I don't know wha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29215</th>\n",
       "      <td>Better than the movie?: YES! This book gets be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256457</th>\n",
       "      <td>The Best RE yet: This is the best in the RE se...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210215</th>\n",
       "      <td>What are they waiting for?: This has got to be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200693</th>\n",
       "      <td>Hollywood - promoting the Antichrist again?: C...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169551</th>\n",
       "      <td>Best American TV Series Ever: With its combina...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     class\n",
       "id                                                                 \n",
       "204215  Do NOT WASTE Your Time: This book, to put it n...  negative\n",
       "208138  Peels the paint off the walls: I first heard t...  positive\n",
       "157010  History With Modern Appeal: This is a must rea...  positive\n",
       "274316  Worse Music cd ever: I tried putting this in a...  negative\n",
       "57708   Deliberately Obtuse Nonsense: I don't know wha...  negative\n",
       "...                                                   ...       ...\n",
       "29215   Better than the movie?: YES! This book gets be...  positive\n",
       "256457  The Best RE yet: This is the best in the RE se...  positive\n",
       "210215  What are they waiting for?: This has got to be...  positive\n",
       "200693  Hollywood - promoting the Antichrist again?: C...  negative\n",
       "169551  Best American TV Series Ever: With its combina...  positive\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_amazon_reviews = pd.read_csv(\"datasets/amazon_reviews_mini.txt\",index_col=\"id\")\n",
    "df_amazon_reviews"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em um método de aprendizado de maquina, cada instância deve ser representada por um vetor numérico utilizando as representações ditas anteriormente. Iremos ilustrar cada exemplo utilizando uma pequena subamostra desta amostra com 5 exemplos positivos e 5 negativos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>Peels the paint off the walls: I first heard t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>History With Modern Appeal: This is a must rea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>NIV Bible: The NIV Bible is good, but I wish I...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>pouch can be better: I recently bought it at A...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>Great book!: Dawn of a Thousand Nights is extr...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>Do NOT WASTE Your Time: This book, to put it n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>Worse Music cd ever: I tried putting this in a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>Deliberately Obtuse Nonsense: I don't know wha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>Disappointed: Very small wipes canister, not v...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>The most horrible Blu-Ray: First, Night scenes...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     class\n",
       "id                                                                 \n",
       "208138  Peels the paint off the walls: I first heard t...  positive\n",
       "157010  History With Modern Appeal: This is a must rea...  positive\n",
       "101657  NIV Bible: The NIV Bible is good, but I wish I...  positive\n",
       "49225   pouch can be better: I recently bought it at A...  positive\n",
       "158265  Great book!: Dawn of a Thousand Nights is extr...  positive\n",
       "204215  Do NOT WASTE Your Time: This book, to put it n...  negative\n",
       "274316  Worse Music cd ever: I tried putting this in a...  negative\n",
       "57708   Deliberately Obtuse Nonsense: I don't know wha...  negative\n",
       "200048  Disappointed: Very small wipes canister, not v...  negative\n",
       "60933   The most horrible Blu-Ray: First, Night scenes...  negative"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_positive = df_amazon_reviews[df_amazon_reviews[\"class\"]==\"positive\"][:5]\n",
    "df_negative = df_amazon_reviews[df_amazon_reviews[\"class\"]==\"negative\"][:5]\n",
    "df_amazon_mini = pd.concat([df_positive,df_negative])\n",
    "df_amazon_mini"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words:** um exemplo simples, sem usar embeddings, é a representação em bag of words, **já discutido aqui**. Assim, podemos  usar a classe `BagOfWords` que está no arquivo `textual_representation.py`. Para as representações bag of words, usaremos a função bag_of_words abaixo. Usando esta representação o nosso dataset ficaria representado da seguinte forma: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>70</th>\n",
       "      <th>absolute</th>\n",
       "      <th>actors</th>\n",
       "      <th>ahead</th>\n",
       "      <th>album</th>\n",
       "      <th>...</th>\n",
       "      <th>wooden</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>worse</th>\n",
       "      <th>worst</th>\n",
       "      <th>wow</th>\n",
       "      <th>wrenching</th>\n",
       "      <th>written</th>\n",
       "      <th>years</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.231984</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119602</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273879</td>\n",
       "      <td>0.13694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              13        16        18        20        21        70  absolute  \\\n",
       "id                                                                             \n",
       "208138  0.115992  0.231984  0.115992  0.115992  0.115992  0.115992  0.115992   \n",
       "157010  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "101657  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "49225   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "158265  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "204215  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "274316  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "57708   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "200048  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "60933   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          actors     ahead     album  ...    wooden   working     world  \\\n",
       "id                                    ...                                 \n",
       "208138  0.000000  0.115992  0.115992  ...  0.000000  0.000000  0.000000   \n",
       "157010  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.239204   \n",
       "101657  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "49225   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "158265  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "204215  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "274316  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "57708   0.129047  0.000000  0.000000  ...  0.129047  0.000000  0.000000   \n",
       "200048  0.000000  0.000000  0.000000  ...  0.000000  0.223607  0.000000   \n",
       "60933   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "           worse     worst      wow  wrenching   written     years     class  \n",
       "id                                                                            \n",
       "208138  0.000000  0.000000  0.00000   0.115992  0.000000  0.000000  positive  \n",
       "157010  0.000000  0.000000  0.00000   0.000000  0.000000  0.119602  positive  \n",
       "101657  0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  positive  \n",
       "49225   0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  positive  \n",
       "158265  0.000000  0.000000  0.00000   0.000000  0.245462  0.000000  positive  \n",
       "204215  0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "274316  0.283463  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "57708   0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "200048  0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "60933   0.000000  0.273879  0.13694   0.000000  0.000000  0.000000  negative  \n",
       "\n",
       "[10 rows x 250 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.textual_representation import BagOfWords\n",
    "#o vocabulario, quando vazio, será considerado todas as palavra (menos stopwords)\n",
    "def bag_of_words(data, vocabulary=None):\n",
    "    #obtem stopwords\n",
    "    stop_words = set()\n",
    "    with open(\"datasets/stopwords.txt\") as stop_file:\n",
    "        stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "    #instancia o bag of words, filtrando stopwords e considerando o vocabulario (se possivel)\n",
    "    bow = BagOfWords(\"bow\", stop_words=list(stop_words), words_to_consider=vocabulary)\n",
    "    \n",
    "    #o bag of words, é gerado separadamente a representação do treino e teste\n",
    "    #iremos usar apenas a representação considerando que \"data\" é o treino\n",
    "    data_preproc = bow.preprocess_train_dataset(data, \"class\")\n",
    "\n",
    "    #exibe apenas colunas não zedadas\n",
    "    m2 = (data_preproc != 0).any()\n",
    "    data_preproc = data_preproc[m2.index[m2].tolist()]\n",
    "    \n",
    "    return data_preproc\n",
    "bag_of_words(df_amazon_mini)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of words (filtrado por palavras chaves e embeddings similares)** Como bag of words é uma representação com milhares de atributos, poderiamos fazer uma restrição por palavras chaves. Por exemplo, caso usássemos como vocabulário do bag of words baseado nas palavras obtidas da roda de emoções proposta por [Scherer K., (2005)](https://journals.sagepub.com/doi/pdf/10.1177/0539018405058216): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'euphoria, temper, enjoy, comfortable, disgust, tear, jittery, buoyancy, contentment, acknowledgement, abhor, sick, dislike, fury, disdain, angry, pride, remorse, animation, anger, sadness, interest, enthusiasm, resent, confident, amazed, respect, furious, curious, depreciate, optimistic, guilt, anxiety, embarrassing, thunderstruck, abashed, satisfaction, proud, recognition, mad, hope, disrelish, denigration, scorn, dejected, joy, rage, elation, wrath, shame, happiness, exhilarating, tedious, faith, blame, anguish, relief, sad, dumbfounded, ardor, hostile, bliss, ennui, hopeless, nausea, contempt, exaltation, melancholy, gloom, indifference, delight, derision, ecstatic, boredom, worry, chagrin, cheer, ashamed, humiliating, nervous, astonishing, surprise, happy, contrition, aversion, incense, infuriating, apprehensive, alert'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_words = {\n",
    "                    \"pride\":{\"proud\"},\n",
    "                    \"elation\":{\"ecstatic\", \"euphoria\", \"exaltation\", \"exhilarating\"},\n",
    "                    \"happiness\":{\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"},\n",
    "                    \"satisfaction\":{\"comfortable\",\"contentment\"},\n",
    "                    \"relief\":{},\n",
    "                    \"hope\":{\"buoyancy\", \"confident\", \"faith\", \"optimistic\"},\n",
    "                    \"interest\":{\"alert\", \"animation\", \"ardor\", \"curious\",\"enthusiasm\"},\n",
    "                    \"surprise\":{\"amazed\", \"astonishing\", \"dumbfounded\",\"thunderstruck\"},\n",
    "                    \"anxiety\":{\"anguish\",\"anxiety\",\"apprehensive\",\"jittery\",\"nervous\",\"worry\"},\n",
    "                    \"sadness\":{\"chagrin\", \"dejected\", \"gloom\", \"hopeless\", \"melancholy\", \"sad\", \"tear\"},\n",
    "                    \"boredom\":{\"ennui\",\"indifference\",\"tedious\"},\n",
    "                    \"shame\":{\"abashed\", \"ashamed\", \"embarrassing\", \"humiliating\"},\n",
    "                    \"guilt\":{\"blame\", \"contrition\", \"remorse\"},\n",
    "                    \"disgust\":{\"abhor\", \"aversion\", \"dislike\", \"disrelish\", \"nausea\",\"sick\"},\n",
    "                    \"contempt\":{\"denigration\",\"depreciate\",\"derision\",\"disdain\",\"scorn\"},\n",
    "                    \"hostile\":{},\n",
    "                    \"anger\":{\"anger\",\"angry\",\"furious\",\"fury\",\"incense\",\"infuriating\",\n",
    "                                \"mad\",\"rage\",\"resent\",\"temper\",\"wrath\"},\n",
    "                    \"recognition\":{\"respect\",\"acknowledgement\"}\n",
    "            }\n",
    "\n",
    "vocabulary = []\n",
    "for emotion_group, set_keywords in emotion_words.items():\n",
    "    vocabulary.append(emotion_group)\n",
    "    for word in set_keywords:\n",
    "        vocabulary.append(word)\n",
    "vocabulary = set(vocabulary)\n",
    "\", \".join(vocabulary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O grande problema é que esse grupo de palavras é muito restrito. Veja como ficou a representação dos nossos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interest</th>\n",
       "      <th>sick</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        interest  sick     class\n",
       "id                              \n",
       "208138       0.0   0.0  positive\n",
       "157010       1.0   0.0  positive\n",
       "101657       0.0   0.0  positive\n",
       "49225        0.0   0.0  positive\n",
       "158265       0.0   0.0  positive\n",
       "204215       0.0   0.0  negative\n",
       "274316       0.0   0.0  negative\n",
       "57708        0.0   1.0  negative\n",
       "200048       0.0   0.0  negative\n",
       "60933        0.0   0.0  negative"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(df_amazon_mini,vocabulary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se que eliminamos as palavras que não apareceram em nenhuma instancia. Assim, como pode-se observar, apenas duas palavras foram usadas e alguns documentos não possuiam nenhuma palavra. Para ampliar o vocabulário, poderiamos expandir esta representação usando palavras similares a estas de acordo com o nosso embedding: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n"
     ]
    }
   ],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\")\n",
    "kdtree_embedding = KDTreeEmbedding(dict_embedding, \"kdt_en.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expanded = []\n",
    "for word in vocabulary:\n",
    "    #obtem as 40 mais similares palavras de cada uma do vocab original\n",
    "    _,words = kdtree_embedding.get_most_similar_embedding(word,40)\n",
    "    vocabulary_expanded.extend(words)\n",
    "vocabulary_expanded = set(vocabulary_expanded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja aqui as palavras usadas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"shields, warlike, enjoy, reticence, unsurprisingly, divine, inexplicable, demonstrate, sticks, crowds, burn, humaneness, jolting, pictures, unconvincing, antagonistic, hug, sassacus, sitting, grief, alerting, naborsind, infused, applauding, emotionalism, believers, cue, admired, turn, engaging, detractors, guiltily, didacticism, judgment, dreadful, satisfied, perfect, angered, rheology, animation, shelter, frankincense, hesitancy, indulged, embarrassed, outraged, buoyant, firing, warn, montanans, candle, meant, bravado, hopeful, nor, antsy, bullish, topside, suggesting, thoughtlessness, languor, acknowledgment, cgi, kalamity, rallying, confusion, str95bb, uncomfortable, propriety, powerless, untruth, motivation, certain, convinced, hope, sociability, familiarity, leery, anticipated, encumber, definitely, prosecuted, attacking, disenchantment, gripped, cannons, underwhelmed, scented, raise, tankage, implication, onslaught, decry, awkward, alarm, evacuation, bizarre, demeaning, ennui, rebelliousness, unfriendly, pellets, unsettling, termed, animators, dissatisfaction, contrary, reassurances, resultant, melancholy, headache, derision, constrain, sullen, needs, seeming, goofiness, encouragement, unhappiness, worse, disheartened, warnings, despises, disperse, evident, unserious, organgn, detest, burners, ill-founded, happy, going, threats, incense, repentance, broken, sleeplessness, kid, arguing, ingratitude, ungenerous, glory, passion, shouting, unnerving, lg03, respects, inconsolable, perplexity, drubbing, moral, rabbit, outspokenness, tempered, dislike, earnestness, disgruntled, angry, antigravity, appalled, merriment, timeless, malevolent, likening, sensuous, bittersweet, sadness, sensational, excruciatingly, cautiousness, destitution, amazement, believes, signifying, minions, disorientation, spend, ordeal, doubt, lending, morose, skittish, js03, graciousness, flustered, …, unfathomable, irritation, tiredness, cat, appreciate, thunderstruck, pranked, fervor, emasculation, copal, hesitant, price, raising, concerned, principle, concision, mtow, carnality, depreciating, denigration, reproach, ascribe, prideful, raucous, maddening, keep, jealousy, immediate, uplifting, rashes, immediately, heartless, rubber, horrendous, storyboard, fool, unsure, similarly, sinicization, catcalls, patient, visibly, surely, egomania, exuberant, heartbreaking, marvelous, wacky, zeal, withering, pretty, chagrin, villainy, ridicule, contend, providing, perceive, surveillance, enjoyable, hardly, disaster, aversion, disgusting, paradoxically, immortality, yearn, precocity, comfortable, defeats, avoid, resentment, overwrought, raised, js04bb, guileless, clamor, irked, wonderment, warning, startling, fend, glib, grumble, increasing, cuteness, wearying, engage, opportunity, falseness, jaundice, apathy, remorse, implying, overconfident, doubles_biggio, achieved, understand, uncharacteristic, esteem, http://www.nifc.gov/, blames, visuals, inspiration, voluptuousness, censer, amazed, cheerleaders, want, skepticism, covetousness, doom, blurbed, inflexibility, savage, loveliness, fascinated, darkening, excercise, hysterical, shock, unpleasant, hatred, laborious, admiring, callousness, mania, tihg, relentless, shortness, status, http://www.opel.com, befuddling, upbeat, squeamish, 'd, idiocy, response, disillusionment, bitterness, scare, liking, astonishment, bb94, riot, recognizing, appreciation, directness, bullet, omission, longing, impressive, devalue, headaches, fiery, ingenuous, relief, dehydration, displeasing, distressing, cramping, chore, patterson, manner, ardor, insanity, aggressive, livid, disney, misery, bdb94, frustrating, outbursts, hopeless, evoked, menacing, sprayed, astounded, fascination, understandably, lyricism, creations, puzzlement, engagement, intellectualism, leniency, bruising, ornaments, comfy, individualization, alabamians, unflustered, astonishing, disturbing, here, skyrocket, contrition, outsiders, retribution, ignored, constricted, confronted, sending, crispness, anxieties, fit, enchanting, foolish, admonition, engendering, skillz, putting, timelessness, unseemly, mournful, _____________________________________________, interactive, 're, elated, complaining, sick, supportive, ornery, patronize, ignore, paranoia, fury, fears, stun, exclamations, intense, incensed, captivating, david.lazarus@latimes.com, desire, detestation, miffed, solipsism, ..., jubilant, soothing, brashness, remembered, sympathetic, circularity, exhausting, marvellous, anticipation, legitimacy, ignorance, endearing, oly-2004-fhockey, dreamworks, protestors, pains, expressing, evocation, antipathy, resents, wearisome, wondrous, needed, regret, apologize, recognition, easy, mystified, unprompted, felicitously, migraines, cathartic, antics, stress, camphor, tired, howls, recognize, regardless, ire, marveled, exhilarating, malevolence, tedious, politeness, disappointed, aback, 24aou94, helplessness, joyless, honor, upon, formulaic, shout, protesters, restlessness, bafflement, frivolity, regards, desperation, enraging, contempt, shellshocked, certainly, vengeance, aches, ambition, delight, bemused, dizzying, take, sweating, kudos, engaged, uninitiated, irritability, fear, nowadays, petulance, find, generalise, maelstrom, live-action, importance, worried, photography, recovery, glum, elaboration, fluctuate, torturous, soon, revenge, emotion, trouble, canonicus, wrong, dog, rehabilitation, over-produced, bangkokians, misunderstand, jittery, resenting, nostalgia, brooding, invigorating, whimsy, rioters, cry, brought, gph04bb, thermoregulation, equal, deepens, brightly, resented, ponderous, overconfidence, deliriously, frenzied, crystallise, proclivity, greet, treachery, nerves, bb96, undeniable, embittered, approbation, surprisingly, reflected, disgraceful, wistful, vengeful, script, diminishes, calling, temperament, unquestionable, cowardice, remorseful, displeased, giving, equanimity, imprimatur, beguiling, comfortably, cow, anyway, knowing, offering, crankiness, shameful, feelings, incredibly, indestructibility, unconcerned, mistaken, hopefulness, animator, .0342, danger, animations, bb97, diffident, frustrated, unhappy, uproar, symbolizes, astonished, elation, unleashes, shame, fearlessness, rejoicing, unworldly, understanding, flamboyance, enjoying, unbothered, apologise, giddy, passions, biodegrade, permanence, quick, occasion, unlikely, aghast, nightmare, outcry, stop-motion, www.slarmy.org, indignant, menorah, kd96, investor, nowhere, action-packed, delirious, irksome, awestruck, likely, gloomy, come, cheer, provided, fixated, denunciations, wicked, apparent, blamed, crowd, always, clumsy, predilection, fahnt, eagerness, protesting, sparklers, support, revulsion, whatever, pointless, stressing, impressed, conviction, perceived, perplexed, affectation, religion, counter, buoyancy, eager, .0163, aggressiveness, nuttiness, allied, piyanart, condescension, surprised, pleasurably, revelations, concur, scared, convoluted, repudiate, fearful, throwing, dispersing, reflexively, tantrum, dejection, brutalization, interest, ambuscade, loved, startled, actions, clamoring, cinematic, nonplused, depreciate, hypnotised, filmmakers, distrust, ugly, babies, unironic, better, moreover, acquiescence, insisted, celebrating, bhagya, frustration, delighted, came, vulnerability, desultory, disregard, cautiously, invoking, christ, frantic, weird, behest, anguished, sympathize, lopsided, appreciative, presume, coldness, signal, accustomed, honored, bewilderment, rollicking, monster, unassertive, despondency, adore, ridiculousness, unease, vituperation, nevertheless, suited, insulted, swoon, http://www.mediabynumbers.com, optimism, hopelessness, irrepressible, storytelling, help, bliss, delights, mayhem, inviting, 65stk, caring, belief, haunting, jaded, fulfillment, impression, thrilled, engrossing, productions, expect, sluggishness, nasty, depressed, flabbergasted, ecstatic, elicited, truly, phenomenal, terrible, overdressed, earthiness, fretted, amorality, expecting, mortifying, thanks, spared, rootlessness, look, propensity, wishing, helping, boisterous, worship, apotheosis, vega@globe.com, particular, cackles, depredations, film, pleasantness, sentimentality, gobsmacked, troubling, uncouth, obsessed, liken, rebuilding, looking, unexcited, continue, fervour, reassure, sympathy, predicament, brominated, epically, denouncement, potpourri, engendered, true, homesickness, continuing, silliness, disrespected, http://www.oklahomacitynationalmemorial.org, accomplished, exploding, doomed, sorrow, betrayal, consternation, why, humiliated, furious, aid, cannon, poignancy, indication, forgiveness, stylish, dismayed, dumbstruck, borrowing, hopes, seductive, acetylene, impress, lethargy, humility, afraid, discomfort, bad, abashed, mind, disrespect, crazy, twitchy, entranced, triumphalism, disastrous, quietude, introspective, lest, coloradans, unrelenting, hilarity, feel, suggestion, hubbub, pleading, unexpected, mesmerizing, ferocious, weariness, dullness, bloodlust, claiming, offended, reluctance, hostile, interesting, depreciates, denunciation, bulletinyyy, lechery, asserting, time-consuming, aware, puppetry, nausea, threat, dampened, firecrackers, distraction, merciless, rescue, canisters, depressing, enraged, diarrhea, dismaying, hurry, privilege, constricting, disorienting, unsatisfying, creativity, joke, dreams, disgusted, reverie, undeterred, kd97, forcefulness, intemperate, unfairness, monotonous, appreciating, bedraggled, .0208, realize, obstinate, dreamy, decent, rise, helpless, miserable, mo95, enlightening, uninteresting, townsfolk, predicting, protectiveness, cheerfulness, antagonism, addition, neuroses, js94bb, torches, hauteur, gentility, think, accuse, mockery, batons, destitute, bullishness, fondness, loathing, nervousness, spooked, success, reprisal, simple-minded, benefit, symbolized, insouciance, uniqueness, edginess, preoccupation, pervading, perfidy, invective, undecipherable, defiance, blunt, disbelief, bloating, remarkable, burned, patriotism, envious, acknowledging, apologies, publicized, dislikes, determination, rw95, airiness, fatalism, clearly, fluidity, re-visited, heightened, determined, legacy, popi, greatness, wo, unfazed, angst, incredible, breathtaking, prosecution, votive, entrancing, compensator, repetitious, sloppiness, vigilance, agitate, fierce, wanderlust, terrified, migraine, reverence, sanctimony, cautioned, faith, surliness, dazzling, greenness, uncertainty, editing, surreal, excellence, mellow, soothed, rudeness, pity, mortified, peppermints, firestorm, effervescence, alienation, dizziness, reconsideration, creative, indifferent, nastiness, ashamed, overact, inconsequential, adventurousness, intimacy, humiliating, fading, uninterested, surprise, unsatisfied, celebrate, indignation, way, slovenliness, bemoan, clueless, awkwardness, dismay, pitiful, vomiting, disillusion, hipness, sprinkled, cheers, aimlessness, lollipops, perfectionism, frightening, treat, diarrhoea, unleashed, awful, reconstruction, viewed, stupidity, abhor, survivability, symptoms, pride, compassion, masterful, liveliness, although, washingtonians, insularity, filmmaking, admire, furthermore, rather, bombast, impatient, idleness, farcical, reticent, affronted, cartoon, frazzled, humbling, recognizes, downcast, ironically, devotion, dispiriting, curious, calamitous, abhorred, sake, emergency, insipid, menace, wondering, simply, higher, acknowledge, ignominious, admiration, predictably, downsize, dishonesty, 30-270, protection, alerts, alerted, frenzy, chafe, humble, mims, recklessness, giddiness, rousing, despairing, drowsiness, disappointment, fxff, enjoyed, shrewdness, disconsolate, rage, visual, enthused, wanted, electrifying, penchant, establishment, assure, exclusivism, misunderstood, gyroscopic, empathy, scripts, uselessness, riveting, suggests, drudgery, prompts, tendency, jingoism, nonetheless, adulation, instream, spray, anticipating, explode, serenity, disgrace, sandalwood, deployed, pleasantly, trivialize, shamed, plaudits, flame, unfortunately, nosebleeds, loath, hoping, inscrutable, flummoxed, oddness, mystifying, physicality, healthy, de-emphasize, pleased, booing, candles, put, explain, interestingly, cleverness, provide, nervous, liberality, childlike, wretched, alarms, self-pity, em96, erratic, euphoria, thrown, despite, stressed, ironic, chagrined, staying, scorned, tradition, perturb, kd94, exoneration, pathetic, insufferable, wishes, eventful, distasteful, lit, wish, focus, looked, contribution, 'm, vwahr, disheartening, selfishness, revelatory, everybody, vitriol, timidity, regretful, awesome, luck, arrogance, nonplussed, impotent, heartlessness, messiness, explosiveness, sickness, remember, joyful, propitiation, turning, embarrassment, biotechtrst, omnipresence, achievements, merits, frankly, palpitations, unleash, .000088, significance, demented, skylarking, evildoers, embarrassing, payback, .0170, fergalicious, upsetting, illness, lust, idiosyncrasy, satisfaction, inscrutably, greg.wilcoxdailynews.com, spellbinding, southpaws, spirited, intrigued, good, radiance, reminded, tranquillity, joy, panicky, thrilling, cautious, dramatic, thoughtfulness, cries, forlorn, looks, clarity, everyone, academicism, discombobulated, expressed, parents, thus, presumption, magnificent, feeling, unnerved, neediness, pronouncement, theatrical, lucidity, reassured, burst, orderliness, gratitude, suffering, goal_montreal, hatter, gone, given, hydrodynamic, mistake, spirit, sadly, baffled, assistance, risible, deplore, value, sight, hell, soulfulness, cheered, frightened, 'll, benumbed, alert, vruhl, depreciated, noting, accepted, euphoric, reason, mercy, strongly, crazed, inadequacy, odd, comforted, spirituality, stones, heartache, compounded, x.xx.xx.xx.x, amazing, incredulous, unfocused, reaffirmation, responding, confess, redemption, acasta, thank, wisdom, buying, depth-charged, overjoyed, purifier, cumbersome, discomfiting, noticed, enjoyment, flatten, resent, irony, fortunate, confident, thrill, guffaws, ought, god, solemnity, perspire, panicked, constipation, yearning, abrade, joyous, distaste, boring, srivalo, astounding, shouts, selflessness, pregnant, maybe, coarseness, bringing, doldrums, video, candor, sudden, believing, importantly, postmodernists, inclusion, inquisitive, loathe, eloquence, grandiosity, assist, feature, useless, snobbery, jubilation, solid, participation, scandalous, starving, heartwarming, blame, preoccupied, sad, sentimental, oly-2004-tennis, believer, mishandle, symbolize, retardants, exhilaration, transcendence, goodness, blaming, melancholic, .0202, unbelievable, cared, glad, embrace, know, transference, studio, stunned, groggy, ill-informed, competence, fearing, decisiveness, furor, amused, else, beliefs, committment, absurdity, reflecting, affected, prompting, folks, obsession, self-monitoring, thankful, prospects, uneasiness, equated, temper, overabundance, rp-1, dehumanization, letting, uncertain, exasperated, detests, trepidation, acknowledgement, grace, fevers, gleeful, benignly, recognised, irate, error-prone, indigestion, qualms, disdain, yet, inattention, hesitation, bring, superlatives, contrarians, craving, believe, munificence, surprising, lifeline, say, unprepared, oblivious, calumny, abandon, youthfulness, action, hysteria, retaliation, loneliness, stupefied, mindful, testifying, intimidated, saw, quietness, exhortation, ethos, skeptical, conscience, summons, anxiety, relaxed, worrying, heartfelt, quite, getting, envy, inevitable, hurled, solitude, alarmed, fatigue, nice, leave, anti-clericalism, discouragement, sparks, visualise, sure, cushioned, angering, triumph, virtue, purposefulness, deflate, ignoring, demonization, infuriated, insensitivity, viciousness, shocking, anguish, debauchery, scary, dumbfounded, peeved, k587-1, gentleness, grenades, eschew, opposing, cruel, distracted, loving, lucky, ethereal, impishly, kroyts, heartburn, hoped, indifference, truth, tragic, referring, newborn, intimation, allusion, tumult, fascinating, cramps, vulnerable, annoyance, burning, enjoys, sorrowful, teargas, questioning, much, dubbed, precautions, degrading, dissapointed, lament, fabulousness, maneuverability, confidence, dispirited, boldness, amusing, peculiar, spraying, tellingly, devaluating, insinuation, beeswax, awe, cheering, commitment, firebombs, compelled, seething, stupendous, betrayed, tiresome, warned, lightheadedness, interpenetration, inability, distracting, hungry, monotony, searing, ominous, insistent, painstaking, exactitude, ardour, independency, pessimistic, admitting, devaluate, enthusiasm, keeping, usual, sensitivity, soulful, regard, curiously, playful, belated, pessimism, respect, myrrh, edgy, dispatched, studios, optimistic, 3-d, fact, multimedia, efforts, overanalyze, enigmatically, guilt, insomnia, surprises, nonchalance, mothers, bewildered, love, mesmerized, heft, expression, crestfallen, proud, k977-1, dread, innocence, mad, realization, treating, insolence, annoyed, idolize, essence, confused, profess, pathos, hypocritical, dejected, illumined, stoicism, mocked, steadiness, musculature, misfortune, malice, feistiness, amidst, magnanimity, ambivalent, manoeuvre, cacophony, tedium, self-identity, anticipate, otherwise, meekness, talky, ineptitude, kindling, stupefaction, pretentiousness, forget, disliking, worry, garlands, perfunctory, proclivities, thing, shaken, panache, demoralising, unamused, circumspect, swings, infuriating, disliked, thankless, grossness, canister, rates, speculation, expectation, shadows, friendliness, moment, excited, imperieuse, horrified, humanitarian, bemusement, distrusted, belligerent, excitement, need, sense, shamefaced, .0207, complain, strange, resisting, displeasure, ready, loyalty, mendacity, incomparable, homoeroticism, supplant, shocked, combativeness, slander, self-love, crunch, plenty, unambiguous, passivity, drohs, wonderful, distinction, deform, clumsiness, wariness, peacefulness, depress, obviously, arduous, haughtiness, fatigued, smugness, piety, counteract, heaped, k978-1, omniscience, distraught, give, plausibility, contingent, lobbing, indeed, castigation, exultant, opposed, predominance, ghastly, failure, watching, puzzled, saves_mrivera, achievement, rattled, wrath, tantrums, cheerily, rest, malaise, laziness, humiliation, concerns, calmed, grotesque, announcement, informality, cynicism, resentful, see, debase, dispersed, demonstrators, forsake, unfortunate, scornful, anxious, intemperance, welcome, plodding, coded, tears, agitated, affirmation, compensators, terrifying, theatricality, effort, infuriates, takeover, admirable, afflicted, recognising, internalize, apprehensive, regrets, disgust, throngs, precaution, coming, rejecting, soothe, tear, newfound, dying, recognized, discouraged, check, contentment, impatience, documentary, integrity, brave, doctrine, discontent, jumpy, scapegoating, bereft, penury, re-organise, genuine, tempted, attitude, exasperation, artfulness, puzzling, rebukes, respecting, pain, sanguine, ballast, devaluing, pleasure, incredulity, exasperating, messy, wake, films, palpable, discomfited, exuberance, stirred, hypocrisy, unforgettable, fretful, despise, disconcerted, frankness, immaturity, courtesy, irreverence, homeless, praise, affectations, complacent, despair, vilification, weekend, wafted, sorry, longevity, advocation, appalling, provoked, moralize, denial, shyness, bullets, shrilly, panic, stunning, vigilant, .0206, apology, horrible, grateful, apologizing, mocks, catharsis, .000105, fatuous, euoplocephalus, embarassing, happiness, sorrowfully, complained, reassuring, inconstancy, itching, madness, tearing, censers, licentiousness, open-mindedness, though, neither, geotagging, raisonnable, ripoffs, prefer, strangeness, assurance, gloom, torment, sort, elderly, jitters, boredom, enamored, suspenseful, expectations, lanterns, emotions, curses, realizing, exhilarated, vivacity, peevish, stubbornness, deprogrammed, overlong, befuddled, wondered, restless, staggering, hostility, outrage, hand-drawn, baffling, speechless, romanticize, defensiveness, 28aou94, current, prompted, depletes, horrific, evocative, tiring, injustice, fantastic, beast, spurred, dignity, wonder, complacency, unimpressed, imagine, brokenhearted, chance, sublime, blandness, lambasting, ruminative, affection, frustrations, unconvinced, suspicion, exoticism, unsettled, fired, cowardliness, experience, threatening, vatten, playfulness, irritated, upset, anger, curiosity, applaud, equate, admit, freedom, poignant, str94, compliments, laughter, future, followed, rectitude, christian, fun, notice, concern, wary, irritating, perturbed, really, liberty, acceptance, enthralling, breathless, pig, ambivalence, elegiac, dishonour, heartbroken, promise, despondent, idealize, oftentimes, individuality, flood, clear, stranger, foreboding, demonstrates, lonely, debasement, lately, votives, culpability, delightful, scorn, signified, agony, weary, treated, libations, wanting, opprobrium, spite, celebration, stabiliser, despised, perfection, animated, seeing, beenz, stay, cushioning, latest, care, ferocity, insult, splendid, unremorseful, callous, besides, exaltation, abasement, insist, demoralizing, pensive, awed, greeted, suzuya, horrifying, explicitness, assured, stupid, warrant, claustrophobia, pixar, disinterest, ill, backlash, emphasized, attentiveness, sent\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(vocabulary_expanded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas palavras podem não estar relacionadas à emoção, porém, o método de aprendizado de máquina ainda é capaz de considerar palavras mais relevantes para uma determinada instancia, ignorando algum ruído. Veja como ficou a representação: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>canister</th>\n",
       "      <th>disappointed</th>\n",
       "      <th>experience</th>\n",
       "      <th>fact</th>\n",
       "      <th>fascinating</th>\n",
       "      <th>find</th>\n",
       "      <th>give</th>\n",
       "      <th>god</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>put</th>\n",
       "      <th>putting</th>\n",
       "      <th>sense</th>\n",
       "      <th>sick</th>\n",
       "      <th>sympathetic</th>\n",
       "      <th>true</th>\n",
       "      <th>video</th>\n",
       "      <th>wanting</th>\n",
       "      <th>worse</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490297</td>\n",
       "      <td>0.490297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.551556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474727</td>\n",
       "      <td>0.474727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             bad  canister  disappointed  experience      fact  fascinating  \\\n",
       "id                                                                            \n",
       "208138  0.000000  0.000000      0.000000    0.606043  0.000000     0.000000   \n",
       "157010  0.000000  0.000000      0.000000    0.000000  0.490297     0.490297   \n",
       "101657  0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "49225   0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "158265  0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "204215  0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "274316  0.606043  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "57708   0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "200048  0.000000  0.707107      0.707107    0.000000  0.000000     0.000000   \n",
       "60933   0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "\n",
       "        find      give  god      good  ...       put   putting     sense  \\\n",
       "id                                     ...                                 \n",
       "208138   0.0  0.606043  0.0  0.000000  ...  0.000000  0.515192  0.000000   \n",
       "157010   0.0  0.000000  0.0  0.324199  ...  0.416798  0.000000  0.000000   \n",
       "101657   0.0  0.000000  0.0  1.000000  ...  0.000000  0.000000  0.000000   \n",
       "49225    0.0  0.000000  0.0  0.551556  ...  0.000000  0.000000  0.000000   \n",
       "158265   0.0  0.000000  0.5  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "204215   0.0  0.000000  0.0  0.000000  ...  0.515192  0.000000  0.606043   \n",
       "274316   0.0  0.000000  0.0  0.000000  ...  0.000000  0.515192  0.000000   \n",
       "57708    0.5  0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "200048   0.0  0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "60933    0.0  0.000000  0.0  0.313903  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "        sick  sympathetic      true     video  wanting     worse     class  \n",
       "id                                                                          \n",
       "208138   0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "157010   0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "101657   0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "49225    0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "158265   0.0          0.0  0.000000  0.000000      0.5  0.000000  positive  \n",
       "204215   0.0          0.0  0.000000  0.000000      0.0  0.000000  negative  \n",
       "274316   0.0          0.0  0.000000  0.000000      0.0  0.606043  negative  \n",
       "57708    0.5          0.5  0.000000  0.000000      0.0  0.000000  negative  \n",
       "200048   0.0          0.0  0.000000  0.000000      0.0  0.000000  negative  \n",
       "60933    0.0          0.0  0.474727  0.474727      0.0  0.000000  negative  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(df_amazon_mini,vocabulary_expanded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poderiamos agrupar as palavras chaves em conceitos, por exemplo, \"happiness\" ser sempre contabilizado quando houver um conjunto de palavras, por exemplo, '\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"'. Porém, isso pode restringir muito o número de palavras e expandir com palavras usando embeddings, pode extrair palavras relacionadas com a emoção oposta (veja exemplo abaixo). Por isso, optamos por apresentar a representação usando bag of words. Mesmo assim, caso queira ver algum resultado dessa forma, a classe CountWords implementa expansão por grupos de palavras chaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"happy, feel, glad, sure, everyone, 'm, definitely, 'd, 'll, remember, everybody, wish, proud, 're, really, always, maybe, excited, good, lucky, obviously, thrilled, pleased, pretty, wonderful, know, afraid, delighted, looking, want, thing, imagine, think, unhappy, satisfied, realize, knowing, going, tired, crazy\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "distance, words = kdtree_embedding.get_most_similar_embedding(\"happy\",40)\n",
    "#veja que unhappy é relacionado com happy - além de outras palavras negativas e ruido\n",
    "\", \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pride</th>\n",
       "      <th>elation</th>\n",
       "      <th>happiness</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>relief</th>\n",
       "      <th>hope</th>\n",
       "      <th>interest</th>\n",
       "      <th>surprise</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>sadness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>shame</th>\n",
       "      <th>guilt</th>\n",
       "      <th>disgust</th>\n",
       "      <th>contempt</th>\n",
       "      <th>hostile</th>\n",
       "      <th>anger</th>\n",
       "      <th>recognition</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pride  elation  happiness  satisfaction  relief  hope  interest  \\\n",
       "208138      0        0         14             0       0     7         0   \n",
       "157010      0        0          5             0       0     5         2   \n",
       "101657      0        0          7             0       0     5         0   \n",
       "49225       0        0          4             0       0     3         0   \n",
       "158265      0        0          3             0       0     1         0   \n",
       "204215      0        0          0             0       0     1         0   \n",
       "274316      0        0          2             0       0     0         0   \n",
       "57708       0        0         10             0       0     7         0   \n",
       "200048      0        0          4             0       0     2         0   \n",
       "60933       0        0         12             0       0     2         0   \n",
       "\n",
       "        surprise  anxiety  sadness  boredom  shame  guilt  disgust  contempt  \\\n",
       "208138         0        4        0        0      0      0        0         0   \n",
       "157010         0        3        0        0      0      0        0         0   \n",
       "101657         0        0        0        0      0      0        0         0   \n",
       "49225          0        1        0        0      0      0        0         0   \n",
       "158265         0        1        0        0      0      0        0         0   \n",
       "204215         0        1        0        0      0      0        0         0   \n",
       "274316         0        0        0        0      0      0        0         0   \n",
       "57708          0        5        0        0      0      0        1         0   \n",
       "200048         0        0        0        0      0      0        0         0   \n",
       "60933          0       12        1        0      0      0        0         0   \n",
       "\n",
       "        hostile  anger  recognition     class  \n",
       "208138        0      0            0  positive  \n",
       "157010        0      0            0  positive  \n",
       "101657        0      0            1  positive  \n",
       "49225         0      0            0  positive  \n",
       "158265        0      0            0  positive  \n",
       "204215        0      0            0  negative  \n",
       "274316        0      0            0  negative  \n",
       "57708         0      0            0  negative  \n",
       "200048        0      0            0  negative  \n",
       "60933         0      0            0  negative  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.textual_representation import CountWords,InstanceWisePreprocess\n",
    "aggregate = CountWords(dict_embedding, emotion_words,max_distance=0.3)\n",
    "\n",
    "word_counter = InstanceWisePreprocess(\"word-counter\",aggregate)\n",
    "word_counter.preprocess_train_dataset(df_amazon_mini, \"class\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O max_distance é responsável por obter as palavras similares. Veja que diversos documentos negativos foram classificados com o grupo \"happiness\". "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representação agregando embeddings das palavras:** Conforme proposto por [Shen et al.](https://arxiv.org/pdf/1805.09843.pdf), dado que uma frase é representado por um conjunto de embeddings $\\{e_1, e_2, ..., e_n\\}$  uma forma simples e que geralmente obtém resultados **comparáveis a métodos mais complexos** é fazer operações em cada dimensão do embedding, tais como: média e máximo por dimensão do embedding. Por exemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'house', 'is', 'green']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embeddings de alguams palavras: \n",
    "dict_embedding = {'my':      [10, 11,14, 20, 15, 80],\n",
    "                  'house':   [11, 12,10, 24, 11, 30],\n",
    "                  'is':      [1,  3,  5, -1, 10, 20],\n",
    "                  'green':   [12,10, 20, 12, 10, 20]\n",
    "                   }\n",
    "#representação do texto \"my house is green\"\n",
    "arr_texto = \"my house is green\".split()\n",
    "arr_texto      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando a média de cada dimensão dos embeddings:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representação: [8.5, 9.0, 12.25, 13.75, 11.5, 37.5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def average_pooling(arr_texto, dim_embedding):\n",
    "    representacao = []\n",
    "    for i in range(dim_embedding):\n",
    "        #calcula a média da iésima posição do embedding\n",
    "        sum_pos = 0\n",
    "        for word in arr_texto:\n",
    "            sum_pos += dict_embedding[word][i]\n",
    "\n",
    "        representacao.append(sum_pos/len(arr_texto))\n",
    "    return representacao\n",
    "dim_embedding = 6\n",
    "representacao = average_pooling(arr_texto, dim_embedding)\n",
    "print(f\"Representação: {representacao}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando o máximo de cada dimensão dos embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representação: [12, 12, 20, 24, 15, 80]\n"
     ]
    }
   ],
   "source": [
    "dim_embedding = 6\n",
    "def max_pooling(arr_texto, dim_embedding):\n",
    "    representacao = []\n",
    "    for i in range(dim_embedding):\n",
    "        #calcula o valor máximo de cada iésima posição do embedding\n",
    "        first_word = arr_texto[0]\n",
    "        max_pos = dict_embedding[first_word][i]\n",
    "        for word in arr_texto[1:]:\n",
    "            if max_pos < dict_embedding[word][i]:\n",
    "                max_pos = dict_embedding[word][i]\n",
    "\n",
    "        representacao.append(max_pos)\n",
    "    return representacao\n",
    "representacao = max_pooling(arr_texto, dim_embedding)\n",
    "print(f\"Representação: {representacao}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como há palavras pouco relevantes (como stopwords) podemos remove-las e, também podemos utilizar apenas as palavras de um vocabulario controlado. Abaixo veja a representação. Como esta representação é vetorial, a mesma não é uma representação simples de ser entendida por humanos, porém, pode-se obter bons resultados. Você pode adicionar o vocabulario controlado ou as stopwords por meio dos parametros correpondentes. O parâmetro `aggregate_method` define se será feito um maximo ou média entre os embeddings colocando os valores `max` ou `avg`, respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>-0.059204</td>\n",
       "      <td>0.191895</td>\n",
       "      <td>0.372070</td>\n",
       "      <td>0.126587</td>\n",
       "      <td>-0.102600</td>\n",
       "      <td>-0.024734</td>\n",
       "      <td>-0.340088</td>\n",
       "      <td>0.083252</td>\n",
       "      <td>-0.264160</td>\n",
       "      <td>-0.224731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258545</td>\n",
       "      <td>0.219971</td>\n",
       "      <td>-0.457031</td>\n",
       "      <td>-0.332764</td>\n",
       "      <td>-0.326172</td>\n",
       "      <td>-0.164795</td>\n",
       "      <td>-0.160400</td>\n",
       "      <td>0.378906</td>\n",
       "      <td>0.456543</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>-0.064270</td>\n",
       "      <td>0.268555</td>\n",
       "      <td>0.395020</td>\n",
       "      <td>-0.094360</td>\n",
       "      <td>-0.176514</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>-0.220093</td>\n",
       "      <td>-0.191528</td>\n",
       "      <td>-0.177979</td>\n",
       "      <td>-0.433350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135132</td>\n",
       "      <td>0.084656</td>\n",
       "      <td>-0.274170</td>\n",
       "      <td>-0.524414</td>\n",
       "      <td>-0.061218</td>\n",
       "      <td>-0.264160</td>\n",
       "      <td>-0.521973</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.541992</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>-0.030762</td>\n",
       "      <td>0.119934</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>-0.437012</td>\n",
       "      <td>-0.739258</td>\n",
       "      <td>-0.153442</td>\n",
       "      <td>0.081116</td>\n",
       "      <td>-0.385498</td>\n",
       "      <td>-0.687988</td>\n",
       "      <td>-0.416260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.440430</td>\n",
       "      <td>0.083313</td>\n",
       "      <td>0.200317</td>\n",
       "      <td>-0.754883</td>\n",
       "      <td>0.169189</td>\n",
       "      <td>-0.265625</td>\n",
       "      <td>-0.528809</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>1.065430</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>-0.127930</td>\n",
       "      <td>0.394287</td>\n",
       "      <td>0.441895</td>\n",
       "      <td>-0.243408</td>\n",
       "      <td>-0.411377</td>\n",
       "      <td>-0.300293</td>\n",
       "      <td>0.028076</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>-0.478516</td>\n",
       "      <td>-0.224731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.283691</td>\n",
       "      <td>0.092102</td>\n",
       "      <td>-0.050598</td>\n",
       "      <td>-0.695801</td>\n",
       "      <td>0.098633</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>-0.408447</td>\n",
       "      <td>0.257324</td>\n",
       "      <td>0.741699</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.449951</td>\n",
       "      <td>0.435547</td>\n",
       "      <td>0.288086</td>\n",
       "      <td>-0.087097</td>\n",
       "      <td>-0.048950</td>\n",
       "      <td>0.714844</td>\n",
       "      <td>-0.627441</td>\n",
       "      <td>-0.139526</td>\n",
       "      <td>0.037781</td>\n",
       "      <td>-0.392822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330322</td>\n",
       "      <td>0.201050</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>-0.448730</td>\n",
       "      <td>-0.454346</td>\n",
       "      <td>-0.360840</td>\n",
       "      <td>-0.517578</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.399902</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.040314</td>\n",
       "      <td>0.040253</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.346191</td>\n",
       "      <td>-0.253418</td>\n",
       "      <td>0.219482</td>\n",
       "      <td>-0.346924</td>\n",
       "      <td>-0.459229</td>\n",
       "      <td>-0.234009</td>\n",
       "      <td>-0.016251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>-0.037048</td>\n",
       "      <td>-0.417480</td>\n",
       "      <td>-0.152466</td>\n",
       "      <td>0.052826</td>\n",
       "      <td>-0.471924</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>0.480225</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.158936</td>\n",
       "      <td>-0.016418</td>\n",
       "      <td>0.793945</td>\n",
       "      <td>-0.266113</td>\n",
       "      <td>-0.669434</td>\n",
       "      <td>-0.004871</td>\n",
       "      <td>-0.398193</td>\n",
       "      <td>-0.331055</td>\n",
       "      <td>0.040588</td>\n",
       "      <td>-0.129272</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127441</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>-0.206665</td>\n",
       "      <td>-0.333496</td>\n",
       "      <td>0.056488</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.239014</td>\n",
       "      <td>0.317383</td>\n",
       "      <td>-0.004375</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.162231</td>\n",
       "      <td>0.439941</td>\n",
       "      <td>0.299561</td>\n",
       "      <td>-0.111328</td>\n",
       "      <td>-0.139404</td>\n",
       "      <td>0.153687</td>\n",
       "      <td>-0.460449</td>\n",
       "      <td>-0.149048</td>\n",
       "      <td>0.320801</td>\n",
       "      <td>-0.473389</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045044</td>\n",
       "      <td>0.411377</td>\n",
       "      <td>-0.177979</td>\n",
       "      <td>-0.549805</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>-0.155273</td>\n",
       "      <td>-0.111450</td>\n",
       "      <td>0.056763</td>\n",
       "      <td>0.045990</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>-0.154175</td>\n",
       "      <td>0.127686</td>\n",
       "      <td>0.332520</td>\n",
       "      <td>-0.328125</td>\n",
       "      <td>-0.582031</td>\n",
       "      <td>0.210815</td>\n",
       "      <td>0.188110</td>\n",
       "      <td>0.594238</td>\n",
       "      <td>0.397949</td>\n",
       "      <td>0.190674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.284912</td>\n",
       "      <td>-0.298096</td>\n",
       "      <td>-0.150879</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>0.336914</td>\n",
       "      <td>0.061096</td>\n",
       "      <td>0.478516</td>\n",
       "      <td>-0.390137</td>\n",
       "      <td>0.254883</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>-0.006786</td>\n",
       "      <td>0.071411</td>\n",
       "      <td>0.483154</td>\n",
       "      <td>-0.466309</td>\n",
       "      <td>-0.287598</td>\n",
       "      <td>0.137451</td>\n",
       "      <td>-0.036652</td>\n",
       "      <td>0.012230</td>\n",
       "      <td>-0.068115</td>\n",
       "      <td>-0.241211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056396</td>\n",
       "      <td>0.058685</td>\n",
       "      <td>0.099060</td>\n",
       "      <td>-0.135742</td>\n",
       "      <td>-0.311035</td>\n",
       "      <td>-0.453125</td>\n",
       "      <td>-0.349121</td>\n",
       "      <td>0.174072</td>\n",
       "      <td>0.460205</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "208138 -0.059204  0.191895  0.372070  0.126587 -0.102600 -0.024734 -0.340088   \n",
       "157010 -0.064270  0.268555  0.395020 -0.094360 -0.176514  0.011276 -0.220093   \n",
       "101657 -0.030762  0.119934  0.539062 -0.437012 -0.739258 -0.153442  0.081116   \n",
       "49225  -0.127930  0.394287  0.441895 -0.243408 -0.411377 -0.300293  0.028076   \n",
       "158265  0.449951  0.435547  0.288086 -0.087097 -0.048950  0.714844 -0.627441   \n",
       "204215  0.040314  0.040253  0.532227 -0.346191 -0.253418  0.219482 -0.346924   \n",
       "274316  0.158936 -0.016418  0.793945 -0.266113 -0.669434 -0.004871 -0.398193   \n",
       "57708   0.162231  0.439941  0.299561 -0.111328 -0.139404  0.153687 -0.460449   \n",
       "200048 -0.154175  0.127686  0.332520 -0.328125 -0.582031  0.210815  0.188110   \n",
       "60933  -0.006786  0.071411  0.483154 -0.466309 -0.287598  0.137451 -0.036652   \n",
       "\n",
       "               7         8         9  ...        91        92        93  \\\n",
       "208138  0.083252 -0.264160 -0.224731  ... -0.258545  0.219971 -0.457031   \n",
       "157010 -0.191528 -0.177979 -0.433350  ... -0.135132  0.084656 -0.274170   \n",
       "101657 -0.385498 -0.687988 -0.416260  ... -0.440430  0.083313  0.200317   \n",
       "49225  -0.054688 -0.478516 -0.224731  ... -0.283691  0.092102 -0.050598   \n",
       "158265 -0.139526  0.037781 -0.392822  ... -0.330322  0.201050  0.036438   \n",
       "204215 -0.459229 -0.234009 -0.016251  ... -0.179688  0.093750 -0.037048   \n",
       "274316 -0.331055  0.040588 -0.129272  ... -0.127441 -0.073242 -0.206665   \n",
       "57708  -0.149048  0.320801 -0.473389  ... -0.045044  0.411377 -0.177979   \n",
       "200048  0.594238  0.397949  0.190674  ... -0.284912 -0.298096 -0.150879   \n",
       "60933   0.012230 -0.068115 -0.241211  ...  0.056396  0.058685  0.099060   \n",
       "\n",
       "              94        95        96        97        98        99     class  \n",
       "208138 -0.332764 -0.326172 -0.164795 -0.160400  0.378906  0.456543  positive  \n",
       "157010 -0.524414 -0.061218 -0.264160 -0.521973  0.185547  0.541992  positive  \n",
       "101657 -0.754883  0.169189 -0.265625 -0.528809  0.175781  1.065430  positive  \n",
       "49225  -0.695801  0.098633  0.142578 -0.408447  0.257324  0.741699  positive  \n",
       "158265 -0.448730 -0.454346 -0.360840 -0.517578  0.019989  0.399902  positive  \n",
       "204215 -0.417480 -0.152466  0.052826 -0.471924  0.008827  0.480225  negative  \n",
       "274316 -0.333496  0.056488  0.071777  0.239014  0.317383 -0.004375  negative  \n",
       "57708  -0.549805 -0.000109 -0.155273 -0.111450  0.056763  0.045990  negative  \n",
       "200048  0.051758  0.336914  0.061096  0.478516 -0.390137  0.254883  negative  \n",
       "60933  -0.135742 -0.311035 -0.453125 -0.349121  0.174072  0.460205  negative  \n",
       "\n",
       "[10 rows x 101 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtem as stopwords\n",
    "stop_words = set()\n",
    "with open(\"datasets/stopwords.txt\") as stop_file:\n",
    "    stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "from embeddings.textual_representation import AggregateEmbeddings,InstanceWisePreprocess\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\")\n",
    "aggregate_keywords_exp = AggregateEmbeddings(dict_embedding, aggregate_method=\"avg\", \n",
    "                                            words_to_filter=stop_words, words_to_consider=vocabulary_expanded)\n",
    "emb_keywords_exp = InstanceWisePreprocess(\"emb_keywords_exp\",aggregate_keywords_exp)\n",
    "emb_keywords_exp.preprocess_train_dataset(df_amazon_mini, \"class\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação por meio de um método de aprendizado de máquina"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os embeddings podem oferecer uma informação de proximidade de conceitos que o uso de Bag of Words não seria capaz. Mesmo assim, cada representação e preprocessamento tem sua vantagem e desvantagem e não existe um método que será sempre o melhor. Assim, para sabermos qual representação é melhor para uma tarefa, é importante avaliarmos em quais delas são maiores para a tarefa em questão. Como o foco desta prática não é a avaliação, iremos apenas apresentar o resultado, caso queira, você pode [assistir a video aula](https://www.youtube.com/watch?v=Ag06UuWTsr4&list=PLwIaU1DGYV6tUx10fCTw5aPnqypbbK_GJ&index=12) e [fazer a prática sobre avaliação](https://github.com/daniel-hasan/ap-de-maquina-cefetmg-avaliacao/archive/master.zip). Nesta parte, iremos apenas usar a avaliação para verificar qual método é melhor.  \n",
    "\n",
    "Para que esta seção seja auto contida, iremos fazer toda a preparação que fizemos nas seções anteriores\n",
    "\n",
    "**Criação da lista de stopwords e de vocabulário:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n"
     ]
    }
   ],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "\n",
    "emotion_words = {\n",
    "                    \"pride\":{\"proud\"},\n",
    "                    \"elation\":{\"ecstatic\", \"euphoria\", \"exaltation\", \"exhilarating\"},#vs boredom\n",
    "                    \"happiness\":{\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"},#vs sad\n",
    "                    \"satisfaction\":{\"comfortable\",\"contentment\"},#\n",
    "                    \"relief\":{},\n",
    "                    \"hope\":{\"buoyancy\", \"confident\", \"faith\", \"optimistic\"},\n",
    "                    \"interest\":{\"alert\", \"animation\", \"ardor\", \"curious\",\"enthusiasm\"},\n",
    "                    \"surprise\":{\"amazed\", \"astonishing\", \"dumbfounded\",\"thunderstruck\"},\n",
    "                    \"anxiety\":{\"anguish\",\"anxiety\",\"apprehensive\",\"jittery\",\"nervous\",\"worry\"},\n",
    "                    \"sadness\":{\"chagrin\", \"dejected\", \"gloom\", \"hopeless\", \"melancholy\", \"sad\", \"tear\"},\n",
    "                    \"boredom\":{\"ennui\",\"indifference\",\"tedious\"},\n",
    "                    \"shame\":{\"abashed\", \"ashamed\", \"embarrassing\", \"humiliating\"},\n",
    "                    \"guilt\":{\"blame\", \"contrition\", \"remorse\"},\n",
    "                    \"disgust\":{\"abhor\", \"aversion\", \"dislike\", \"disrelish\", \"nausea\",\"sick\"},\n",
    "                    \"contempt\":{\"denigration\",\"depreciate\",\"derision\",\"disdain\",\"scorn\"},\n",
    "                    \"hostile\":{},\n",
    "                    \"anger\":{\"anger\",\"angry\",\"furious\",\"fury\",\"incense\",\"infuriating\",\n",
    "                                \"mad\",\"rage\",\"resent\",\"temper\",\"wrath\"},\n",
    "                    \"recognition\":{\"respect\",\"acknowledgement\"}\n",
    "            }\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\") \n",
    "kdtree_embedding = KDTreeEmbedding(dict_embedding, \"kdt_en.p\")\n",
    "\n",
    "#obtem as stopwords\n",
    "stop_words = set()\n",
    "with open(\"datasets/stopwords.txt\") as stop_file:\n",
    "    stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "\n",
    "#palavras chaves a serem consideradas\n",
    "set_vocabulary = set()\n",
    "for key_word, arr_related_words in emotion_words.items():\n",
    "    set_vocabulary.add(key_word)\n",
    "    set_vocabulary = set_vocabulary | set(arr_related_words)\n",
    "\n",
    "#kdtree - para gerar o conjunto com palavras chaves e suas similares\n",
    "vocabulary_expanded = []\n",
    "for word in set_vocabulary:\n",
    "    _, words = kdtree_embedding.get_most_similar_embedding(word,60)\n",
    "    vocabulary_expanded.extend(words)\n",
    "vocabulary_expanded = set(vocabulary_expanded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Representações usadas**: Iremos avaliar a filtragem de stopwords e usando um vocabulário restrito da representação bag of words e também da representação usando a média de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.textual_representation import BagOfWords, AggregateEmbeddings,InstanceWisePreprocess\n",
    "\n",
    "#gera as representações\n",
    "aggregate = AggregateEmbeddings(dict_embedding, \"avg\")\n",
    "embedding = InstanceWisePreprocess(\"embbeding\",aggregate)\n",
    "\n",
    "aggregate_stop = AggregateEmbeddings(dict_embedding, \"avg\",words_to_filter=stop_words)\n",
    "emb_nostop = InstanceWisePreprocess(\"emb_nostop\",aggregate_stop)\n",
    "\n",
    "\n",
    "aggregate_keywords_exp = AggregateEmbeddings(dict_embedding, \"avg\",words_to_consider=vocabulary_expanded)\n",
    "emb_keywords_exp = InstanceWisePreprocess(\"emb_keywords_exp\",aggregate_keywords_exp)\n",
    "\n",
    "stop_words_list = list(stop_words)\n",
    "bow_keywords = BagOfWords(\"bow_keywords_exp\", words_to_consider=vocabulary_expanded)\n",
    "bow = BagOfWords(\"bow\", stop_words=stop_words_list)\n",
    "\n",
    "arr_representations = [embedding,emb_nostop, emb_keywords_exp, bow,bow_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>Do NOT WASTE Your Time: This book, to put it n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>Peels the paint off the walls: I first heard t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>History With Modern Appeal: This is a must rea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>Worse Music cd ever: I tried putting this in a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>Deliberately Obtuse Nonsense: I don't know wha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29215</th>\n",
       "      <td>Better than the movie?: YES! This book gets be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256457</th>\n",
       "      <td>The Best RE yet: This is the best in the RE se...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210215</th>\n",
       "      <td>What are they waiting for?: This has got to be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200693</th>\n",
       "      <td>Hollywood - promoting the Antichrist again?: C...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169551</th>\n",
       "      <td>Best American TV Series Ever: With its combina...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     class\n",
       "id                                                                 \n",
       "204215  Do NOT WASTE Your Time: This book, to put it n...  negative\n",
       "208138  Peels the paint off the walls: I first heard t...  positive\n",
       "157010  History With Modern Appeal: This is a must rea...  positive\n",
       "274316  Worse Music cd ever: I tried putting this in a...  negative\n",
       "57708   Deliberately Obtuse Nonsense: I don't know wha...  negative\n",
       "...                                                   ...       ...\n",
       "29215   Better than the movie?: YES! This book gets be...  positive\n",
       "256457  The Best RE yet: This is the best in the RE se...  positive\n",
       "210215  What are they waiting for?: This has got to be...  positive\n",
       "200693  Hollywood - promoting the Antichrist again?: C...  negative\n",
       "169551  Best American TV Series Ever: With its combina...  positive\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"datasets/amazon_reviews_mini.txt\",index_col=\"id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, é executado um método de aprendizado  para cada representação. Esse processo pode demorar um pouco pois é feito a procura do melhor parametro do algoritmo. Algumas otimizações que talvez, você precise fazer é no arquivo `embedding/avaliacao_embedding.py` alterar o parametro `n_jobs` no método `obtem_metodo` da classe `OtimizacaoObjetivoRandomForest`. Esse parametro é responsável por utiizar mais threads ao executar o Random Forests.  O valor pode ser levemente inferior a quantidades de núcleos que seu computador tem, caso ele tenha mais de 2, caso contrário, o ideal é colocarmos `n_jobs=1`. Caso queira visualizar resultados mais rapidamente, diminua o valor da variável `num_trials` e `num_folds` abaixo. Atenção que `num_folds` deve ser um valor maior que um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Development\\Python\\Python 3.11.2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Representação: embbeding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 17:39:25,347]\u001b[0m A new study created in RDB with name: random_forest_embbeding_fold_0\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:39:35,421]\u001b[0m Trial 0 finished with value: 0.7026844411452852 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.7026844411452852.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:39:45,245]\u001b[0m Trial 1 finished with value: 0.7076808594215102 and parameters: {'min_samples_split': 7, 'max_features': 75, 'num_arvores': 30}. Best is trial 1 with value: 0.7076808594215102.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:39:55,374]\u001b[0m Trial 2 finished with value: 0.7257292199352072 and parameters: {'min_samples_split': 5, 'max_features': 80, 'num_arvores': 35}. Best is trial 2 with value: 0.7257292199352072.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:40:05,119]\u001b[0m Trial 3 finished with value: 0.7100922628235254 and parameters: {'min_samples_split': 11, 'max_features': 80, 'num_arvores': 45}. Best is trial 2 with value: 0.7257292199352072.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:40:14,994]\u001b[0m Trial 4 finished with value: 0.7093917088711424 and parameters: {'min_samples_split': 5, 'max_features': 100, 'num_arvores': 30}. Best is trial 2 with value: 0.7257292199352072.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:40:24,398]\u001b[0m Trial 5 finished with value: 0.6963224688291163 and parameters: {'min_samples_split': 15, 'max_features': 80, 'num_arvores': 40}. Best is trial 2 with value: 0.7257292199352072.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:40:34,908]\u001b[0m Trial 6 finished with value: 0.7187088565569706 and parameters: {'min_samples_split': 3, 'max_features': 75, 'num_arvores': 50}. Best is trial 2 with value: 0.7257292199352072.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:40:44,252]\u001b[0m Trial 7 finished with value: 0.683282053795201 and parameters: {'min_samples_split': 21, 'max_features': 80, 'num_arvores': 45}. Best is trial 2 with value: 0.7257292199352072.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:40:53,523]\u001b[0m Trial 8 finished with value: 0.6797143711243168 and parameters: {'min_samples_split': 19, 'max_features': 100, 'num_arvores': 30}. Best is trial 2 with value: 0.7257292199352072.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:41:04,167]\u001b[0m Trial 9 finished with value: 0.7340698717945805 and parameters: {'min_samples_split': 1, 'max_features': 75, 'num_arvores': 50}. Best is trial 9 with value: 0.7340698717945805.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:41:08,677]\u001b[0m A new study created in RDB with name: random_forest_embbeding_fold_1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7365612911831398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 17:41:19,216]\u001b[0m Trial 0 finished with value: 0.7222799954681726 and parameters: {'min_samples_split': 3, 'max_features': 80, 'num_arvores': 50}. Best is trial 0 with value: 0.7222799954681726.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:41:28,756]\u001b[0m Trial 1 finished with value: 0.7067929887823774 and parameters: {'min_samples_split': 11, 'max_features': 90, 'num_arvores': 35}. Best is trial 0 with value: 0.7222799954681726.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:41:38,134]\u001b[0m Trial 2 finished with value: 0.690268529085199 and parameters: {'min_samples_split': 15, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.7222799954681726.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:41:47,622]\u001b[0m Trial 3 finished with value: 0.6871506821302912 and parameters: {'min_samples_split': 17, 'max_features': 100, 'num_arvores': 45}. Best is trial 0 with value: 0.7222799954681726.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:41:57,332]\u001b[0m Trial 4 finished with value: 0.7135292832673573 and parameters: {'min_samples_split': 7, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.7222799954681726.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:42:07,084]\u001b[0m Trial 5 finished with value: 0.7052235262492168 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 35}. Best is trial 0 with value: 0.7222799954681726.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:42:16,622]\u001b[0m Trial 6 finished with value: 0.7065851206386559 and parameters: {'min_samples_split': 7, 'max_features': 70, 'num_arvores': 30}. Best is trial 0 with value: 0.7222799954681726.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:42:25,956]\u001b[0m Trial 7 finished with value: 0.6922181080599351 and parameters: {'min_samples_split': 15, 'max_features': 75, 'num_arvores': 35}. Best is trial 0 with value: 0.7222799954681726.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:42:35,444]\u001b[0m Trial 8 finished with value: 0.6993618840537401 and parameters: {'min_samples_split': 11, 'max_features': 70, 'num_arvores': 40}. Best is trial 0 with value: 0.7222799954681726.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:42:45,848]\u001b[0m Trial 9 finished with value: 0.7249207702806876 and parameters: {'min_samples_split': 3, 'max_features': 90, 'num_arvores': 45}. Best is trial 9 with value: 0.7249207702806876.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:42:50,323]\u001b[0m A new study created in RDB with name: random_forest_embbeding_fold_2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7465286656068304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 17:43:00,636]\u001b[0m Trial 0 finished with value: 0.7201785797844312 and parameters: {'min_samples_split': 3, 'max_features': 80, 'num_arvores': 45}. Best is trial 0 with value: 0.7201785797844312.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:43:10,336]\u001b[0m Trial 1 finished with value: 0.699776015116016 and parameters: {'min_samples_split': 9, 'max_features': 70, 'num_arvores': 40}. Best is trial 0 with value: 0.7201785797844312.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:43:19,947]\u001b[0m Trial 2 finished with value: 0.6856857677883778 and parameters: {'min_samples_split': 15, 'max_features': 85, 'num_arvores': 50}. Best is trial 0 with value: 0.7201785797844312.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:43:29,352]\u001b[0m Trial 3 finished with value: 0.6890500880502324 and parameters: {'min_samples_split': 13, 'max_features': 100, 'num_arvores': 30}. Best is trial 0 with value: 0.7201785797844312.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:43:39,628]\u001b[0m Trial 4 finished with value: 0.7089834273368224 and parameters: {'min_samples_split': 3, 'max_features': 95, 'num_arvores': 35}. Best is trial 0 with value: 0.7201785797844312.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:43:49,988]\u001b[0m Trial 5 finished with value: 0.7035298194891331 and parameters: {'min_samples_split': 3, 'max_features': 100, 'num_arvores': 35}. Best is trial 0 with value: 0.7201785797844312.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:43:59,618]\u001b[0m Trial 6 finished with value: 0.6756252600847198 and parameters: {'min_samples_split': 17, 'max_features': 95, 'num_arvores': 50}. Best is trial 0 with value: 0.7201785797844312.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:44:09,019]\u001b[0m Trial 7 finished with value: 0.6874229318362918 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 35}. Best is trial 0 with value: 0.7201785797844312.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:44:19,205]\u001b[0m Trial 8 finished with value: 0.7106546331499247 and parameters: {'min_samples_split': 5, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.7201785797844312.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:44:28,593]\u001b[0m Trial 9 finished with value: 0.6737470061432735 and parameters: {'min_samples_split': 21, 'max_features': 90, 'num_arvores': 45}. Best is trial 0 with value: 0.7201785797844312.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:44:32,977]\u001b[0m A new study created in RDB with name: random_forest_embbeding_fold_3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7179188307262616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 17:44:43,458]\u001b[0m Trial 0 finished with value: 0.7128944352036335 and parameters: {'min_samples_split': 3, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.7128944352036335.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:44:52,923]\u001b[0m Trial 1 finished with value: 0.6838556197407164 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 35}. Best is trial 0 with value: 0.7128944352036335.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:45:02,201]\u001b[0m Trial 2 finished with value: 0.6664411431054836 and parameters: {'min_samples_split': 19, 'max_features': 90, 'num_arvores': 30}. Best is trial 0 with value: 0.7128944352036335.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:45:11,563]\u001b[0m Trial 3 finished with value: 0.6867754288955733 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 40}. Best is trial 0 with value: 0.7128944352036335.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:45:21,323]\u001b[0m Trial 4 finished with value: 0.6826937455495649 and parameters: {'min_samples_split': 19, 'max_features': 80, 'num_arvores': 50}. Best is trial 0 with value: 0.7128944352036335.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:45:30,853]\u001b[0m Trial 5 finished with value: 0.6913993256782258 and parameters: {'min_samples_split': 13, 'max_features': 70, 'num_arvores': 50}. Best is trial 0 with value: 0.7128944352036335.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:45:40,254]\u001b[0m Trial 6 finished with value: 0.6808881122016931 and parameters: {'min_samples_split': 15, 'max_features': 100, 'num_arvores': 30}. Best is trial 0 with value: 0.7128944352036335.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:45:50,990]\u001b[0m Trial 7 finished with value: 0.7170388578278817 and parameters: {'min_samples_split': 3, 'max_features': 100, 'num_arvores': 45}. Best is trial 7 with value: 0.7170388578278817.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:46:01,778]\u001b[0m Trial 8 finished with value: 0.720388942067586 and parameters: {'min_samples_split': 1, 'max_features': 95, 'num_arvores': 45}. Best is trial 8 with value: 0.720388942067586.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:46:11,055]\u001b[0m Trial 9 finished with value: 0.6679346444551628 and parameters: {'min_samples_split': 21, 'max_features': 90, 'num_arvores': 30}. Best is trial 8 with value: 0.720388942067586.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:46:15,665]\u001b[0m A new study created in RDB with name: random_forest_embbeding_fold_4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7365232181965737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 17:46:25,370]\u001b[0m Trial 0 finished with value: 0.7207572854636092 and parameters: {'min_samples_split': 1, 'max_features': 70, 'num_arvores': 30}. Best is trial 0 with value: 0.7207572854636092.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:46:35,719]\u001b[0m Trial 1 finished with value: 0.7169367127513508 and parameters: {'min_samples_split': 5, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.7207572854636092.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:46:45,105]\u001b[0m Trial 2 finished with value: 0.6978286957673175 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.7207572854636092.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:46:55,307]\u001b[0m Trial 3 finished with value: 0.7102414464488311 and parameters: {'min_samples_split': 7, 'max_features': 90, 'num_arvores': 50}. Best is trial 0 with value: 0.7207572854636092.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:47:04,931]\u001b[0m Trial 4 finished with value: 0.6957899906211091 and parameters: {'min_samples_split': 13, 'max_features': 70, 'num_arvores': 50}. Best is trial 0 with value: 0.7207572854636092.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:47:14,981]\u001b[0m Trial 5 finished with value: 0.7231783051848016 and parameters: {'min_samples_split': 5, 'max_features': 95, 'num_arvores': 35}. Best is trial 5 with value: 0.7231783051848016.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:47:24,441]\u001b[0m Trial 6 finished with value: 0.6828795406777354 and parameters: {'min_samples_split': 19, 'max_features': 95, 'num_arvores': 40}. Best is trial 5 with value: 0.7231783051848016.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:47:34,136]\u001b[0m Trial 7 finished with value: 0.7113360468698035 and parameters: {'min_samples_split': 3, 'max_features': 70, 'num_arvores': 30}. Best is trial 5 with value: 0.7231783051848016.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:47:44,131]\u001b[0m Trial 8 finished with value: 0.7290150506880995 and parameters: {'min_samples_split': 1, 'max_features': 70, 'num_arvores': 35}. Best is trial 8 with value: 0.7290150506880995.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:47:53,424]\u001b[0m Trial 9 finished with value: 0.6886663297616725 and parameters: {'min_samples_split': 15, 'max_features': 85, 'num_arvores': 30}. Best is trial 8 with value: 0.7290150506880995.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:47:57,660]\u001b[0m A new study created in RDB with name: random_forest_emb_nostop_fold_0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6758563074352548\n",
      "Representação: embbeding concluida\n",
      "===== Representação: emb_nostop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 17:48:08,462]\u001b[0m Trial 0 finished with value: 0.7035810115270137 and parameters: {'min_samples_split': 1, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.7035810115270137.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:48:18,432]\u001b[0m Trial 1 finished with value: 0.6923730392066213 and parameters: {'min_samples_split': 5, 'max_features': 75, 'num_arvores': 45}. Best is trial 0 with value: 0.7035810115270137.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:48:28,810]\u001b[0m Trial 2 finished with value: 0.699876086501197 and parameters: {'min_samples_split': 5, 'max_features': 90, 'num_arvores': 50}. Best is trial 0 with value: 0.7035810115270137.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:48:38,320]\u001b[0m Trial 3 finished with value: 0.6611263153513383 and parameters: {'min_samples_split': 19, 'max_features': 75, 'num_arvores': 40}. Best is trial 0 with value: 0.7035810115270137.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:48:47,640]\u001b[0m Trial 4 finished with value: 0.6787875935943545 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.7035810115270137.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:48:57,667]\u001b[0m Trial 5 finished with value: 0.7064933309223935 and parameters: {'min_samples_split': 1, 'max_features': 70, 'num_arvores': 40}. Best is trial 5 with value: 0.7064933309223935.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:49:07,192]\u001b[0m Trial 6 finished with value: 0.6829459201480783 and parameters: {'min_samples_split': 13, 'max_features': 85, 'num_arvores': 35}. Best is trial 5 with value: 0.7064933309223935.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:49:16,462]\u001b[0m Trial 7 finished with value: 0.6613812646429658 and parameters: {'min_samples_split': 21, 'max_features': 90, 'num_arvores': 35}. Best is trial 5 with value: 0.7064933309223935.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:49:26,047]\u001b[0m Trial 8 finished with value: 0.681226771949658 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 45}. Best is trial 5 with value: 0.7064933309223935.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:49:36,112]\u001b[0m Trial 9 finished with value: 0.6949240986212261 and parameters: {'min_samples_split': 5, 'max_features': 70, 'num_arvores': 35}. Best is trial 5 with value: 0.7064933309223935.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:49:40,629]\u001b[0m A new study created in RDB with name: random_forest_emb_nostop_fold_1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72997299729973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 17:49:50,547]\u001b[0m Trial 0 finished with value: 0.6668600521873617 and parameters: {'min_samples_split': 13, 'max_features': 75, 'num_arvores': 45}. Best is trial 0 with value: 0.6668600521873617.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:50:01,303]\u001b[0m Trial 1 finished with value: 0.6895924356044895 and parameters: {'min_samples_split': 1, 'max_features': 75, 'num_arvores': 50}. Best is trial 1 with value: 0.6895924356044895.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:50:11,886]\u001b[0m Trial 2 finished with value: 0.6830954361582454 and parameters: {'min_samples_split': 5, 'max_features': 90, 'num_arvores': 40}. Best is trial 1 with value: 0.6895924356044895.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:50:21,456]\u001b[0m Trial 3 finished with value: 0.6438793581726391 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 30}. Best is trial 1 with value: 0.6895924356044895.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:50:31,457]\u001b[0m Trial 4 finished with value: 0.6619833593612074 and parameters: {'min_samples_split': 17, 'max_features': 95, 'num_arvores': 50}. Best is trial 1 with value: 0.6895924356044895.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:50:43,670]\u001b[0m Trial 5 finished with value: 0.6450606710584972 and parameters: {'min_samples_split': 21, 'max_features': 70, 'num_arvores': 35}. Best is trial 1 with value: 0.6895924356044895.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:50:56,249]\u001b[0m Trial 6 finished with value: 0.6682761245597723 and parameters: {'min_samples_split': 13, 'max_features': 100, 'num_arvores': 50}. Best is trial 1 with value: 0.6895924356044895.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:51:07,568]\u001b[0m Trial 7 finished with value: 0.6636614246245495 and parameters: {'min_samples_split': 13, 'max_features': 100, 'num_arvores': 45}. Best is trial 1 with value: 0.6895924356044895.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:51:18,539]\u001b[0m Trial 8 finished with value: 0.6741364407280238 and parameters: {'min_samples_split': 9, 'max_features': 85, 'num_arvores': 45}. Best is trial 1 with value: 0.6895924356044895.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:51:28,873]\u001b[0m Trial 9 finished with value: 0.6682761245597723 and parameters: {'min_samples_split': 13, 'max_features': 100, 'num_arvores': 50}. Best is trial 1 with value: 0.6895924356044895.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:51:33,712]\u001b[0m A new study created in RDB with name: random_forest_emb_nostop_fold_2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.719922200611281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 17:51:43,882]\u001b[0m Trial 0 finished with value: 0.6870832321689521 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 30}. Best is trial 0 with value: 0.6870832321689521.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:51:54,156]\u001b[0m Trial 1 finished with value: 0.6993184674584633 and parameters: {'min_samples_split': 3, 'max_features': 70, 'num_arvores': 40}. Best is trial 1 with value: 0.6993184674584633.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:52:05,725]\u001b[0m Trial 2 finished with value: 0.7083920652447411 and parameters: {'min_samples_split': 1, 'max_features': 100, 'num_arvores': 50}. Best is trial 2 with value: 0.7083920652447411.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:52:16,028]\u001b[0m Trial 3 finished with value: 0.7098044405057493 and parameters: {'min_samples_split': 1, 'max_features': 75, 'num_arvores': 35}. Best is trial 3 with value: 0.7098044405057493.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:52:26,710]\u001b[0m Trial 4 finished with value: 0.6934232419718857 and parameters: {'min_samples_split': 3, 'max_features': 95, 'num_arvores': 35}. Best is trial 3 with value: 0.7098044405057493.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:52:36,642]\u001b[0m Trial 5 finished with value: 0.6609563361840829 and parameters: {'min_samples_split': 21, 'max_features': 90, 'num_arvores': 50}. Best is trial 3 with value: 0.7098044405057493.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:52:46,372]\u001b[0m Trial 6 finished with value: 0.6588584581183431 and parameters: {'min_samples_split': 19, 'max_features': 100, 'num_arvores': 40}. Best is trial 3 with value: 0.7098044405057493.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:52:56,273]\u001b[0m Trial 7 finished with value: 0.6812735998906155 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 35}. Best is trial 3 with value: 0.7098044405057493.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:53:06,067]\u001b[0m Trial 8 finished with value: 0.6804426709520102 and parameters: {'min_samples_split': 11, 'max_features': 90, 'num_arvores': 30}. Best is trial 3 with value: 0.7098044405057493.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:53:15,939]\u001b[0m Trial 9 finished with value: 0.6790666195591929 and parameters: {'min_samples_split': 13, 'max_features': 85, 'num_arvores': 50}. Best is trial 3 with value: 0.7098044405057493.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:53:20,316]\u001b[0m A new study created in RDB with name: random_forest_emb_nostop_fold_3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7262986904616207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 17:53:30,545]\u001b[0m Trial 0 finished with value: 0.6915592329105215 and parameters: {'min_samples_split': 7, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.6915592329105215.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:53:41,119]\u001b[0m Trial 1 finished with value: 0.698244094726185 and parameters: {'min_samples_split': 5, 'max_features': 95, 'num_arvores': 45}. Best is trial 1 with value: 0.698244094726185.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:53:51,841]\u001b[0m Trial 2 finished with value: 0.6995291719150231 and parameters: {'min_samples_split': 1, 'max_features': 80, 'num_arvores': 45}. Best is trial 2 with value: 0.6995291719150231.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:54:01,359]\u001b[0m Trial 3 finished with value: 0.6732060612685578 and parameters: {'min_samples_split': 21, 'max_features': 70, 'num_arvores': 50}. Best is trial 2 with value: 0.6995291719150231.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:54:11,603]\u001b[0m Trial 4 finished with value: 0.6885211505534286 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 45}. Best is trial 2 with value: 0.6995291719150231.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:54:21,207]\u001b[0m Trial 5 finished with value: 0.6723267931348701 and parameters: {'min_samples_split': 19, 'max_features': 90, 'num_arvores': 45}. Best is trial 2 with value: 0.6995291719150231.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:54:31,507]\u001b[0m Trial 6 finished with value: 0.6936465262868516 and parameters: {'min_samples_split': 7, 'max_features': 90, 'num_arvores': 45}. Best is trial 2 with value: 0.6995291719150231.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:54:41,144]\u001b[0m Trial 7 finished with value: 0.6707060162423684 and parameters: {'min_samples_split': 19, 'max_features': 95, 'num_arvores': 45}. Best is trial 2 with value: 0.6995291719150231.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:54:50,688]\u001b[0m Trial 8 finished with value: 0.6682105582721224 and parameters: {'min_samples_split': 19, 'max_features': 80, 'num_arvores': 45}. Best is trial 2 with value: 0.6995291719150231.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:55:00,435]\u001b[0m Trial 9 finished with value: 0.6889900820997156 and parameters: {'min_samples_split': 9, 'max_features': 80, 'num_arvores': 40}. Best is trial 2 with value: 0.6995291719150231.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:55:05,072]\u001b[0m A new study created in RDB with name: random_forest_emb_nostop_fold_4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7307504703504982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 17:55:14,916]\u001b[0m Trial 0 finished with value: 0.696529841850048 and parameters: {'min_samples_split': 9, 'max_features': 80, 'num_arvores': 45}. Best is trial 0 with value: 0.696529841850048.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:55:25,067]\u001b[0m Trial 1 finished with value: 0.6916671380751621 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 45}. Best is trial 0 with value: 0.696529841850048.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:55:35,143]\u001b[0m Trial 2 finished with value: 0.7025148587120373 and parameters: {'min_samples_split': 5, 'max_features': 80, 'num_arvores': 35}. Best is trial 2 with value: 0.7025148587120373.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:55:44,848]\u001b[0m Trial 3 finished with value: 0.6741922141902327 and parameters: {'min_samples_split': 17, 'max_features': 100, 'num_arvores': 50}. Best is trial 2 with value: 0.7025148587120373.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:55:54,265]\u001b[0m Trial 4 finished with value: 0.684719664155771 and parameters: {'min_samples_split': 15, 'max_features': 75, 'num_arvores': 35}. Best is trial 2 with value: 0.7025148587120373.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:56:03,928]\u001b[0m Trial 5 finished with value: 0.6799551741624968 and parameters: {'min_samples_split': 19, 'max_features': 85, 'num_arvores': 50}. Best is trial 2 with value: 0.7025148587120373.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:56:13,597]\u001b[0m Trial 6 finished with value: 0.6782508724848649 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 40}. Best is trial 2 with value: 0.7025148587120373.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:56:23,076]\u001b[0m Trial 7 finished with value: 0.6813979146999402 and parameters: {'min_samples_split': 17, 'max_features': 85, 'num_arvores': 40}. Best is trial 2 with value: 0.7025148587120373.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:56:32,852]\u001b[0m Trial 8 finished with value: 0.7011614040154256 and parameters: {'min_samples_split': 7, 'max_features': 70, 'num_arvores': 35}. Best is trial 2 with value: 0.7025148587120373.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:56:43,270]\u001b[0m Trial 9 finished with value: 0.6975802181210055 and parameters: {'min_samples_split': 1, 'max_features': 100, 'num_arvores': 30}. Best is trial 2 with value: 0.7025148587120373.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:56:47,451]\u001b[0m A new study created in RDB with name: random_forest_emb_keywords_exp_fold_0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6773573392846695\n",
      "Representação: emb_nostop concluida\n",
      "===== Representação: emb_keywords_exp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 17:56:56,888]\u001b[0m Trial 0 finished with value: 0.6652260033037036 and parameters: {'min_samples_split': 17, 'max_features': 100, 'num_arvores': 45}. Best is trial 0 with value: 0.6652260033037036.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:57:06,256]\u001b[0m Trial 1 finished with value: 0.6795096383503066 and parameters: {'min_samples_split': 13, 'max_features': 75, 'num_arvores': 40}. Best is trial 1 with value: 0.6795096383503066.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:57:15,791]\u001b[0m Trial 2 finished with value: 0.6852179460116528 and parameters: {'min_samples_split': 7, 'max_features': 75, 'num_arvores': 40}. Best is trial 2 with value: 0.6852179460116528.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:57:25,852]\u001b[0m Trial 3 finished with value: 0.6774484495962461 and parameters: {'min_samples_split': 7, 'max_features': 100, 'num_arvores': 50}. Best is trial 2 with value: 0.6852179460116528.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:57:35,370]\u001b[0m Trial 4 finished with value: 0.6790786471412987 and parameters: {'min_samples_split': 5, 'max_features': 70, 'num_arvores': 30}. Best is trial 2 with value: 0.6852179460116528.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:57:44,756]\u001b[0m Trial 5 finished with value: 0.6766355161565851 and parameters: {'min_samples_split': 11, 'max_features': 95, 'num_arvores': 35}. Best is trial 2 with value: 0.6852179460116528.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:57:54,729]\u001b[0m Trial 6 finished with value: 0.6778445132719607 and parameters: {'min_samples_split': 5, 'max_features': 95, 'num_arvores': 40}. Best is trial 2 with value: 0.6852179460116528.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:58:04,086]\u001b[0m Trial 7 finished with value: 0.6777913603915361 and parameters: {'min_samples_split': 13, 'max_features': 75, 'num_arvores': 30}. Best is trial 2 with value: 0.6852179460116528.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:58:13,405]\u001b[0m Trial 8 finished with value: 0.6753842678302758 and parameters: {'min_samples_split': 11, 'max_features': 85, 'num_arvores': 30}. Best is trial 2 with value: 0.6852179460116528.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:58:22,860]\u001b[0m Trial 9 finished with value: 0.670404400935989 and parameters: {'min_samples_split': 13, 'max_features': 85, 'num_arvores': 45}. Best is trial 2 with value: 0.6852179460116528.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:58:26,863]\u001b[0m A new study created in RDB with name: random_forest_emb_keywords_exp_fold_1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7049008138846669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 17:58:37,048]\u001b[0m Trial 0 finished with value: 0.6806867867245728 and parameters: {'min_samples_split': 3, 'max_features': 95, 'num_arvores': 35}. Best is trial 0 with value: 0.6806867867245728.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:58:46,349]\u001b[0m Trial 1 finished with value: 0.67964007303619 and parameters: {'min_samples_split': 11, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.6806867867245728.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:58:56,566]\u001b[0m Trial 2 finished with value: 0.683882388988933 and parameters: {'min_samples_split': 7, 'max_features': 100, 'num_arvores': 50}. Best is trial 2 with value: 0.683882388988933.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:59:06,291]\u001b[0m Trial 3 finished with value: 0.6833677413958075 and parameters: {'min_samples_split': 11, 'max_features': 100, 'num_arvores': 50}. Best is trial 2 with value: 0.683882388988933.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:59:15,065]\u001b[0m Trial 4 finished with value: 0.6662400242056802 and parameters: {'min_samples_split': 19, 'max_features': 95, 'num_arvores': 30}. Best is trial 2 with value: 0.683882388988933.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:59:24,190]\u001b[0m Trial 5 finished with value: 0.6747668105862795 and parameters: {'min_samples_split': 15, 'max_features': 90, 'num_arvores': 45}. Best is trial 2 with value: 0.683882388988933.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:59:33,453]\u001b[0m Trial 6 finished with value: 0.6742115182064582 and parameters: {'min_samples_split': 17, 'max_features': 70, 'num_arvores': 45}. Best is trial 2 with value: 0.683882388988933.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:59:42,653]\u001b[0m Trial 7 finished with value: 0.674774880619422 and parameters: {'min_samples_split': 17, 'max_features': 75, 'num_arvores': 35}. Best is trial 2 with value: 0.683882388988933.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 17:59:52,118]\u001b[0m Trial 8 finished with value: 0.6733959556063578 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 45}. Best is trial 2 with value: 0.683882388988933.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:00:02,059]\u001b[0m Trial 9 finished with value: 0.6790838009558461 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 50}. Best is trial 2 with value: 0.683882388988933.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:00:06,299]\u001b[0m A new study created in RDB with name: random_forest_emb_keywords_exp_fold_2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6731008717310087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:00:16,146]\u001b[0m Trial 0 finished with value: 0.6855340933669751 and parameters: {'min_samples_split': 7, 'max_features': 75, 'num_arvores': 50}. Best is trial 0 with value: 0.6855340933669751.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:00:26,076]\u001b[0m Trial 1 finished with value: 0.6781884329389741 and parameters: {'min_samples_split': 11, 'max_features': 100, 'num_arvores': 45}. Best is trial 0 with value: 0.6855340933669751.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:00:35,400]\u001b[0m Trial 2 finished with value: 0.6661687032324266 and parameters: {'min_samples_split': 21, 'max_features': 85, 'num_arvores': 40}. Best is trial 0 with value: 0.6855340933669751.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:00:45,051]\u001b[0m Trial 3 finished with value: 0.6923533469433679 and parameters: {'min_samples_split': 3, 'max_features': 80, 'num_arvores': 30}. Best is trial 3 with value: 0.6923533469433679.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:00:54,958]\u001b[0m Trial 4 finished with value: 0.6812503060618962 and parameters: {'min_samples_split': 3, 'max_features': 95, 'num_arvores': 30}. Best is trial 3 with value: 0.6923533469433679.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:01:04,863]\u001b[0m Trial 5 finished with value: 0.6847465042684714 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 45}. Best is trial 3 with value: 0.6923533469433679.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:01:14,179]\u001b[0m Trial 6 finished with value: 0.6804165235869705 and parameters: {'min_samples_split': 15, 'max_features': 75, 'num_arvores': 40}. Best is trial 3 with value: 0.6923533469433679.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:01:23,576]\u001b[0m Trial 7 finished with value: 0.6816255576110642 and parameters: {'min_samples_split': 13, 'max_features': 100, 'num_arvores': 35}. Best is trial 3 with value: 0.6923533469433679.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:01:33,504]\u001b[0m Trial 8 finished with value: 0.6908443462124394 and parameters: {'min_samples_split': 5, 'max_features': 75, 'num_arvores': 45}. Best is trial 3 with value: 0.6923533469433679.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:01:42,854]\u001b[0m Trial 9 finished with value: 0.6826564133252466 and parameters: {'min_samples_split': 11, 'max_features': 70, 'num_arvores': 40}. Best is trial 3 with value: 0.6923533469433679.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:01:46,870]\u001b[0m A new study created in RDB with name: random_forest_emb_keywords_exp_fold_3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6848030018761726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:01:56,773]\u001b[0m Trial 0 finished with value: 0.6799844225125128 and parameters: {'min_samples_split': 5, 'max_features': 95, 'num_arvores': 35}. Best is trial 0 with value: 0.6799844225125128.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:02:07,440]\u001b[0m Trial 1 finished with value: 0.679295171990479 and parameters: {'min_samples_split': 1, 'max_features': 90, 'num_arvores': 50}. Best is trial 0 with value: 0.6799844225125128.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:02:17,085]\u001b[0m Trial 2 finished with value: 0.6780320235279539 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 40}. Best is trial 0 with value: 0.6799844225125128.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:02:26,802]\u001b[0m Trial 3 finished with value: 0.6780604273521829 and parameters: {'min_samples_split': 11, 'max_features': 100, 'num_arvores': 50}. Best is trial 0 with value: 0.6799844225125128.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:02:37,170]\u001b[0m Trial 4 finished with value: 0.6760858958764899 and parameters: {'min_samples_split': 1, 'max_features': 90, 'num_arvores': 40}. Best is trial 0 with value: 0.6799844225125128.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:02:46,239]\u001b[0m Trial 5 finished with value: 0.6696740843312413 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 35}. Best is trial 0 with value: 0.6799844225125128.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:02:56,085]\u001b[0m Trial 6 finished with value: 0.67094592092861 and parameters: {'min_samples_split': 1, 'max_features': 85, 'num_arvores': 30}. Best is trial 0 with value: 0.6799844225125128.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:03:05,689]\u001b[0m Trial 7 finished with value: 0.6704456919698503 and parameters: {'min_samples_split': 13, 'max_features': 95, 'num_arvores': 45}. Best is trial 0 with value: 0.6799844225125128.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:03:15,021]\u001b[0m Trial 8 finished with value: 0.6689392984710145 and parameters: {'min_samples_split': 17, 'max_features': 80, 'num_arvores': 50}. Best is trial 0 with value: 0.6799844225125128.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:03:24,805]\u001b[0m Trial 9 finished with value: 0.680158653446646 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 45}. Best is trial 9 with value: 0.680158653446646.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:03:28,874]\u001b[0m A new study created in RDB with name: random_forest_emb_keywords_exp_fold_4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6946259167480164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:03:38,895]\u001b[0m Trial 0 finished with value: 0.6777129323854263 and parameters: {'min_samples_split': 3, 'max_features': 70, 'num_arvores': 30}. Best is trial 0 with value: 0.6777129323854263.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:03:48,769]\u001b[0m Trial 1 finished with value: 0.6662154103288196 and parameters: {'min_samples_split': 1, 'max_features': 85, 'num_arvores': 30}. Best is trial 0 with value: 0.6777129323854263.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:03:59,001]\u001b[0m Trial 2 finished with value: 0.6792114708255209 and parameters: {'min_samples_split': 1, 'max_features': 95, 'num_arvores': 35}. Best is trial 2 with value: 0.6792114708255209.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:04:08,358]\u001b[0m Trial 3 finished with value: 0.6749658523674725 and parameters: {'min_samples_split': 17, 'max_features': 85, 'num_arvores': 40}. Best is trial 2 with value: 0.6792114708255209.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:04:17,999]\u001b[0m Trial 4 finished with value: 0.6785677452904952 and parameters: {'min_samples_split': 11, 'max_features': 85, 'num_arvores': 50}. Best is trial 2 with value: 0.6792114708255209.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:04:27,469]\u001b[0m Trial 5 finished with value: 0.6784739955830412 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 30}. Best is trial 2 with value: 0.6792114708255209.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:04:36,835]\u001b[0m Trial 6 finished with value: 0.6770525881016106 and parameters: {'min_samples_split': 17, 'max_features': 70, 'num_arvores': 45}. Best is trial 2 with value: 0.6792114708255209.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:04:46,585]\u001b[0m Trial 7 finished with value: 0.6662154103288196 and parameters: {'min_samples_split': 1, 'max_features': 85, 'num_arvores': 30}. Best is trial 2 with value: 0.6792114708255209.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:04:55,910]\u001b[0m Trial 8 finished with value: 0.6693242247013416 and parameters: {'min_samples_split': 21, 'max_features': 100, 'num_arvores': 45}. Best is trial 2 with value: 0.6792114708255209.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:05:05,122]\u001b[0m Trial 9 finished with value: 0.6718047840167752 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 30}. Best is trial 2 with value: 0.6792114708255209.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:05:09,494]\u001b[0m A new study created in RDB with name: random_forest_bow_fold_0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6729699666295884\n",
      "Representação: emb_keywords_exp concluida\n",
      "===== Representação: bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:05:33,902]\u001b[0m Trial 0 finished with value: 0.7247956068075488 and parameters: {'min_samples_split': 5, 'max_features': 90, 'num_arvores': 50}. Best is trial 0 with value: 0.7247956068075488.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:05:47,359]\u001b[0m Trial 1 finished with value: 0.717542906137592 and parameters: {'min_samples_split': 21, 'max_features': 90, 'num_arvores': 45}. Best is trial 0 with value: 0.7247956068075488.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:05:57,731]\u001b[0m Trial 2 finished with value: 0.7151111151846973 and parameters: {'min_samples_split': 21, 'max_features': 80, 'num_arvores': 35}. Best is trial 0 with value: 0.7247956068075488.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:06:18,032]\u001b[0m Trial 3 finished with value: 0.7215331493111808 and parameters: {'min_samples_split': 3, 'max_features': 70, 'num_arvores': 50}. Best is trial 0 with value: 0.7247956068075488.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:06:39,197]\u001b[0m Trial 4 finished with value: 0.7211355632033896 and parameters: {'min_samples_split': 3, 'max_features': 75, 'num_arvores': 50}. Best is trial 0 with value: 0.7247956068075488.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:06:58,719]\u001b[0m Trial 5 finished with value: 0.7235334334889308 and parameters: {'min_samples_split': 7, 'max_features': 100, 'num_arvores': 35}. Best is trial 0 with value: 0.7247956068075488.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:07:18,794]\u001b[0m Trial 6 finished with value: 0.7227675141821676 and parameters: {'min_samples_split': 11, 'max_features': 80, 'num_arvores': 50}. Best is trial 0 with value: 0.7247956068075488.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:07:37,472]\u001b[0m Trial 7 finished with value: 0.7218719195896132 and parameters: {'min_samples_split': 9, 'max_features': 85, 'num_arvores': 40}. Best is trial 0 with value: 0.7247956068075488.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:07:48,467]\u001b[0m Trial 8 finished with value: 0.7179148252334966 and parameters: {'min_samples_split': 21, 'max_features': 70, 'num_arvores': 45}. Best is trial 0 with value: 0.7247956068075488.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:07:59,887]\u001b[0m Trial 9 finished with value: 0.7223092627571267 and parameters: {'min_samples_split': 19, 'max_features': 70, 'num_arvores': 40}. Best is trial 0 with value: 0.7247956068075488.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:08:20,165]\u001b[0m A new study created in RDB with name: random_forest_bow_fold_1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7532647957766045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:08:43,803]\u001b[0m Trial 0 finished with value: 0.7173646209294268 and parameters: {'min_samples_split': 7, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.7173646209294268.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:09:04,105]\u001b[0m Trial 1 finished with value: 0.716059762993861 and parameters: {'min_samples_split': 15, 'max_features': 85, 'num_arvores': 45}. Best is trial 0 with value: 0.7173646209294268.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:09:20,960]\u001b[0m Trial 2 finished with value: 0.7190069934700688 and parameters: {'min_samples_split': 7, 'max_features': 70, 'num_arvores': 40}. Best is trial 2 with value: 0.7190069934700688.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:09:45,524]\u001b[0m Trial 3 finished with value: 0.7188339204407607 and parameters: {'min_samples_split': 7, 'max_features': 85, 'num_arvores': 50}. Best is trial 2 with value: 0.7190069934700688.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:10:01,348]\u001b[0m Trial 4 finished with value: 0.7186624785814288 and parameters: {'min_samples_split': 9, 'max_features': 75, 'num_arvores': 35}. Best is trial 2 with value: 0.7190069934700688.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:10:19,525]\u001b[0m Trial 5 finished with value: 0.711430786891949 and parameters: {'min_samples_split': 21, 'max_features': 100, 'num_arvores': 50}. Best is trial 2 with value: 0.7190069934700688.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:10:29,179]\u001b[0m Trial 6 finished with value: 0.7085624223936478 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 30}. Best is trial 2 with value: 0.7190069934700688.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:10:44,235]\u001b[0m Trial 7 finished with value: 0.7172402234720919 and parameters: {'min_samples_split': 5, 'max_features': 80, 'num_arvores': 30}. Best is trial 2 with value: 0.7190069934700688.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:11:05,731]\u001b[0m Trial 8 finished with value: 0.7147919944838499 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 50}. Best is trial 2 with value: 0.7190069934700688.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:11:25,838]\u001b[0m Trial 9 finished with value: 0.7174317750269599 and parameters: {'min_samples_split': 5, 'max_features': 90, 'num_arvores': 35}. Best is trial 2 with value: 0.7190069934700688.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:11:39,897]\u001b[0m A new study created in RDB with name: random_forest_bow_fold_2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.761586533251454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:11:55,250]\u001b[0m Trial 0 finished with value: 0.7218422047172064 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 30}. Best is trial 0 with value: 0.7218422047172064.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:12:17,788]\u001b[0m Trial 1 finished with value: 0.7305138671431481 and parameters: {'min_samples_split': 5, 'max_features': 90, 'num_arvores': 40}. Best is trial 1 with value: 0.7305138671431481.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:12:35,817]\u001b[0m Trial 2 finished with value: 0.7230605970868892 and parameters: {'min_samples_split': 11, 'max_features': 75, 'num_arvores': 40}. Best is trial 1 with value: 0.7305138671431481.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:12:51,829]\u001b[0m Trial 3 finished with value: 0.7181162581830091 and parameters: {'min_samples_split': 21, 'max_features': 95, 'num_arvores': 40}. Best is trial 1 with value: 0.7305138671431481.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:13:08,475]\u001b[0m Trial 4 finished with value: 0.7182808152101735 and parameters: {'min_samples_split': 21, 'max_features': 100, 'num_arvores': 40}. Best is trial 1 with value: 0.7305138671431481.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:13:23,265]\u001b[0m Trial 5 finished with value: 0.7206452077048929 and parameters: {'min_samples_split': 1, 'max_features': 70, 'num_arvores': 30}. Best is trial 1 with value: 0.7305138671431481.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:13:47,254]\u001b[0m Trial 6 finished with value: 0.7292571958093442 and parameters: {'min_samples_split': 9, 'max_features': 80, 'num_arvores': 50}. Best is trial 1 with value: 0.7305138671431481.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:14:15,472]\u001b[0m Trial 7 finished with value: 0.7251372399527863 and parameters: {'min_samples_split': 3, 'max_features': 100, 'num_arvores': 45}. Best is trial 1 with value: 0.7305138671431481.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:14:34,601]\u001b[0m Trial 8 finished with value: 0.7175473018472974 and parameters: {'min_samples_split': 19, 'max_features': 90, 'num_arvores': 45}. Best is trial 1 with value: 0.7305138671431481.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:14:49,805]\u001b[0m Trial 9 finished with value: 0.7277202156521572 and parameters: {'min_samples_split': 11, 'max_features': 70, 'num_arvores': 35}. Best is trial 1 with value: 0.7305138671431481.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:15:07,757]\u001b[0m A new study created in RDB with name: random_forest_bow_fold_3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7262286324786325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:15:28,172]\u001b[0m Trial 0 finished with value: 0.7183464941980141 and parameters: {'min_samples_split': 11, 'max_features': 95, 'num_arvores': 40}. Best is trial 0 with value: 0.7183464941980141.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:15:40,804]\u001b[0m Trial 1 finished with value: 0.722207748011364 and parameters: {'min_samples_split': 11, 'max_features': 70, 'num_arvores': 30}. Best is trial 1 with value: 0.722207748011364.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:15:59,859]\u001b[0m Trial 2 finished with value: 0.7246662879118633 and parameters: {'min_samples_split': 17, 'max_features': 80, 'num_arvores': 50}. Best is trial 2 with value: 0.7246662879118633.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:16:18,964]\u001b[0m Trial 3 finished with value: 0.7321732660407093 and parameters: {'min_samples_split': 5, 'max_features': 80, 'num_arvores': 40}. Best is trial 3 with value: 0.7321732660407093.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:16:31,644]\u001b[0m Trial 4 finished with value: 0.7234001750304918 and parameters: {'min_samples_split': 17, 'max_features': 85, 'num_arvores': 30}. Best is trial 3 with value: 0.7321732660407093.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:16:53,216]\u001b[0m Trial 5 finished with value: 0.7241509321687646 and parameters: {'min_samples_split': 7, 'max_features': 85, 'num_arvores': 45}. Best is trial 3 with value: 0.7321732660407093.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:17:04,360]\u001b[0m Trial 6 finished with value: 0.7224790673622961 and parameters: {'min_samples_split': 17, 'max_features': 70, 'num_arvores': 30}. Best is trial 3 with value: 0.7321732660407093.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:17:19,773]\u001b[0m Trial 7 finished with value: 0.7177082231662429 and parameters: {'min_samples_split': 11, 'max_features': 95, 'num_arvores': 30}. Best is trial 3 with value: 0.7321732660407093.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:17:38,936]\u001b[0m Trial 8 finished with value: 0.7239456811594724 and parameters: {'min_samples_split': 15, 'max_features': 90, 'num_arvores': 40}. Best is trial 3 with value: 0.7321732660407093.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:17:49,648]\u001b[0m Trial 9 finished with value: 0.7169031648804811 and parameters: {'min_samples_split': 21, 'max_features': 90, 'num_arvores': 30}. Best is trial 3 with value: 0.7321732660407093.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:18:04,657]\u001b[0m A new study created in RDB with name: random_forest_bow_fold_4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7397107897664071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:18:30,080]\u001b[0m Trial 0 finished with value: 0.7106424104632438 and parameters: {'min_samples_split': 11, 'max_features': 100, 'num_arvores': 50}. Best is trial 0 with value: 0.7106424104632438.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:18:48,832]\u001b[0m Trial 1 finished with value: 0.7164378276426273 and parameters: {'min_samples_split': 15, 'max_features': 90, 'num_arvores': 40}. Best is trial 1 with value: 0.7164378276426273.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:19:10,840]\u001b[0m Trial 2 finished with value: 0.7193367995177958 and parameters: {'min_samples_split': 7, 'max_features': 85, 'num_arvores': 45}. Best is trial 2 with value: 0.7193367995177958.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:19:27,143]\u001b[0m Trial 3 finished with value: 0.7214907015445777 and parameters: {'min_samples_split': 7, 'max_features': 95, 'num_arvores': 30}. Best is trial 3 with value: 0.7214907015445777.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:19:44,233]\u001b[0m Trial 4 finished with value: 0.7139111169310549 and parameters: {'min_samples_split': 17, 'max_features': 80, 'num_arvores': 45}. Best is trial 3 with value: 0.7214907015445777.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:20:02,581]\u001b[0m Trial 5 finished with value: 0.7161294632454126 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 35}. Best is trial 3 with value: 0.7214907015445777.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:20:21,424]\u001b[0m Trial 6 finished with value: 0.7185555385876313 and parameters: {'min_samples_split': 5, 'max_features': 95, 'num_arvores': 35}. Best is trial 3 with value: 0.7214907015445777.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:20:37,280]\u001b[0m Trial 7 finished with value: 0.7144342378221372 and parameters: {'min_samples_split': 17, 'max_features': 95, 'num_arvores': 35}. Best is trial 3 with value: 0.7214907015445777.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:20:50,667]\u001b[0m Trial 8 finished with value: 0.7190458195875086 and parameters: {'min_samples_split': 5, 'max_features': 75, 'num_arvores': 30}. Best is trial 3 with value: 0.7214907015445777.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:03,079]\u001b[0m Trial 9 finished with value: 0.720640514014408 and parameters: {'min_samples_split': 9, 'max_features': 70, 'num_arvores': 30}. Best is trial 3 with value: 0.7214907015445777.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:15,663]\u001b[0m A new study created in RDB with name: random_forest_bow_keywords_exp_fold_0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7395833333333333\n",
      "Representação: bow concluida\n",
      "===== Representação: bow_keywords_exp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:21:18,675]\u001b[0m Trial 0 finished with value: 0.6905196639661385 and parameters: {'min_samples_split': 15, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.6905196639661385.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:21,350]\u001b[0m Trial 1 finished with value: 0.6974996406698596 and parameters: {'min_samples_split': 5, 'max_features': 75, 'num_arvores': 50}. Best is trial 1 with value: 0.6974996406698596.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:23,400]\u001b[0m Trial 2 finished with value: 0.6978588286510506 and parameters: {'min_samples_split': 15, 'max_features': 85, 'num_arvores': 35}. Best is trial 2 with value: 0.6978588286510506.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:25,701]\u001b[0m Trial 3 finished with value: 0.6893859238489545 and parameters: {'min_samples_split': 19, 'max_features': 85, 'num_arvores': 50}. Best is trial 2 with value: 0.6978588286510506.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:27,627]\u001b[0m Trial 4 finished with value: 0.6903898473686056 and parameters: {'min_samples_split': 15, 'max_features': 85, 'num_arvores': 30}. Best is trial 2 with value: 0.6978588286510506.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:29,385]\u001b[0m Trial 5 finished with value: 0.6958441579804567 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 35}. Best is trial 2 with value: 0.6978588286510506.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:31,396]\u001b[0m Trial 6 finished with value: 0.7033985927560202 and parameters: {'min_samples_split': 9, 'max_features': 80, 'num_arvores': 30}. Best is trial 6 with value: 0.7033985927560202.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:33,496]\u001b[0m Trial 7 finished with value: 0.6965502063560298 and parameters: {'min_samples_split': 17, 'max_features': 75, 'num_arvores': 45}. Best is trial 6 with value: 0.7033985927560202.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:35,578]\u001b[0m Trial 8 finished with value: 0.6965502063560298 and parameters: {'min_samples_split': 17, 'max_features': 75, 'num_arvores': 45}. Best is trial 6 with value: 0.7033985927560202.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:37,703]\u001b[0m Trial 9 finished with value: 0.6927340483038344 and parameters: {'min_samples_split': 17, 'max_features': 90, 'num_arvores': 40}. Best is trial 6 with value: 0.7033985927560202.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:38,694]\u001b[0m A new study created in RDB with name: random_forest_bow_keywords_exp_fold_1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6948092557848655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:21:42,008]\u001b[0m Trial 0 finished with value: 0.677025336928974 and parameters: {'min_samples_split': 7, 'max_features': 95, 'num_arvores': 50}. Best is trial 0 with value: 0.677025336928974.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:44,423]\u001b[0m Trial 1 finished with value: 0.6759559315305369 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.677025336928974.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:46,448]\u001b[0m Trial 2 finished with value: 0.675155896463877 and parameters: {'min_samples_split': 19, 'max_features': 75, 'num_arvores': 35}. Best is trial 0 with value: 0.677025336928974.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:49,259]\u001b[0m Trial 3 finished with value: 0.6772963584034807 and parameters: {'min_samples_split': 7, 'max_features': 70, 'num_arvores': 50}. Best is trial 3 with value: 0.6772963584034807.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:51,933]\u001b[0m Trial 4 finished with value: 0.6727880547109786 and parameters: {'min_samples_split': 9, 'max_features': 85, 'num_arvores': 40}. Best is trial 3 with value: 0.6772963584034807.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:54,159]\u001b[0m Trial 5 finished with value: 0.683196713283249 and parameters: {'min_samples_split': 3, 'max_features': 75, 'num_arvores': 30}. Best is trial 5 with value: 0.683196713283249.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:56,461]\u001b[0m Trial 6 finished with value: 0.6844844239882374 and parameters: {'min_samples_split': 11, 'max_features': 75, 'num_arvores': 35}. Best is trial 6 with value: 0.6844844239882374.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:21:58,943]\u001b[0m Trial 7 finished with value: 0.6770363409274817 and parameters: {'min_samples_split': 3, 'max_features': 95, 'num_arvores': 30}. Best is trial 6 with value: 0.6844844239882374.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:00,935]\u001b[0m Trial 8 finished with value: 0.6741164544238556 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 40}. Best is trial 6 with value: 0.6844844239882374.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:03,470]\u001b[0m Trial 9 finished with value: 0.6786867028979456 and parameters: {'min_samples_split': 1, 'max_features': 95, 'num_arvores': 30}. Best is trial 6 with value: 0.6844844239882374.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:04,735]\u001b[0m A new study created in RDB with name: random_forest_bow_keywords_exp_fold_2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6993452407465146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:22:07,323]\u001b[0m Trial 0 finished with value: 0.6820755349700213 and parameters: {'min_samples_split': 11, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.6820755349700213.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:10,180]\u001b[0m Trial 1 finished with value: 0.6777192789220211 and parameters: {'min_samples_split': 13, 'max_features': 75, 'num_arvores': 45}. Best is trial 0 with value: 0.6820755349700213.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:13,107]\u001b[0m Trial 2 finished with value: 0.6773108723454891 and parameters: {'min_samples_split': 15, 'max_features': 75, 'num_arvores': 50}. Best is trial 0 with value: 0.6820755349700213.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:16,608]\u001b[0m Trial 3 finished with value: 0.674375552193985 and parameters: {'min_samples_split': 1, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.6820755349700213.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:20,001]\u001b[0m Trial 4 finished with value: 0.6841127721679351 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 45}. Best is trial 4 with value: 0.6841127721679351.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:21,777]\u001b[0m Trial 5 finished with value: 0.6741490591099284 and parameters: {'min_samples_split': 21, 'max_features': 75, 'num_arvores': 30}. Best is trial 4 with value: 0.6841127721679351.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:24,584]\u001b[0m Trial 6 finished with value: 0.6722997869082045 and parameters: {'min_samples_split': 21, 'max_features': 100, 'num_arvores': 45}. Best is trial 4 with value: 0.6841127721679351.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:27,887]\u001b[0m Trial 7 finished with value: 0.6783149050823557 and parameters: {'min_samples_split': 11, 'max_features': 90, 'num_arvores': 45}. Best is trial 4 with value: 0.6841127721679351.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:31,295]\u001b[0m Trial 8 finished with value: 0.6841127721679351 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 45}. Best is trial 4 with value: 0.6841127721679351.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:34,312]\u001b[0m Trial 9 finished with value: 0.6701256281753117 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 45}. Best is trial 4 with value: 0.6841127721679351.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:36,384]\u001b[0m A new study created in RDB with name: random_forest_bow_keywords_exp_fold_3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6899138649624896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:22:40,183]\u001b[0m Trial 0 finished with value: 0.6887313628968622 and parameters: {'min_samples_split': 1, 'max_features': 95, 'num_arvores': 50}. Best is trial 0 with value: 0.6887313628968622.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:42,481]\u001b[0m Trial 1 finished with value: 0.6879407402761996 and parameters: {'min_samples_split': 5, 'max_features': 70, 'num_arvores': 35}. Best is trial 0 with value: 0.6887313628968622.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:45,966]\u001b[0m Trial 2 finished with value: 0.6849925920288288 and parameters: {'min_samples_split': 7, 'max_features': 100, 'num_arvores': 40}. Best is trial 0 with value: 0.6887313628968622.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:48,384]\u001b[0m Trial 3 finished with value: 0.6873895902446554 and parameters: {'min_samples_split': 7, 'max_features': 70, 'num_arvores': 35}. Best is trial 0 with value: 0.6887313628968622.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:51,151]\u001b[0m Trial 4 finished with value: 0.6894523935759859 and parameters: {'min_samples_split': 13, 'max_features': 80, 'num_arvores': 40}. Best is trial 4 with value: 0.6894523935759859.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:54,297]\u001b[0m Trial 5 finished with value: 0.6901227439726267 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 40}. Best is trial 5 with value: 0.6901227439726267.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:22:57,103]\u001b[0m Trial 6 finished with value: 0.6837707417886448 and parameters: {'min_samples_split': 7, 'max_features': 90, 'num_arvores': 35}. Best is trial 5 with value: 0.6901227439726267.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:00,078]\u001b[0m Trial 7 finished with value: 0.6930222323636611 and parameters: {'min_samples_split': 11, 'max_features': 80, 'num_arvores': 45}. Best is trial 7 with value: 0.6930222323636611.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:02,764]\u001b[0m Trial 8 finished with value: 0.6902105543294752 and parameters: {'min_samples_split': 19, 'max_features': 85, 'num_arvores': 45}. Best is trial 7 with value: 0.6930222323636611.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:05,224]\u001b[0m Trial 9 finished with value: 0.684513549519874 and parameters: {'min_samples_split': 11, 'max_features': 90, 'num_arvores': 30}. Best is trial 7 with value: 0.6930222323636611.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:07,144]\u001b[0m A new study created in RDB with name: random_forest_bow_keywords_exp_fold_4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6645891216740507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:23:10,536]\u001b[0m Trial 0 finished with value: 0.6811825503501995 and parameters: {'min_samples_split': 9, 'max_features': 100, 'num_arvores': 45}. Best is trial 0 with value: 0.6811825503501995.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:13,066]\u001b[0m Trial 1 finished with value: 0.6824244444025345 and parameters: {'min_samples_split': 17, 'max_features': 85, 'num_arvores': 40}. Best is trial 1 with value: 0.6824244444025345.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:15,150]\u001b[0m Trial 2 finished with value: 0.674725997003903 and parameters: {'min_samples_split': 19, 'max_features': 70, 'num_arvores': 40}. Best is trial 1 with value: 0.6824244444025345.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:17,035]\u001b[0m Trial 3 finished with value: 0.677772241434715 and parameters: {'min_samples_split': 21, 'max_features': 85, 'num_arvores': 30}. Best is trial 1 with value: 0.6824244444025345.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:19,317]\u001b[0m Trial 4 finished with value: 0.6787700786610996 and parameters: {'min_samples_split': 19, 'max_features': 75, 'num_arvores': 45}. Best is trial 1 with value: 0.6824244444025345.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:22,543]\u001b[0m Trial 5 finished with value: 0.6800168799317147 and parameters: {'min_samples_split': 11, 'max_features': 100, 'num_arvores': 45}. Best is trial 1 with value: 0.6824244444025345.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:24,951]\u001b[0m Trial 6 finished with value: 0.6794021984892478 and parameters: {'min_samples_split': 19, 'max_features': 70, 'num_arvores': 50}. Best is trial 1 with value: 0.6824244444025345.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:28,336]\u001b[0m Trial 7 finished with value: 0.67468951656152 and parameters: {'min_samples_split': 1, 'max_features': 80, 'num_arvores': 50}. Best is trial 1 with value: 0.6824244444025345.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:32,154]\u001b[0m Trial 8 finished with value: 0.6797643892446373 and parameters: {'min_samples_split': 1, 'max_features': 95, 'num_arvores': 50}. Best is trial 1 with value: 0.6824244444025345.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:35,156]\u001b[0m Trial 9 finished with value: 0.6785424794376279 and parameters: {'min_samples_split': 9, 'max_features': 75, 'num_arvores': 50}. Best is trial 1 with value: 0.6824244444025345.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6805797505400321\n",
      "Representação: bow_keywords_exp concluida\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from embeddings.avaliacao_embedding import calcula_experimento_representacao, OtimizacaoObjetivoRandomForest\n",
    "\n",
    "# Método de aprendizado de máquina a ser usado\n",
    "dict_metodo = {\"random_forest\":{\"classe_otimizacao\":OtimizacaoObjetivoRandomForest,\n",
    "                                \"sampler\":optuna.samplers.TPESampler(seed=1, n_startup_trials=10)},\n",
    "              }\n",
    "df_amazon_reviews = pd.read_csv(\"datasets/amazon_reviews_mini.txt\",index_col=\"id\")\n",
    "\n",
    "#executa experimento com a representacao determinada e o método\n",
    "for metodo, param_metodo in dict_metodo.items():\n",
    "    for representation in arr_representations:\n",
    "        print(f\"===== Representação: {representation.nome}\")\n",
    "        col_classe = \"class\"\n",
    "        num_folds = 5\n",
    "        num_folds_validacao = 3\n",
    "        num_trials = 10\n",
    "\n",
    "\n",
    "        nom_experimento = f\"{metodo}_\"+representation.nome\n",
    "        experimento = calcula_experimento_representacao(nom_experimento,representation,df_amazon_reviews,\n",
    "                                            col_classe,num_folds,num_folds_validacao,num_trials,\n",
    "                                            ClasseObjetivoOtimizacao=param_metodo['classe_otimizacao'],\n",
    "                                                sampler=param_metodo['sampler'])\n",
    "        print(f\"Representação: {representation.nome} concluida\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a experimentação é uma tarefa custosa, todos os resultados são salvos na pasta \"resultados\" - inclusive os valores dos parametros na classe optuna (a prática de avaliação apresenta mais detalhes da biblioteca Optuna). A macro f1 é uma métrica relacionada a taxa de acerto (se necessário, [veja a explicação neste video - tópico 2 e 3)](https://www.youtube.com/watch?v=u7o7CSeXaNs&list=PLwIaU1DGYV6tUx10fCTw5aPnqypbbK_GJ&index=13). Analise os resultados abaixo: qual representação foi melhor? A restrição de vocabulário ou eliminação de stopwords auxiliou? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 18:23:36,573]\u001b[0m Using an existing study with name 'random_forest_bow_fold_0' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:36,605]\u001b[0m Using an existing study with name 'random_forest_bow_fold_1' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:36,635]\u001b[0m Using an existing study with name 'random_forest_bow_fold_2' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:36,674]\u001b[0m Using an existing study with name 'random_forest_bow_fold_3' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:36,706]\u001b[0m Using an existing study with name 'random_forest_bow_fold_4' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:36,751]\u001b[0m Using an existing study with name 'random_forest_bow_keywords_exp_fold_0' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:36,781]\u001b[0m Using an existing study with name 'random_forest_bow_keywords_exp_fold_1' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:36,817]\u001b[0m Using an existing study with name 'random_forest_bow_keywords_exp_fold_2' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:36,845]\u001b[0m Using an existing study with name 'random_forest_bow_keywords_exp_fold_3' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:36,880]\u001b[0m Using an existing study with name 'random_forest_bow_keywords_exp_fold_4' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:36,919]\u001b[0m Using an existing study with name 'random_forest_embbeding_fold_0' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:36,949]\u001b[0m Using an existing study with name 'random_forest_embbeding_fold_1' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:36,978]\u001b[0m Using an existing study with name 'random_forest_embbeding_fold_2' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:37,009]\u001b[0m Using an existing study with name 'random_forest_embbeding_fold_3' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:37,041]\u001b[0m Using an existing study with name 'random_forest_embbeding_fold_4' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:37,085]\u001b[0m Using an existing study with name 'random_forest_emb_keywords_exp_fold_0' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:37,118]\u001b[0m Using an existing study with name 'random_forest_emb_keywords_exp_fold_1' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:37,145]\u001b[0m Using an existing study with name 'random_forest_emb_keywords_exp_fold_2' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:37,177]\u001b[0m Using an existing study with name 'random_forest_emb_keywords_exp_fold_3' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:37,204]\u001b[0m Using an existing study with name 'random_forest_emb_keywords_exp_fold_4' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:37,249]\u001b[0m Using an existing study with name 'random_forest_emb_nostop_fold_0' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:37,277]\u001b[0m Using an existing study with name 'random_forest_emb_nostop_fold_1' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:37,305]\u001b[0m Using an existing study with name 'random_forest_emb_nostop_fold_2' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:37,335]\u001b[0m Using an existing study with name 'random_forest_emb_nostop_fold_3' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-30 18:23:37,365]\u001b[0m Using an existing study with name 'random_forest_emb_nostop_fold_4' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nom_experimento</th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>f1-negative</th>\n",
       "      <th>precision-negative</th>\n",
       "      <th>recall-negative</th>\n",
       "      <th>f1-positive</th>\n",
       "      <th>precision-positive</th>\n",
       "      <th>recall-positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forest_bow</td>\n",
       "      <td>0.744075</td>\n",
       "      <td>0.747400</td>\n",
       "      <td>0.741480</td>\n",
       "      <td>0.753738</td>\n",
       "      <td>0.740750</td>\n",
       "      <td>0.747258</td>\n",
       "      <td>0.734700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_forest_bow_keywords_exp</td>\n",
       "      <td>0.685847</td>\n",
       "      <td>0.695220</td>\n",
       "      <td>0.679316</td>\n",
       "      <td>0.712214</td>\n",
       "      <td>0.676475</td>\n",
       "      <td>0.694038</td>\n",
       "      <td>0.660106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest_embbeding</td>\n",
       "      <td>0.722678</td>\n",
       "      <td>0.727223</td>\n",
       "      <td>0.719860</td>\n",
       "      <td>0.734882</td>\n",
       "      <td>0.718133</td>\n",
       "      <td>0.725953</td>\n",
       "      <td>0.710629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_forest_emb_keywords_exp</td>\n",
       "      <td>0.686080</td>\n",
       "      <td>0.694799</td>\n",
       "      <td>0.680590</td>\n",
       "      <td>0.710886</td>\n",
       "      <td>0.677361</td>\n",
       "      <td>0.694003</td>\n",
       "      <td>0.662761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random_forest_emb_nostop</td>\n",
       "      <td>0.716860</td>\n",
       "      <td>0.721150</td>\n",
       "      <td>0.715720</td>\n",
       "      <td>0.727707</td>\n",
       "      <td>0.712571</td>\n",
       "      <td>0.719894</td>\n",
       "      <td>0.706562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  nom_experimento  macro-f1  f1-negative  precision-negative  \\\n",
       "0               random_forest_bow  0.744075     0.747400            0.741480   \n",
       "1  random_forest_bow_keywords_exp  0.685847     0.695220            0.679316   \n",
       "2         random_forest_embbeding  0.722678     0.727223            0.719860   \n",
       "3  random_forest_emb_keywords_exp  0.686080     0.694799            0.680590   \n",
       "4        random_forest_emb_nostop  0.716860     0.721150            0.715720   \n",
       "\n",
       "   recall-negative  f1-positive  precision-positive  recall-positive  \n",
       "0         0.753738     0.740750            0.747258         0.734700  \n",
       "1         0.712214     0.676475            0.694038         0.660106  \n",
       "2         0.734882     0.718133            0.725953         0.710629  \n",
       "3         0.710886     0.677361            0.694003         0.662761  \n",
       "4         0.727707     0.712571            0.719894         0.706562  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from base_am.avaliacao import Experimento\n",
    "\n",
    "arr_resultado = []\n",
    "results_folder = \"resultados\"\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "for resultado_csv in os.listdir(\"resultados\"):\n",
    "    if resultado_csv.endswith(\"csv\"):\n",
    "        nom_experimento = resultado_csv.split(\".\")[0]\n",
    "        \n",
    "        #carrega resultados previamente realizados\n",
    "        experimento = Experimento(nom_experimento,[])\n",
    "        experimento.carrega_resultados_existentes()\n",
    "        \n",
    "        #adiciona experimento\n",
    "        num_folds = len(experimento.resultados)\n",
    "        dict_resultados = {\"nom_experimento\":nom_experimento, \n",
    "                            \"macro-f1\":sum([r.macro_f1 for r in experimento.resultados])/num_folds}\n",
    "        #resultados por classe\n",
    "        for classe in experimento.resultados[0].mat_confusao.keys():\n",
    "\n",
    "            dict_resultados[f\"f1-{classe}\"] = sum([r.f1_por_classe[classe] for r in experimento.resultados])/num_folds\n",
    "            dict_resultados[f\"precision-{classe}\"] = sum([r.precisao[classe] for r in experimento.resultados])/num_folds\n",
    "            dict_resultados[f\"recall-{classe}\"] = sum([r.revocacao[classe] for r in experimento.resultados])/num_folds\n",
    "\n",
    "        arr_resultado.append(dict_resultados)\n",
    "\n",
    "pd.DataFrame.from_dict(arr_resultado)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao analisar as métricas, principalmente a macro-f1, pode-se perceber que houve pouca variação nos resultados. Porém, mesmo com essa baixa variação, o que foi melhor resporesentado foi o `random_forest_bow`. Este resultado nos leva a pensar que a eliminação de stopwords e a restrição de vocabulário irá resultar em um melhor desempenho dependendo de cada caso, isto é, não é uma boa forma de garantir a melhora em qualquer cenário."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bolukbasi, T., Chang, K. W., Zou, J., Saligrama, V., & Kalai, A. (2016). **[Man is to computer programmer as woman is to homemaker? Debiasing word embeddings](https://arxiv.org/abs/1607.06520)**. \n",
    "\n",
    "Hartmann, N., Fonseca, E., Shulby, C., Treviso, M., Rodrigues, J., & Aluisio, S. (2017). [**Portuguese word embeddings: Evaluating on word analogies and natural language tasks.**](https://arxiv.org/abs/1708.06025)\n",
    "\n",
    "\n",
    "Pennington, J., Socher, R., & Manning, C. D. (2014, October).**[GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)**. In EMNLP 2015 \n",
    "\n",
    "\n",
    "Scherer, Klaus R. **[What are emotions? And how can they be measured?](https://journals.sagepub.com/doi/pdf/10.1177/0539018405058216)**. Social science information, v. 44, n. 4, p. 695-729, 2005.\n",
    "\n",
    "Shen, D., Wang, G., Wang, W., Min, M. R., Su, Q., Zhang, Y., Carin, L. (2018). [Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms](https://arxiv.org/pdf/1805.09843.pdf).\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Licença Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />Este obra está licenciado com uma Licença <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Atribuição-CompartilhaIgual 4.0 Internacional</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
