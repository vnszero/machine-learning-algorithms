{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta prática foi baseada no Capítulo 4 do livro \"Deep Learning with Python\" (Chollet F., 2017). Nela iremos estudar arquiteturas de Redes Neurais Profundas para identificar gatos e cachorros em fotos. Esse dataset foi uma amostra extraida do [desafio do Kaggle](colab.research.google.com). O objetivo desta prática é:\n",
    "- demonstrar como é possível usar Redes Neurais Convolucionarias (CNN)  mesmo em um dataset pequeno;\n",
    "- conhecer o framework Keras;\n",
    "- apresentar, por meio de mini-batches, como usar de forma eficiente a memória quando possuimos um dataset com milhares de atributos e, consequentemente, consumiria muita memória principal;\n",
    "- demonstrar a vantagem da CNN ao comparar  com uma Rede Neural Totalmente Conectada (FC, do inglês _Fully Connected Network_) quando a entrada é uma imagem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A limitação dessa prática é que assumimos que a imagem corresponde a apenas um animal (ou vários sendo apenas cachorros ou apenas gatos). Em um problema maior, deveriamos identificar objetos na imagem, para, logo após, realizarmos cortes na imagem e classificamos cada objeto identificado. \n",
    "\n",
    "Esta prática não possui testes unitários mas, para cada atividade, apresento que é esperado da mesma (ou como testar seu funcionamento). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para entregar: ** Caso você tenha usado o google colab para algo, coloque o link. Elimine as imagens do dataset e os arquivos de modelos treinados (.h5). Não esqueça de executar todas os códigos para que eu consiga avaliar todas as saídas - não será considerado código que não foi executado e salvo a saída da execução no Jupyter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração da prática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, você deverá instalar o Keras e o TensorFlow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip3 install keras, tensorflow, pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notei que em alguns computadores que não possuem GPU, deve-se instalar o `tensorflow-cpu` ao invés do tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from cats_vs_dogs import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import array_to_img\n",
    "from keras import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja a inicialização da classe `Constantes` e da `ParametrosRedeNeural` no arquivo `cats_vs_dogs.py`. Você precisará desses valores ao longo da prática. Veja que lá, passamos o tipo de otimização usando classes do módulo [optimizers](https://keras.io/optimizers/) do Keras. Dentro desse objeto que definimos a taxa de aprendizado. No otimizador RMSProp, o parametro $\\beta$ que usamos na nossa aula teórica é o parâmetro `rho` da inicialização da classe `RMSProp`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notação utilizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $m$: quantidade de instâncias;\n",
    "- $t$: tamanho do mini-batch;\n",
    "- $w$: largura da imagem. Toda imagem possuirá o mesmo tamanho;\n",
    "- $h$: altura da imagem;\n",
    "- $c$: número de canais da imagem (ex. 1 canal: escala de cinzas; 3 canais: vermelho, verde e azul);\n",
    "- $X$: Matriz de instâncias representadas pelos pixels das imagens. A ordem da matriz é $t \\times h \\times w \\times c$;\n",
    "- $\\pmb{y}$: Vetor de tamanho $m$ representando a classe real de cada instância;\n",
    "- $\\pmb{\\hat{y}}$: Vetor de predições que, para cada instancia, possui o valor predito para ela. Caso seja uma classificação binária, este valor será 0 ou 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenção do Dataset utilizando Batches e DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos usar um dataset de 4096 fotos de cachorros e gatos em que são 2048 fotos de cachorros e 2048 fotos de gato. Ele foi dividido de forma balanceada e aleatória em treino (2048 instancias), validação (1024 instâncias) e teste (1024 instâncias). Esse dataset está [disponivel no GitHub](https://github.com/daniel-hasan/cats-vs-dogs/archive/master.zip). Efetue o download, armazene no diretório `data` para fazer as tarefas. \n",
    "\n",
    "Ao trabalharmos milhares/milhões de imagens, nos deparamos com um problema: não conseguimos armazenar todas elas em memória principal. Por isso, a utilização do mini-batch gradient decent é importante, junto com os algoritmos de otimização que aprendemos (Gradiente Descent with Momentum, RMSProp e Adam). \n",
    "\n",
    "O Keras possui a classe [ImageGenerator](https://keras.io/preprocessing/image/#imagedatagenerator-class) para preprocessar imagens,  trabalhar com mini-batches de imagens e fazer, inclusive, _Data Augmentation_.  Na primeira parte desta prática iremos utilizá-la apenas para facilitar o uso de mini-batches e realizar o preprocessamento. Mais especificamente, iremos usar o método [flow_from_data_dir](https://keras.io/preprocessing/image/#flow_from_directory) que recebe como entrada uma estrutura de diretórios similar a esta:\n",
    "\n",
    "<img src=\"imgs/dataset-generator.png\">\n",
    "\n",
    "Nessa estrutura, cada subdiretório será uma classe com todas as imagens referentes a ela. `flow_from_data_dir` retorna um objeto iterável que, em cada iteração, retornará a tupla $(X,\\pmb{y})$, ou seja, um mini-batch $X$ e o vetor de classe $\\pmb{y}$ de cada instancia retornada pelo mini-batch (ver notação utilizada). Dessa forma, esse método armazena em memória principal apenas $t$ imagens por vez, ao invés do dataset completo.  \n",
    "\n",
    "O `flow_from_data_dir` retorna um iterador que entraria em um loop infinito: após iterar sobre todas as imagens, ele faz a operação toda novamente. Por isso, caso desejarmos navegar em todas as imagens por um `for`, temos que definir um ponto de parada que, sem _data augmentation_, é o número de imagens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 1 - Hello World DataGenerator:** crie uma pasta `img_toy_test` baseando-se estrutura acima com duas classes. Na primeira, coloque três cachorrinhos e, na outra, três gatinhos de sua escolha. Complete o código da função `plot_imgs_from_iterator` para efetuar a exibição dos mesmos. Essa função não possui teste unitário.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No image generator, você usará apenas um parametro rescale \n",
    "#..para deixar cada pixel entre 0 e 1. \n",
    "#..Lembre-se que cada pixel originalmente é um valor entre 0 e 255\n",
    "img_generator = ImageDataGenerator(rescale=None)\n",
    "\n",
    "#ao fazer o iterador, você deverá definir: \n",
    "#..O tamanho para que todas as imagens sejam redimensionadas; \n",
    "#..O modo de classificação que será binário (caso contrario, a representação nao seria um vetor)\n",
    "#..O seed sempre fixo de Constantes.SEED para permitirmos a reprodutibilidade dos experimentos\n",
    "#..Escolha um tamanho do batch de 2\n",
    "it_datagen = img_generator.flow_from_directory(\n",
    "                                                       \"img_toy_test\",\n",
    "                                                        target_size=None,\n",
    "                                                        batch_size=None,\n",
    "                                                        class_mode=None,\n",
    "                                                        seed=2\n",
    "                                                        )\n",
    "plot_imgs_from_iterator(it_datagen,2,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que o iterador passou duas vezes pelas imagens apresentando, aleatoriamente, cada uma das imagens. Verifique as dimensões (shape) dos elementos para entender como as imagens foram representadas. Apenas na apresentação, o tamanho da imagem apresentada pelo gnuplot é redimensionada proporcional ao valor passado na função figsize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 2 - função `get_dataset`: ** Nosso dataset já foi dividido em 3 partições: treino, teste e validação. Entendendo o funcionamento do `ImageDataGenerator`, você deverá implementar a função `get_dataset` que retornará um vetor três iteradores um para o treino, outro para teste e outro para validação. Esse vetor de iteradores obedecerá a mesma ordem de `Constantes.ARR_STR_DATA_DIR`. Para testar, você pode fazer um treino, teste e validação cada um com 6 itens e testar usando o `plot_imgs_from_iterator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação dos modelos para identificar gatineos e cachorrineos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a execução dos modelos que iremos criar pode demorar uns 30 minutos, iremos, primeiramente criar cada modelo para, na próxima seção, executarmos e fazermos a análise. Sempre consideramos que a imagem possuirá as dimensões 150x150x3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 3 - Criação de uma Fully Connected Network (FC) usando Keras:** Inicialmente, iremos implementar uma Rede Neural Totalmente Conectada (sem uso de convoluções) por meio do Keras. Você deverá implementar a seguinte Rede Neural:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/arquitetura-fc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem duas formas de implementar uma Rede Neural no Keras. Abaixo apresentamos o uso da [API funcional do Keras](https://keras.io/getting-started/functional-api-guide/) para uma arquetura similar aquela que você irá implementar. Existe outra API ([Sequential Model](https://keras.io/getting-started/sequential-model-guide/)), porém ela é menos flexivel. Perceba que aqui você irá criar o modelo/arquitetura mas ainda não irá realizar o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entrada (imagens de dimensões 150x150x3)\n",
    "entrada = Input(shape=(150,150,3),name=\"Entrada\")\n",
    "\n",
    "\n",
    "achatar = layers.Flatten()(entrada)\n",
    "#uma camada (ativação = relu) com 100 neuronios (unidades) a entrada (em formato de vetor) é passada como parametro\n",
    "camada_um = layers.Dense(100,activation=\"relu\",name=\"Camada1\")(achatar)\n",
    "camada_dois = layers.Dense(200,activation=\"relu\",name=\"Camada2\")(camada_um)\n",
    "#camada de saida\n",
    "#lembre-se que é uma classificação binária, a ativação é sigmoid\n",
    "saida = layers.Dense(1,activation=\"sigmoid\", name=\"saida\")(camada_dois)\n",
    "\n",
    "#cria-se o modelo\n",
    "modelo = Model(inputs=entrada, outputs=saida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada variável `camada_*` é uma camada. Veja que passamos uma camada como entrada da outra. [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) é uma FC, ou seja, umacamada totalmente conectada (pois, podemos chamá-la também de densely conected layers). Usamos uma camada [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) para \"achatar\"  a representação. Ou seja, cada instancia de imagem é representada por uma matriz 150x150x3 é transformada em um vetor de 67.500 atributos. Podemos ver a arquitetura usando `modelo.summary()`. Também podemos usar a função [plot_model](https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model) para exibir a imagem da arquitetura também - para visualizar a imagem, é necessário instalar algumas dependencias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que, mesmo uma rede neural com pouca profundidade, temos milhões de parametros a serem aprendidos.\n",
    "\n",
    "Agora, implemente a função `fully_connected_model` do arquivo `cats_vs_dogs.py` a seguinte rede neural totalmente conectada:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a função abaixo e armazene o modelo em `model_fc`. O número esperado de parametros a serem aprendidos é de 33.870.901."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fc = None\n",
    "model_fc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 4 - criação de uma Rede Neural Convolucional: ** Agora você implementará na função `simple_cnn_model` o modelo da seguinte rede convolucional usando o Keras: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/arquitetura-cnn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para isso, veja a seguir um exemplo das camadas de convolução e MaxPooling ([Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) e [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrada = Input(shape=(150,150,3),name=\"Entrada\")\n",
    "conv_2d_a = layers.Conv2D(32,(3,3),activation=\"relu\",name=\"Convolucao1\")(entrada)\n",
    "max_polling_a = layers.MaxPool2D((2,2))(conv_2d_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse exemplo é a parte inicial da arquitetura que você deverá implementar. Não esqueça de \n",
    "usar Flatten quando necessário. Logo após \"achatar\" a representação em um vetor, caso o parametro `add_dropout==True` você deverá usar o [Dropout](https://keras.io/layers/core/) em que 50% das ativações serão aleatoriamente zeradas. Abaixo, crie o modelo e armazene em `model_cnn_dropout` e  `model_cnn` as versões com e sem dropout, respectivamente. As duas arquiteturas possuirão 3.453.121 parametros a serem aprendidos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = simple_cnn_model(add_dropout=False)\n",
    "model_cnn_dropout = simple_cnn_model(add_dropout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exiba as arquiteturas das CNNs criadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de Data Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5 - Data Augmentation** Antes de rodar os modelos, iremos também aprender a utilizar _data augmentation_ em imagens. Para isso, no ImageGenerator abaixo, você usará, além do parametro rescale, parametros que serão reponsáveis por:\n",
    "- Executar rotações aleatorias na imagem (com limite de 40 graus)\n",
    "- aumentar/diminuir a largura e altura em 20%\n",
    "- Zoom in/out de 20% \n",
    "- cisalhamento (inclinação) de 20%\n",
    "- Espelhamento horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#..para deixar cada pixel entre 0 e 1. \n",
    "#..Lembre-se que cada pixel originalmente é um valor entre 0 e 255\n",
    "img_generator_aug = ImageDataGenerator(rescale=1/255,\n",
    "                                 rotation_range=None,\n",
    "                                 width_shift_range=None,\n",
    "                                 height_shift_range=None,\n",
    "                                 shear_range=None,\n",
    "                                 zoom_range=None,\n",
    "                                 horizontal_flip=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça o _Data Augmentation_ no dataset nosso de teste `img_toy_test` e execute o plot_imgs_from_iterator. Você peceberá que o DataGenerator nunca gera exatamente a mesma imagem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_datagen = img_generator_aug.flow_from_directory(\n",
    "                                                       None,\n",
    "                                                        target_size=(150,150),\n",
    "                                                        batch_size=2,\n",
    "                                                        class_mode='binary',\n",
    "                                                        seed=Constantes.SEED\n",
    "                                                        )\n",
    "plot_imgs_from_iterator(it_datagen,4,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução e avaliação dos métodos usados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 6 - Criação da função run_model: ** Crie a função `run_model` nela, você deverá:\n",
    "\n",
    "1. compile o modelo por meio do [método compile](https://keras.io/models/model/#compile) do objeto;\n",
    "2. também no objeto do modelo, execute o método [fit_generator](https://keras.io/models/model/#fit_generator) para criar o modelo. Esse método é o que recebe como parametro o iterador que aprendemos nas seções anteriores. Não será usado neste prática, mas também existe o [método fit](https://keras.io/models/model/#fit) que recebe como parametro os atributos - neste caso seria a matriz $X$ (similar ao scikit learn).\n",
    "3. Salve o modelo: A criação do modelo é uma parte custosa - em nossa prática pode durar de 15 a 30 minutos. Porém, alguns modelos poderiam demorar dias/semanas para serem criados, dependendo da profundidade e número de pesos a serem aprendidos. Por isso, é uma boa prática salvar os modelos por meio do método [save](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model). Se necessário, você pode carregar o modelo por meio da função [load_model](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model). Você pode também [salvar checkpoints do modelo](https://www.tensorflow.org/tutorials/keras/save_and_load) (opcional na prática);\n",
    "4. Avalie o modelo usando validação usando o [método evaluate_generator](https://keras.io/models/sequential/#evaluate_generator) e retorne a acurácia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 7 - Execução dos modelos**: Execute os modelos criados `model_fc`, `model_cnn` e `model_cnn_dropout` com a função run_model. Obtenha o resultado da validação. Sem GPU, a execução de cada modelo levar de 30 minutos a 4 horas. Você pode mandar executar e ir dormir :) - ou, para testar, diminua para um o número de épocas para ver funcionando mais rápido e, logo após, execute \"de verdade\" com os parametros padrão. Você pode também usar a GPU do [Google colab](colab.research.google.com) (coloque o link do seu colab aqui para ser avaliado). Com o colab, você terá apenas um trabalho a mais para enviar o dataset ao colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parametros - deixar o padrao\n",
    "param_default = ParametrosRedeNeural(\n",
    "                                    int_num_epochs=1#coloque apos testar com 1, você pode um valor maoir para analisar\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sempre pegar o dataset - para reiniciar o iterator\n",
    "arr_it_gen = get_dataset(param_default,Constantes.ARR_STR_DATA_DIR)\n",
    "#rode run_model para o fully connected model use o arr_it_gen em cada posição\n",
    "#.. Constantes.IDX_TREINO e Constantes.IDX_VALIDACAO para pegar os iteradores\n",
    "acc_fc = run_model(None,\n",
    "                   None,\n",
    "                   None,\n",
    "                   None,\n",
    "                   \"cats_vs_dogs_fc.h5\",\n",
    "                   #a quantidade de passos na validação deve ser a apresentada abaixo\n",
    "                   #pois precisamos de garanteir que o iterador da validação passe \n",
    "                   #por todos os itens de validação\n",
    "                   Constantes.QTD_VALIDACAO/param_default.int_batch_size)\n",
    "print(f\"Acurácia FC: {acc_fc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sempre pegar o dataset - para reiniciar o iterator\n",
    "arr_it_gen = get_dataset(param_default,Constantes.ARR_STR_DATA_DIR)\n",
    "#rode para run_model a CNN (sem dropout)\n",
    "acc_cnn = None\n",
    "print(f\"Acurácia CNN: {acc_cnn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rode run_model para a CNN (com dropout)\n",
    "model_cnn_dropout = None\n",
    "print(f\"Acurácia CNN (Dropout): {model_cnn_dropout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 8 - Data Augmentation** Agora, execute o modelo `model_cnn_dropout` porém usando `data augmentation`. Para isso, use o `img_generator_aug` (criado na seção anterior) e crie o iterador que percorra o dataset treino. Use o batch size de `param_default.int_batch_size` que é o mesmo usado no treino e use `Constantes.ARR_STR_DATA_DIR[Constantes.IDX_TREINO]` para obter o diretório do treino. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_gen_aug_train = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_it_gen = get_dataset(param_default,Constantes.ARR_STR_DATA_DIR)\n",
    "\n",
    "acc_cnn_data_aug = None\n",
    "print(f\"Acurácia CNN (data augmentation): {acc_cnn_data_aug}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 9 **: Para cada modelo, apresente a acurácia no teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo FC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo CNN_SIMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELO CNN_DATA_AUG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 9 - Análise** Descreva abaixo suas conclusões: existe algum modelo que gera overfitting? E underfitting, lembrando que o estado da arte gera resultados acima de 90% de acurácia? Qual é o impacto do _dropout_ e do uso de _Data augmentation_ no modelo? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O restante desta prática é opcional, se divirta! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\[Opcional 1\\]** Use outras técnicas para prevenir overfitting e analise também o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Opcional 2]** Use o predict_generator e analise, na partição de validação, quando o melhor modelo erra. Discuta e gere um plot de algumas imagens com os erros de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Opcional 3]** Usando a melhor arquitetura, varie os parametros na validação, encontre o melhor resultado e execute o teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\[Opcional 4\\]** Use o [tensor board](https://www.tensorflow.org/tensorboard/get_started) para fazer uma análise mais aprofundada do resultado. Apresente os gráficos e uma discussão dos resultados abaixo. O TensorFlow é usado pelo Keras. Você verá que ele cria o modelo de outra forma. Nós usamos a functional API, ela é mais flexivel que a [Sequetial API](http://keras.io/models/sequential/) do keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Opcional 5]** Faça transfer learning usando a arquitetura VGG (veja instruções abaixo). Pode demorar bastante em uma máquina sem GPU. Usar o [colab](https://colab.research.google.com) pode ser uma boa - coloque o link aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como dito em sala, o TransferLearning é uma técnica em que é possível extrair camadas de uma rede neural prétreinada e, com ela, usar em outro dominio. Isso é muito interessante pois, pode-se demorar horas/dias para se treinar uma rede neural e precisa-se de muitas instancias. \n",
    "\n",
    "Caso tenhamos uma rede que já foi treinada com milhares/milhões de instancias rotuladas, se deixaramos os pesos \"congelados\" de algumas camadas, adiantamos o processo de otimização e, assim, conseguimos uma melhor performance. Chollet F. (2017) conseguiu com uma metodologia parecida com essa da prática, um dataset de mesmo tamanho e tranfer learning uma acurácia acima de 90%. Algumas regras importantes também segundo Chollet F., (2017):  \n",
    "\n",
    "- Em uma CNN, temos geralmente no final camadas FC. Não é recomendável congelar camadas FC, pois, tais camadas não constumam generalizar tanto quanto as camadas Convolucionais;\n",
    "\n",
    "- Quanto mais próximo nossas instancias são das instancias da rede pretreinada, mais camadas podemos congelar. Por exemplo, a VGG16 treinado com o [ImageNet](http://image-net.org/explore) possui 1000 classes, inclusive, animais. Assim, para nosso problema de classificar cães e gatos, como é proximo desse problema, podemos congelar bastante camadas. Por exemplo, podemos até deixar descongelada apenas a camada de saída. \n",
    "\n",
    "O exemplo baixo possui o modelo com a arquitetura VGG16 treinada com o dataset ImageNet e uma função para congelar até uma determinada camada. O parametro `include_top=False` faz com que só usarmos a parte convolucional (sem as FC) desta arquitetura. Temos que criar a parte de FC para ser treinada.\n",
    "\n",
    "Caso opte por descongelar algumas camadas da VGG16 (e não apenas incrementar) faça da seguinte forma: \n",
    "\n",
    "- Mantenha todas as camadas da VGG16 congeladas, adicione as camadas FC (ou CONV) que você deseja e realize o treinamento;\n",
    "- Descongele as camadas da VGG16 desejadas; \n",
    "- Volte a treinar o modelo.\n",
    "\n",
    "O nome dessa técnica também é chamada de *fine tunning*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                 include_top=False,\n",
    "                 input_shape=(150,150,3))\n",
    "\n",
    "conv_base.summary()\n",
    "#conv_base.get_layer(\"block5_pool\").output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_first_layers(model,last_freezed_layer_name):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        if layer.name == last_freezed_layer_name:\n",
    "            break\n",
    "    return model\n",
    "\n",
    "def fine_tune_model(conv_base_model):\n",
    "    last_layer = len(conv_base_model.layers)-1\n",
    "    \n",
    "    achatar = layers.Flatten()(conv_base_model.layers[last_layer].output)\n",
    "    fc = layers.Dense(300,activation=\"relu\",name=\"Camada2\")(achatar)\n",
    "    saida = layers.Dense(1,activation=\"sigmoid\", name=\"saida\")(fc)\n",
    "    \n",
    "    return Model(inputs=conv_base_model.input, outputs=saida)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra utilidade de modelos pretreinados é fazer a extração de atributos, ou seja, representar imagens com uma matriz/vetor de uma camada intermediária de algum modelo pretreinado. Veja um código que extrai os atributos das imagens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def extract_features(model,layer_name,data_generator,batch_size, sample_size):\n",
    "    \"\"\"\n",
    "    Extrai a saída da camada layer_name para representar as features de\n",
    "    cada instancia\n",
    "    \n",
    "    Adaptado de: https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer\n",
    "    e do livro \"Deep Learning with Python\" (François Chollet, 2018)\n",
    "    \"\"\"\n",
    "    #obtem o modelo da primeiras camadas do model (até a layer_name)\n",
    "    output_layer = model.get_layer(layer_name).output \n",
    "    model_base = Model(inputs=model.input,\n",
    "                                     outputs=output_layer)\n",
    "    #inicializa o vetor de features com a mesma dimensão da ultima camada\n",
    "    last_layer_shape = output_layer.shape.as_list()\n",
    "    last_layer_shape[0] = batch_size\n",
    "    features = np.zeros(last_layer_shape)\n",
    "    \n",
    "    #inicializa o vetor de labels com o mesmo tamanho da amostra\n",
    "    labels = np.zeros(sample_size)\n",
    "    i = 0\n",
    "    for inputs_batch, labels in data_generator:\n",
    "        print(f\"Gerando features mini-batch #{i}...\")\n",
    "        #obtem as features\n",
    "        features_batch = model_base.predict(inputs_batch)\n",
    "        \n",
    "        #adiciona as features no batach\n",
    "        features[i*batch_size:(i+1)*batch_size] = features_batch\n",
    "        labels[i*batch_size:(i+1)*batch_size] = labels\n",
    "        \n",
    "        i += 1\n",
    "        print(i)\n",
    "        #termina assim que atingir o tamanho da amostra\n",
    "        if (i*batch_size)>=sample_size:\n",
    "            break\n",
    "    return labels,features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- McReynolds, Tom, and David Blythe. **[Advanced graphics programming using OpenGL](https://www.sciencedirect.com/science/article/pii/B9781558606593500147#cesec16). Cap. 14.** Elsevier, 2005.\n",
    "- Chollet , François. **Deep Learning with Python**. Manning Publications, 2017\n",
    "- Andrew D.G.. **[Especialização Deep Learning](https://www.coursera.org/specializations/deep-learning), curso Convolutional Neural Networks (semana 1 e 2)**. Coursera. 2019\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
