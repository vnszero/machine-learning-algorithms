{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notação matemática utilizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos uma notação matemática similar a da prática de regressão logística, com algumas modificações. Elementos modificados estão em azul, novos estão em verdes e, não modificados, em preto.\n",
    "\n",
    "- $m$: quantidade de instancias\n",
    "- $f$: quantidade de atributos\n",
    "- $n^{[l]}$: quantidade de unidades da camada $l$ \n",
    "- $X$: Matriz de instâncias representadas pelo seus atributos a ordem da matriz é $m \\times f$\n",
    "- $\\pmb{y}$: Vetor de tamanho $m$ representando a classe real de cada instância\n",
    "- $\\pmb{\\hat{y}}$: Vetor de predições que, para cada instancia, possui o valor predito para ela. Caso seja uma classificação binária, este valor será 0 ou 1\n",
    "\n",
    "- $Z^{[l]}$: Matriz, para cada instancia, o valor $z^{[l]}_{i,j}$ representando o resultado da função `z` para instancia $i$ e unidade $j$. Essa matriz é de ordem $m \\times n^{[l]}$\n",
    "\n",
    "- $A^{[l]}$ Matriz de ativações da camada $l$ em que, $a^{[l]}_{i,j}$ é um elemento dessa matriz que representa a ativação da instancia $i$ na camada $l$ na unidade $j$. Dessa forma essa é uma matriz de ordem $m \\times n^{[l]}$.\n",
    "\n",
    "- $W^{[l]}$ Matriz de pesos da camada $l$ em que cada elemento $w^{[l]}_{i,j}$ representa ponderação que a unidade $i$ da camada $l$ faz na unidade $j$ da matriz de ativação $A^{[l-1]}$. Caso seja primeira camada, a ponderação feita no atributo $j$ da matriz $X$. Dessa forma, essa matriz é de ordem $n^{[l]} \\times n^{[l-1]}$ exceto na primeira camada que é de ordem $n^{[l]} \\times f$\n",
    "\n",
    "- $Z^{d[l]}$: Derivada $\\frac{\\partial J}{\\partial z^{[l]}_{i,j}}$. Possui a mesma ordem que $Z$\n",
    "\n",
    "- $W^{d[l]}$: Derivada $\\frac{\\partial J}{\\partial w^{[l]}_{i,j}}$. Possui a mesma ordem que $W$\n",
    "\n",
    "Como em nossa implementação temos também representado os elementos por unidade, também usaremos a representação por unidade $j$ em uma determinada camada $l$, apresetnada a seguir. Note que são os mesmos elementos da prática de regressão, porém, nesta prática, temos que definir a qual unidade $j$ e camada $l$ estamos nos referindo.\n",
    "\n",
    "- $\\pmb{z}^{[l]}_j$: $j$-ésima coluna da matriz $Z^{[l]}$ representado o resultado da função z, para cada instancia $m$ da unidade $j$ e camada $l$. Assim, este vetor possui o tamanho $m$.\n",
    "- $\\pmb{a}^{[l]}_j$: $j$-ésima coluna da matriz $A^{[l]}$ representando o vetor de ativações da unidade $j$ de tamanho $m$ calculada por meio do vetor $z^[l]_j$\n",
    "- $\\pmb{w}^{[l]}_{j}$: j-ésima **linha** da matriz $W^{[l]}$ que representa vetor de pesos de uma unidade (neurônio) $j$ para ponderar os pesos da camada anterior ou, caso seja a primeira camada, pondera os atributos. Assim, esse vetor possui o tamanho $f$ (primeira camada) e tamanho  $n^{[l-1]}$ (demais camadas).\n",
    "\n",
    "\n",
    "- $b^{[l]}_j$: Valor de viés da unidade $j$ e camada $l$ (do inglês, bias term)\n",
    "<!--- <span class=\"blue\" style=\"color:blue\">$w_j$: $j$-ésimo valor do vetor $\\pmb{w}$ que pondera $j$-ésima coluna da matriz $A^{[l]}$. Essa coluna representa a $j$-ésima  unidade da camada anterior ou, quando for a primeira camada, o $j$-ésimo atributo da matriz $X$</span>.-->\n",
    "\n",
    "Além das representações das derivadas por unidade $j$  em uma determinada camada $l$:\n",
    "\n",
    "- $\\pmb{z}^{d[l]}_j$: Vetor de derivada $\\frac{\\partial J}{\\partial z_j}$ para cada instância $i$ do modelo de uma determinada unidade $j$ e camada $l$ em uma determinada camada $l$. Possui o mesmo tamanho que  $\\pmb{z}$\n",
    "- $\\pmb{w}^{d[l]}_j$ Vetor de derivada $\\frac{\\partial J}{\\partial w_j}$ para cada unidade $j$ do modelo, possui o mesmo tamanho de $\\pmb{w}^{[l]}_{j}$\n",
    "- $b^{d[l]}_j$: Derivada $\\frac{\\partial J}{\\partial b^{[l]}_j}$ \n",
    "\n",
    "Perceba que esses vetores representam a $j$-ésima coluna de suas respectivas matrizes, para cada unidade $j$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, apresentaremos as modificações que devem ser feitas por classe. Primeiramente, execute as importanções que usaremos na prática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from rede_neural_profunda import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe FuncaoAtivacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 1 - Funções de ativações novas e alteração na função sigmoid**: Você agora poderá ver 4 objetos da classe `FuncaoAtivacao` instanciados: sigmoid, relu, leaky_relu e tanh. Você deverá modificar alterar as suas funções lambda para retornar o resultado correto, dependendo da função de ativação.\n",
    "\n",
    "O valor do vetor de derivadas $\\pmb{z^{d[l]}_j}$ é diferente caso estejamos na última camada. Por isso, na classe `FuncaoAtivacao` agora temos um atributo (opcional) `dz_ultima_camada` que é a função da derivada quando  calculamos o vetor $\\pmb{z^{d[l]}_j}$ na última camada com uma determinada função de ativação. A função de ativação `sigmoid` será a única que usaremos como última camada, assim, altere o objeto `sigmoid`com a função  `dz_ultima_camada` correspondente. \n",
    "\n",
    "O atributo `dz_ultima_camada` do objeto sigmoid terá a função que usamos como `dz_funcao`  na prática passada. Agora, o parametro `dz_funcao` será diferente: temos que usar o parâmetro `arr_dz_w_prox` representando a $j$-ésima coluna do produto $Z^{d[l+1]} \\times W^{[l+1]}$. Crie também as funções de ativação relu, leaky_relu e tangente hiperbólica (variáveis `relu`, `leaky_relu` e `tanh`, respectivamente) usando apenas os parâmetros do construtor `funcao` e `dz_funcao`. Lembrando que não usaremos as funções de ativação `relu`, `leaky_relu` e `tanh` como última camada (camada de saída). Para implementar essas expressões lambda, você poderá usar vetorização. Lembre-se do seguinte código da prática de regressão logística: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "meu_querido_vetor = np.array([3,2,8,9,2])\n",
    "#a linha abaixo retorna true ou false, dependendo do valor\n",
    "print(meu_querido_vetor>4)\n",
    "#Se multiplicamos um número por um vetor numpy de true e false \n",
    "#. é o mesmo de multiplicarmos o número por 1 ou 0, respectivamente\n",
    "print(3*(meu_querido_vetor>4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestFuncaoAtivacao.test_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestFuncaoAtivacao.test_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestFuncaoAtivacao.test_leaky_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestFuncaoAtivacao.test_tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alteração da classe `RegressaoLogistica` para `Unidade`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classe Unidade:** A classe RegressaoLogistica da prática anterior foi renomeada para Unidade.  Essa classe já está pronta, agora, ela representará uma unidade (neurônio). Alguns atributos  foram removidos (pois agora não criamos o modelo apenas com uma unidade) e um único atributo foi modificado: tinhamos antes o `mat_x`, representando a matriz $X$, ao invés dele, temos que utilizar a matriz $A^{[l-1]}$, ou seja, a matriz de ativações da camada anterior. Essa matriz, representada pelo atributo `mat_a_ant`, é usada no lugar de `mat_x` para calcular o `forward_propagation` e o `backward_propagation`. Reveja a implementação e relembre os atributos dela:\n",
    "\n",
    "- `b`: Valor $b$ de viés da regressão logística \n",
    "- `func_ativacao`: função de ativação a ser usada. Esse atributo é uma função Python\n",
    "- `dz_func`: função derivada a ser usada de acordo com a função de ativação\n",
    "- `arr_w`: vetor de pesos $\\pmb{w^{[l]}_{j}}$\n",
    "- `arr_z`: vetor de resultados $\\pmb{z^{[l]}_{j}}$ \n",
    "- `arr_a`: vetor de ativações $\\pmb{a^{[l]}_{j}}$\n",
    "- `mat_a_ant`: Matriz de ativações da camada anterior $A^{[l-1]}$ (ou matriz $X$, caso esta seja a primeira camada)\n",
    "- `gradiente`: Instancia da classe `Gradiente` que possui os atributos `arr_dz`, `arr_dw` e `db` representando, respectivamente, $\\pmb{z^{d[l]}_j}$, $\\pmb{w^{d[l]}_j}$ e $b^{d[l]}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação da classe Camada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe camada possui os seguintes atributos:\n",
    "\n",
    "- `arr_unidades`: Vetor de unidades (neurônios) desta camada. Representada por instâncias da classe `Unidade`. São inicializados por meio dos parametros do construtor que definem a quantidade de unidades, função de ativação e derivada (`qtd_unidades`, `func_ativacao` e `func_dz` respectivamente).\n",
    "- `ant_camada`: Instância da classe `Camada` referente à camada anterior. Caso seja a primeira camada escondida, então `ant_camada == None`\n",
    "- `prox_camada`: Instância da classe `Camada` referente à proxima camada. Caso seja a última camada (camada de saída), então `prox_camada == None`\n",
    "- `qtd_un_camada_ant`: Quantidade de unidades da camada anterior, armazena o valor $n^{[l-1]}$ exceto na primeira camada que recebe o valor $f$ \n",
    "- `mat_w`: atributo calculado (`property`) representando a matriz de pesos $W^{[l]}$ desta camada\n",
    "- `mat_dz`: atributo calculado (`property`) representando a derivada $Z^{d[l]}$. \n",
    "- `mat_dz_w`: Atributo calculado que efetua o produto $Z^{d[l]} \\times W^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 2 - construtor da classe `Camada`**: Inicialize o vetor de `arr_unidades` com a quantidade de unidades correspondente (use os parâmetros do construtor da classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestCamada.test_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 3 - método `forward_propagation` da classe `Camada`**: Nesse método, você deverá criar a matriz de ativação $A^{[l]}$ da camada atual $l$ (representada por `mat_a`) por meio da matriz de ativações anterior $A^{[l-1]}$ (representada por `mat_a_ant`). Para isso, você deverá chamar o método `forward_propagation` de cada unidade (classe Unidade), implementado por vocês na prática de Regressão Logística, que cria o vetor $\\pmb{a^{[l]}_j}$, representado por `arr_a` de cada unidade. Para criar a matriz `mat_a`, observe sempre a sua dimensão. Dica: veja abaixo uma forma de preencher valores em linhas/colunas de uma matriz numpy por meio de vetores. Você deverá usar [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html) para inicializar a matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#considere a matriz mat_h e o vetor arr_linha e arr_coluna:\n",
    "mat_h = np.zeros((10,5))\n",
    "arr_linha = np.random.rand(5)\n",
    "arr_coluna = np.random.rand(10)\n",
    "print(mat_h)\n",
    "print(arr_linha)\n",
    "print(arr_coluna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para preenchermos a linha 3 com o vetor arr_linha:\n",
    "mat_h[3,:] = arr_linha\n",
    "mat_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para preenchermos a coluna 2 com o vetor arr_coluna: \n",
    "mat_h[:,2] = arr_coluna\n",
    "mat_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestCamada.test_forward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 4 - propriedades da classe `Camada`**: Você deverá implementar as propriedades `mat_w` e `mat_dz` para criar as matrizes $W^{[l]}$ e $Z^{d[l]}$. Logo após, vocẽ deverá implementar a propriedade `mat_dz_w` que calcula $Z^{d[l]} \\times W^{[l]}$. Fique atento nas dimensões das matrizes para usar o preenchimento de matrizes corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestCamada.test_mat_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestCamada.test_mat_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestCamada.test_mat_dz_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5 - método backward_propagation da classe `Camada`:** Por meio da matriz `mat_dz_dw` da camada seguinte, ou seja, o resultado de $Z^{d[l+1]} \\times W^{[l+1]}$ e do vetor de classes $\\pmb{y}$, execute o backward propagation de todas as unidades. Para obter o `mat_dz_dw` da camada seguinte não esqueça que as camadas possuem o atributo `prox_camada`. Caso essa seja a última camada, então `mat_dz_dw == None`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestCamada.test_backward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação da classe `RedeNeural`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe `RedeNeural` possui os seguintes atributos: \n",
    "\n",
    "- `arr_camadas`: Vetor que armazena a lista de instancias da classe `Camada` dessa Rede Neural. Esses objetos são instanciados por meio do método `config_rede`\n",
    "- `arr_qtd_un_por_camada`: Vetor de inteiros que, o valor na posição `i` do vetor corresponde a quantidade de unidades na i-ésima camada.\n",
    "- `arr_func_a_por_camada`: Função de ativação por camada. Esse vetor armazena objetos da classe `FuncaoAtivacao` em que, cada posição `i` do vetor, corresponde a função de ativação da i-ésima camada\n",
    "- `num_iteracoes`: Número de iterações (épocas) que a rede neural irá rodar\n",
    "- `arr_y`: Representa o vetor $\\pmb{y}$. Esses objetos são instanciados por meio do método `config_rede`\n",
    "- `mat_x`: Representa a matriz de atributos por instancias $X$. Esses objetos são instanciados por meio do método `config_rede`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 6 - método `config_rede`**: instancia  armazena em arr_camada todas as camadas por meio da\n",
    "        quantidade de unidades por camada `arr_qtd_un_por_camada` e funções de ativação `arr_func_a_por_camada`. Para cada camada, você deverá referenciar a camada anterior e a próxima por meio os atributos `ant_camada` e `prox_camada`, respectivamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Código Python útil para atividades 7 e 8:** Muitas vezes, precisamos percorrer vetores a partir de uma determinada posição ou de trás para frente. Veja como isso pode ser feito usando a função [range](https://docs.python.org/3/library/functions.html#func-range) e a função [enumerate](https://docs.python.org/3/library/functions.html#enumerate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_v = [4,2,4,5,3,1]\n",
    "tam_vet = len(arr_v)\n",
    "#percorrer elementos de tras para frente\n",
    "print(\"Tras para frente: \")\n",
    "for i in range(tam_vet-1,-1,-1):\n",
    "    print(f\"Pos arr_v[{i}]: {arr_v[i]}\")\n",
    "print(\"---\")\n",
    "print(\"A partir do 3o: \")\n",
    "#percorrer elementos a partir do indice 3\n",
    "for i in range(3,tam_vet):\n",
    "    print(f\"Pos arr_v[{i}]: {arr_v[i]}\")\n",
    "    \n",
    "#Com o enumerate, também conseguimos percorrer a partir de uma posição\n",
    "#..Lembre que arr_v[3:] retorna o vetor a partir da terceira posição \n",
    "print(\"mesma efeito, com enumerate:\")\n",
    "for i,valor in enumerate(arr_v[3:],3):\n",
    "    print(f\"Pos arr_v[{i}]: {valor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2021-04-14 09:47:38,662] Finished trial#0 with value: 2.0 with parameters: {'y': 2}. Best is trial#0 with value: 2.0.\n",
      "[I 2021-04-14 09:47:38,859] Finished trial#1 with value: 4.0 with parameters: {'y': 4}. Best is trial#0 with value: 2.0.\n",
      "[I 2021-04-14 09:47:39,036] Finished trial#2 with value: 6.0 with parameters: {'y': 6}. Best is trial#0 with value: 2.0.\n",
      "[I 2021-04-14 09:47:39,215] Finished trial#3 with value: 5.0 with parameters: {'y': 5}. Best is trial#0 with value: 2.0.\n",
      "[I 2021-04-14 09:47:39,388] Finished trial#4 with value: 3.0 with parameters: {'y': 3}. Best is trial#0 with value: 2.0.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All grids have been evaluated. If you want to avoid this error, please make sure that unnecessary trials do not run during optimization by properly setting `n_trials` in `study.optimize`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9519ab63f05b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msearch_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 self._optimize_sequential(\n\u001b[0;32m--> 339\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 )\n\u001b[1;32m    341\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial, time_start)\u001b[0m\n\u001b[1;32m    680\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrial_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mtrial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_new_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_study_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0mtrial_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/trial/_trial.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, study, trial_id)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_relative_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_relative_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/trial/_trial.py\u001b[0m in \u001b[0;36m_init_relative_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelative_search_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_relative_search_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         self.relative_params = self.study.sampler.sample_relative(\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelative_search_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/samplers/grid.py\u001b[0m in \u001b[0;36msample_relative\u001b[0;34m(self, study, trial, search_space)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munvisited_grids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             raise ValueError(\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0;34m\"All grids have been evaluated. If you want to avoid this error, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;34m\"please make sure that unnecessary trials do not run during \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;34m\"optimization by properly setting `n_trials` in `study.optimize`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All grids have been evaluated. If you want to avoid this error, please make sure that unnecessary trials do not run during optimization by properly setting `n_trials` in `study.optimize`."
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "def objective(trial):\n",
    "    y = trial.suggest_int(\"y\", -99, 99)\n",
    "    return y\n",
    "\n",
    "\n",
    "search_space = {\"y\": [2,3,4,5,6]}\n",
    "study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space))\n",
    "study.optimize(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestRedeNeural.test_config_rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 7 - método `forward_propagation`**: Execute, para todas as camadas, o método forward_propagation. Note que, para primeira camada,\n",
    "    a entrada será a matriz $X$, representada por `mat_x` e, as demais, seriam a matriz $A^{[l-1]}$, ou seja, a matriz de ativações da camada anterior. Lembre-se que cada camada possui sua matriz de ativações representada\n",
    "    pelo atributo `mat_a` e temos o vetor de camadas para podemos acessar as camadas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestRedeNeural.test_forward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 8 - método `backward_propagation`**: Execute, para todas as camadas, o método backward_propagation. Fique atento na ordem de execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestRedeNeural.test_backward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 9 - método `fit`**: SImilar ao método `fit` que implementamos na prática de Regressão Logística, realiza self.num_iteracoes iterações (épocas) da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestRedeNeural.test_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 10 - método `loss_function`**: Para calcular o `loss_function`, você deverá obter o vetor de ativações $\\pmb{a}$ (representado por `arr_a` em cada unidade) apropriado. Fique atento em qual camada/unidade você deverá obter o arr_a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 11 - método `predict`**: similar ao da regressão logistica, para uma determinada matriz $X$ representando todos os atributos a serem preditos, calcula a predição $\\pmb{\\hat{y}}$ para cada instância $\\in X$ por meio do método `forward_propagation` de um modelo já criado. Esse código será muito similar ao que fizemos na prática de regressão logistica, porém, deverá ficar atento a qual camada/unidade você deverá obter o vetor de ativações arr_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m rede_neural_profunda_test TestRedeNeural.test_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo - Rede Neural Funcionando*  🤩 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Assim espera-se ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usaremos o mesmo dataset da prática de regrssão logistica:\n",
    "mat_x, arr_y = sklearn.datasets.make_moons(400, noise=0.05)\n",
    "plt.scatter(mat_x[:,0], mat_x[:,1], s=40, c=arr_y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie uma rede neural de duas camadas a primeira com 4 unidades escondidas e, a segunda, com 1 unidade (essa seria a camada de saída). A função de ativação da primeira camada será uma Tangente Hiperbólica (`tanh`) e, da segunda, uma `sigmoid`. Rode por 3.000 épocas. Ao trainar, coloque como learning rate=1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = None\n",
    "dnn.fit(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(dnn,mat_x,arr_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça testes aumentando o número de unidades, a função de ativação e/ou aumentando a profundidade da rede neural. Descreva abaixo o que foi observado. Perceba que, nesta prática, fazer uma rede neural profunda geralmente não irá melhorar o modelo. Um dos motivos é que temos apenas 2 atributos. Uma rede neural profunda tende a melhorar o resultado quando temos diversos atributos, pois, em cada camada reduzimos a dimensionalidade do problema. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
